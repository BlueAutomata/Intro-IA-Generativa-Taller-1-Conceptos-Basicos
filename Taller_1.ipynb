{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90fd06a3",
   "metadata": {},
   "source": [
    "# Taller 1 Conceptos bÃ¡sicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7f2bfc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\guill\\Documents\\GitHub\\Intro-IA-Generativa-Taller-1-Conceptos-Basicos>() \n"
     ]
    }
   ],
   "source": [
    "!myenv\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a46b9e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.2.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: transformers==4.44.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 3)) (4.44.2)\n",
      "Requirement already satisfied: nltk==3.9.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 4)) (3.9.1)\n",
      "Requirement already satisfied: spacy==3.7.5 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 5)) (3.7.5)\n",
      "Requirement already satisfied: torch in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 7)) (2.3.1)\n",
      "Requirement already satisfied: nbdime==4.0.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 8)) (4.0.2)\n",
      "Requirement already satisfied: datasets==3.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 9)) (3.0.1)\n",
      "Requirement already satisfied: model2vec in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 10)) (0.2.3)\n",
      "Requirement already satisfied: evaluate==0.4.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 11)) (0.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==2.2.3->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==2.2.3->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==2.2.3->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (0.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (4.66.5)\n",
      "Requirement already satisfied: click in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk==3.9.1->-r requirements.txt (line 4)) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk==3.9.1->-r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (0.12.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (2.6.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (65.5.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (3.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 8)) (0.4.6)\n",
      "Requirement already satisfied: gitpython!=2.1.4,!=2.1.5,!=2.1.6 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 8)) (3.1.43)\n",
      "Requirement already satisfied: jupyter-server in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 8)) (2.14.2)\n",
      "Requirement already satisfied: jupyter-server-mathjax>=0.2.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 8)) (0.2.6)\n",
      "Requirement already satisfied: nbformat in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 8)) (5.10.4)\n",
      "Requirement already satisfied: pygments in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 8)) (2.18.0)\n",
      "Requirement already satisfied: tornado in c:\\users\\guill\\appdata\\roaming\\python\\python311\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 8)) (6.4.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets==3.0.1->-r requirements.txt (line 9)) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets==3.0.1->-r requirements.txt (line 9)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets==3.0.1->-r requirements.txt (line 9)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets==3.0.1->-r requirements.txt (line 9)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==3.0.1->-r requirements.txt (line 9)) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets==3.0.1->-r requirements.txt (line 9)) (3.10.9)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->-r requirements.txt (line 7)) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->-r requirements.txt (line 7)) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->-r requirements.txt (line 7)) (3.3)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->-r requirements.txt (line 7)) (2021.4.0)\n",
      "Requirement already satisfied: rich in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from model2vec->-r requirements.txt (line 10)) (13.9.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from model2vec->-r requirements.txt (line 10)) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets==3.0.1->-r requirements.txt (line 9)) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets==3.0.1->-r requirements.txt (line 9)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets==3.0.1->-r requirements.txt (line 9)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets==3.0.1->-r requirements.txt (line 9)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets==3.0.1->-r requirements.txt (line 9)) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets==3.0.1->-r requirements.txt (line 9)) (1.14.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gitpython!=2.1.4,!=2.1.5,!=2.1.6->nbdime==4.0.2->-r requirements.txt (line 8)) (4.0.11)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy==3.7.5->-r requirements.txt (line 5)) (2.1.5)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (4.3.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (23.1.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in c:\\users\\guill\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\guill\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (5.7.2)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (0.5.3)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (7.16.4)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (0.21.0)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (2.0.13)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\guill\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (26.2.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (0.18.1)\n",
      "Requirement already satisfied: traitlets>=5.6.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (5.14.3)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (1.8.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.5->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->-r requirements.txt (line 7)) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->-r requirements.txt (line 7)) (2021.13.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbformat->nbdime==4.0.2->-r requirements.txt (line 8)) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbformat->nbdime==4.0.2->-r requirements.txt (line 8)) (4.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5->-r requirements.txt (line 5)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5->-r requirements.txt (line 5)) (2.16.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.2.3->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.44.2->-r requirements.txt (line 3)) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.44.2->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.44.2->-r requirements.txt (line 3)) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.44.2->-r requirements.txt (line 3)) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.5->-r requirements.txt (line 5)) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.5->-r requirements.txt (line 5)) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.5->-r requirements.txt (line 5)) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->model2vec->-r requirements.txt (line 10)) (3.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.5->-r requirements.txt (line 5)) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.5->-r requirements.txt (line 5)) (7.0.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->model2vec->-r requirements.txt (line 10)) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->model2vec->-r requirements.txt (line 10)) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch->-r requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio>=3.1.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (21.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=2.1.4,!=2.1.5,!=2.1.6->nbdime==4.0.2->-r requirements.txt (line 8)) (5.0.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat->nbdime==4.0.2->-r requirements.txt (line 8)) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat->nbdime==4.0.2->-r requirements.txt (line 8)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat->nbdime==4.0.2->-r requirements.txt (line 8)) (0.20.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (4.2.2)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\guill\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (306)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (0.1.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.5->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->model2vec->-r requirements.txt (line 10)) (0.1.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (0.10.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (1.5.1)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy==3.7.5->-r requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==3.0.1->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (0.5.1)\n",
      "Requirement already satisfied: fqdn in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (3.0.0)\n",
      "Requirement already satisfied: uri-template in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (24.8.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (2.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (2.9.0.20241003)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "efa107ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from transformers import pipeline, set_seed\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from evaluate import evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eec24ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c612c8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     - ------------------------------------- 0.5/12.8 MB 578.7 kB/s eta 0:00:22\n",
      "     - ------------------------------------- 0.5/12.8 MB 578.7 kB/s eta 0:00:22\n",
      "     -- ------------------------------------ 0.8/12.8 MB 588.4 kB/s eta 0:00:21\n",
      "     -- ------------------------------------ 0.8/12.8 MB 588.4 kB/s eta 0:00:21\n",
      "     --- ----------------------------------- 1.0/12.8 MB 606.3 kB/s eta 0:00:20\n",
      "     --- ----------------------------------- 1.0/12.8 MB 606.3 kB/s eta 0:00:20\n",
      "     --- ----------------------------------- 1.3/12.8 MB 645.3 kB/s eta 0:00:18\n",
      "     ---- ---------------------------------- 1.6/12.8 MB 655.4 kB/s eta 0:00:18\n",
      "     ---- ---------------------------------- 1.6/12.8 MB 655.4 kB/s eta 0:00:18\n",
      "     ----- --------------------------------- 1.8/12.8 MB 671.0 kB/s eta 0:00:17\n",
      "     ----- --------------------------------- 1.8/12.8 MB 671.0 kB/s eta 0:00:17\n",
      "     ------ -------------------------------- 2.1/12.8 MB 699.0 kB/s eta 0:00:16\n",
      "     ------- ------------------------------- 2.4/12.8 MB 721.7 kB/s eta 0:00:15\n",
      "     ------- ------------------------------- 2.6/12.8 MB 743.8 kB/s eta 0:00:14\n",
      "     ------- ------------------------------- 2.6/12.8 MB 743.8 kB/s eta 0:00:14\n",
      "     -------- ------------------------------ 2.9/12.8 MB 762.5 kB/s eta 0:00:14\n",
      "     --------- ----------------------------- 3.1/12.8 MB 775.4 kB/s eta 0:00:13\n",
      "     ---------- ---------------------------- 3.4/12.8 MB 780.4 kB/s eta 0:00:13\n",
      "     ---------- ---------------------------- 3.4/12.8 MB 780.4 kB/s eta 0:00:13\n",
      "     ----------- --------------------------- 3.7/12.8 MB 790.1 kB/s eta 0:00:12\n",
      "     ----------- --------------------------- 3.9/12.8 MB 796.1 kB/s eta 0:00:12\n",
      "     ----------- --------------------------- 3.9/12.8 MB 796.1 kB/s eta 0:00:12\n",
      "     ------------ -------------------------- 4.2/12.8 MB 796.3 kB/s eta 0:00:11\n",
      "     ------------- ------------------------- 4.5/12.8 MB 801.2 kB/s eta 0:00:11\n",
      "     -------------- ------------------------ 4.7/12.8 MB 807.9 kB/s eta 0:00:11\n",
      "     -------------- ------------------------ 4.7/12.8 MB 807.9 kB/s eta 0:00:11\n",
      "     --------------- ----------------------- 5.0/12.8 MB 816.2 kB/s eta 0:00:10\n",
      "     --------------- ----------------------- 5.2/12.8 MB 823.7 kB/s eta 0:00:10\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 834.7 kB/s eta 0:00:09\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 834.7 kB/s eta 0:00:09\n",
      "     ----------------- --------------------- 5.8/12.8 MB 830.9 kB/s eta 0:00:09\n",
      "     ------------------ -------------------- 6.0/12.8 MB 831.2 kB/s eta 0:00:09\n",
      "     ------------------ -------------------- 6.0/12.8 MB 831.2 kB/s eta 0:00:09\n",
      "     ------------------- ------------------- 6.3/12.8 MB 831.6 kB/s eta 0:00:08\n",
      "     ------------------- ------------------- 6.6/12.8 MB 833.6 kB/s eta 0:00:08\n",
      "     ------------------- ------------------- 6.6/12.8 MB 833.6 kB/s eta 0:00:08\n",
      "     -------------------- ------------------ 6.8/12.8 MB 835.5 kB/s eta 0:00:08\n",
      "     --------------------- ----------------- 7.1/12.8 MB 838.9 kB/s eta 0:00:07\n",
      "     --------------------- ----------------- 7.1/12.8 MB 838.9 kB/s eta 0:00:07\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 838.8 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 837.3 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 837.3 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.9/12.8 MB 837.4 kB/s eta 0:00:06\n",
      "     ------------------------ -------------- 8.1/12.8 MB 837.4 kB/s eta 0:00:06\n",
      "     ------------------------ -------------- 8.1/12.8 MB 837.4 kB/s eta 0:00:06\n",
      "     ------------------------- ------------- 8.4/12.8 MB 838.8 kB/s eta 0:00:06\n",
      "     -------------------------- ------------ 8.7/12.8 MB 844.1 kB/s eta 0:00:05\n",
      "     --------------------------- ----------- 8.9/12.8 MB 845.2 kB/s eta 0:00:05\n",
      "     --------------------------- ----------- 8.9/12.8 MB 845.2 kB/s eta 0:00:05\n",
      "     --------------------------- ----------- 9.2/12.8 MB 838.9 kB/s eta 0:00:05\n",
      "     --------------------------- ----------- 9.2/12.8 MB 838.9 kB/s eta 0:00:05\n",
      "     --------------------------- ----------- 9.2/12.8 MB 838.9 kB/s eta 0:00:05\n",
      "     ---------------------------- ---------- 9.4/12.8 MB 816.6 kB/s eta 0:00:05\n",
      "     ---------------------------- ---------- 9.4/12.8 MB 816.6 kB/s eta 0:00:05\n",
      "     ----------------------------- --------- 9.7/12.8 MB 814.0 kB/s eta 0:00:04\n",
      "     ----------------------------- -------- 10.0/12.8 MB 816.8 kB/s eta 0:00:04\n",
      "     ----------------------------- -------- 10.0/12.8 MB 816.8 kB/s eta 0:00:04\n",
      "     ------------------------------ ------- 10.2/12.8 MB 819.5 kB/s eta 0:00:04\n",
      "     ------------------------------- ------ 10.5/12.8 MB 819.9 kB/s eta 0:00:03\n",
      "     ------------------------------- ------ 10.5/12.8 MB 819.9 kB/s eta 0:00:03\n",
      "     ------------------------------- ------ 10.7/12.8 MB 818.4 kB/s eta 0:00:03\n",
      "     -------------------------------- ----- 11.0/12.8 MB 818.9 kB/s eta 0:00:03\n",
      "     -------------------------------- ----- 11.0/12.8 MB 818.9 kB/s eta 0:00:03\n",
      "     --------------------------------- ---- 11.3/12.8 MB 822.2 kB/s eta 0:00:02\n",
      "     ---------------------------------- --- 11.5/12.8 MB 824.5 kB/s eta 0:00:02\n",
      "     ----------------------------------- -- 11.8/12.8 MB 826.7 kB/s eta 0:00:02\n",
      "     ----------------------------------- -- 11.8/12.8 MB 826.7 kB/s eta 0:00:02\n",
      "     ----------------------------------- -- 12.1/12.8 MB 827.8 kB/s eta 0:00:01\n",
      "     ------------------------------------ - 12.3/12.8 MB 828.0 kB/s eta 0:00:01\n",
      "     ------------------------------------ - 12.3/12.8 MB 828.0 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.6/12.8 MB 825.7 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.6/12.8 MB 825.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 12.8/12.8 MB 824.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.2)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "\u001b[38;5;2mâ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb0a2e",
   "metadata": {},
   "source": [
    "## 1. IntroducciÃ³n a NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce6930",
   "metadata": {},
   "source": [
    "### 1.1. Pregunta 1: Â¿QuÃ© es el Procesamiento de Lenguaje Natural y cuÃ¡les son sus principales aplicaciones?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc47014",
   "metadata": {},
   "source": [
    "### 1.1.1 Ejercicio: Investigar 3 aplicaciones actuales de NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Pregunta 2: Explica los conceptos de tokenizaciÃ³n y segmentaciÃ³n de oraciones. Â¿Por quÃ© son importantes en NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4237df",
   "metadata": {},
   "source": [
    "#### 1.2.1 Ejercicio: Usar la biblioteca nltk o spaCy en Python para tokenizar un texto corto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb7a8f",
   "metadata": {},
   "source": [
    "##### 1.2.1.1 Ejercicio con Nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a299848",
   "metadata": {},
   "source": [
    "###### 1.2.1.1.1 Escribir un texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31209251",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text = \"I am learning about tokenization!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b13422",
   "metadata": {},
   "source": [
    "###### 1.2.1.1.2 Tokenizar por oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bead825c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentance list: ['I am learning about tokenization!'].\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(nltk_text)\n",
    "print(f\"Sentance list: {sentences}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638e2c3d",
   "metadata": {},
   "source": [
    "###### 1.2.1.1.3 Tokenizar por palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "073b19e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word list: ['I', 'am', 'learning', 'about', 'tokenization', '!'].\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(nltk_text)\n",
    "print(f\"Word list: {words}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994196ea",
   "metadata": {},
   "source": [
    "###### 1.2.1.1.4 Tokenizar por subpalabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8ab81ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subword list: ['i', 'am', 'learn', 'about', 'token', '!'].\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer_subwords = [stemmer.stem(word) for word in words]\n",
    "print(f\"Subword list: {stemmer_subwords}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f94dc2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subword list: ['I', 'am', 'learning', 'about', 'tokenization', '!'].\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer_subwords = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(f\"Subword list: {lemmatizer_subwords}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1826e77f",
   "metadata": {},
   "source": [
    "###### 1.2.1.1.5 Tokenizar por caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "34448aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character List: ['I', ' ', 'a', 'm', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'b', 'o', 'u', 't', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', '!']\n"
     ]
    }
   ],
   "source": [
    "characters = list(nltk_text)\n",
    "print(f\"Character List: {characters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e09289",
   "metadata": {},
   "source": [
    "#### 1.2.1.2 Ejercicio con spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806836f2",
   "metadata": {},
   "source": [
    "###### 1.2.1.2.1 Cargar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d770c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fcb12428",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_text = \"I am learning how to do tokenization with spacy. It is awesome!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff991372",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(spacy_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d2e799",
   "metadata": {},
   "source": [
    "###### 1.2.1.2.2 Tokenizar por oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a1f94bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I am learning how to do tokenization with spacy., It is awesome!]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "for sentence in doc.sents:\n",
    "    sentences.append(sentence)\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab2178",
   "metadata": {},
   "source": [
    "###### 1.2.1.2.2 Tokenizar por words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ab00e897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'how',\n",
       " 'to',\n",
       " 'do',\n",
       " 'tokenization',\n",
       " 'with',\n",
       " 'spacy',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'awesome',\n",
       " '!']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for token in doc:\n",
    "    words.append(token.text)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db6c0b9",
   "metadata": {},
   "source": [
    "###### 1.2.1.2.3 Tokenizar por caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5736e1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " ' ',\n",
       " 'a',\n",
       " 'm',\n",
       " ' ',\n",
       " 'l',\n",
       " 'e',\n",
       " 'a',\n",
       " 'r',\n",
       " 'n',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'h',\n",
       " 'o',\n",
       " 'w',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'd',\n",
       " 'o',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " 'k',\n",
       " 'e',\n",
       " 'n',\n",
       " 'i',\n",
       " 'z',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'w',\n",
       " 'i',\n",
       " 't',\n",
       " 'h',\n",
       " ' ',\n",
       " 's',\n",
       " 'p',\n",
       " 'a',\n",
       " 'c',\n",
       " 'y',\n",
       " '.',\n",
       " ' ',\n",
       " 'I',\n",
       " 't',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 'w',\n",
       " 'e',\n",
       " 's',\n",
       " 'o',\n",
       " 'm',\n",
       " 'e',\n",
       " '!']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters = list(spacy_text)\n",
    "characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d227308c",
   "metadata": {},
   "source": [
    "## 2. Embeddings y Representaciones Vectoriales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f6e946",
   "metadata": {},
   "source": [
    "### Ejercicio: Cargar un modelo de word embeddings de Hugging Face (por ejemplo, glove o word2vec) y visualizar los embeddings de palabras similares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0363cd7",
   "metadata": {},
   "source": [
    "### Ejercicio: Comparar GloVe (estÃ¡tico) con BERT (dinÃ¡mico) mediante un ejemplo prÃ¡ctico en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd8259",
   "metadata": {},
   "source": [
    "## 3. Modelos Basados en Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4021a4e0",
   "metadata": {},
   "source": [
    "### 3.1 Pregunta 5: Â¿QuÃ© son las redes neuronales recurrentes (RNN) y las LSTM, y cÃ³mo se usan en NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1960ed7",
   "metadata": {},
   "source": [
    "#### 3.1.1 Ejercicio: Implementar una RNN simple para predicciÃ³n de secuencias en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415c7d9",
   "metadata": {},
   "source": [
    "### 3.2 Pregunta 6: Â¿QuÃ© son los transformers y por quÃ© han reemplazado en gran medida a las RNN en NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f9054",
   "metadata": {},
   "source": [
    "#### 3.2.1 Ejercicio: Utilizar el modelo de transformers BERT o GPT-2 de Hugging Face para completar una frase simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1fb9f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Being a computer scientist is all about\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f25d02e",
   "metadata": {},
   "source": [
    "##### 3.2.1.1 Utilizar EleutherAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c9beb236",
   "metadata": {},
   "outputs": [],
   "source": [
    "eleutherAI_generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d62a8b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Being a computer scientist is all about the ability to analyze and understand the world. The world is a'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(85)\n",
    "eleutherAI_prediction = eleutherAI_generator(text)\n",
    "eleutherAI_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748b30bd",
   "metadata": {},
   "source": [
    "#### Utilizar GPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f36f38c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "gpt2_generator = pipeline(\"text-generation\", model=\"gpt2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5592502c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Being a computer scientist is all about getting what you want. But you're still a little frustrated when you can't make progress, and those things are what drive your problems: your frustration with your work, your frustration with technology.\\n\\nThere is\"}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_prediction = gpt2_generator(text)\n",
    "gpt2_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9df00d0",
   "metadata": {},
   "source": [
    "## 4. Modelos Pre-entrenados y Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade7df70",
   "metadata": {},
   "source": [
    "### Pregunta 7: Â¿QuÃ© significa que un modelo estÃ© preentrenado? Â¿CÃ³mo se puede ajustar para una tarea especÃ­fica?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9dfe86",
   "metadata": {},
   "source": [
    "#### Ejercicio: Usar un modelo de Hugging Face (distilBERT) para clasificar un conjunto de datos de sentimientos (usar el dataset de Rotten tomatoes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10309397",
   "metadata": {},
   "source": [
    "### Pregunta 8: Â¿QuÃ© es el enmascaramiento de palabras en el preentrenamiento de modelos como BERT?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47691e9",
   "metadata": {},
   "source": [
    "#### Ejercicio: Usar BERT para predecir palabras enmascaradas en una oraciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a9c4c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Being a computer scientist is all about [MASK].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "38cd6370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "00cc48a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.11361566185951233,\n",
       "  'token': 7588,\n",
       "  'token_str': 'computers',\n",
       "  'sequence': 'being a computer scientist is all about computers.'},\n",
       " {'score': 0.06099285930395126,\n",
       "  'token': 2671,\n",
       "  'token_str': 'science',\n",
       "  'sequence': 'being a computer scientist is all about science.'},\n",
       " {'score': 0.05559048056602478,\n",
       "  'token': 2009,\n",
       "  'token_str': 'it',\n",
       "  'sequence': 'being a computer scientist is all about it.'},\n",
       " {'score': 0.048937588930130005,\n",
       "  'token': 2470,\n",
       "  'token_str': 'research',\n",
       "  'sequence': 'being a computer scientist is all about research.'},\n",
       " {'score': 0.031744930893182755,\n",
       "  'token': 8785,\n",
       "  'token_str': 'math',\n",
       "  'sequence': 'being a computer scientist is all about math.'}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_predition = bert_mask(text)\n",
    "bert_predition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870140cb",
   "metadata": {},
   "source": [
    "## 5. Tareas ClÃ¡sicas de NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e938d75",
   "metadata": {},
   "source": [
    "### 5.1 **Pregunta 9**: Â¿QuÃ© es la clasificaciÃ³n de texto y quÃ© modelos se suelen utilizar para esta tarea?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946135f3",
   "metadata": {},
   "source": [
    "#### Ejercicio: Implementar un modelo NER utilizando spaCy o Hugging Face y aplicar etiquetas a un conjunto de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff632c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a000aa1f",
   "metadata": {},
   "source": [
    "### 5.2 **Pregunta 10**: Â¿QuÃ© es el anÃ¡lisis de sentimientos y cÃ³mo se relaciona con la clasificaciÃ³n de texto?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827fa79d",
   "metadata": {},
   "source": [
    "#### Ejercicio: Utilizar Hugging Face para hacer anÃ¡lisis de sentimientos en un conjunto de datos de reseÃ±as (usar el dataset de IMDB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d55173",
   "metadata": {},
   "source": [
    "### 5.3 **Pregunta 11**: Explica el concepto de etiquetado de secuencias (named entity recognition, NER). Â¿Para quÃ© se utiliza?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15aa3f7",
   "metadata": {},
   "source": [
    "#### 5.3.1 **Ejercicio**: Implementar un modelo NER utilizando spaCy o Hugging Face y aplicar etiquetas a un conjunto de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7083953",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\"\n",
    "Trinamool Congress leader Mahua Moitra has moved the Supreme Court against her expulsion from the Lok Sabha over the cash-for-query allegations against her. Moitra was ousted from the Parliament last week after the Ethics Committee of the Lok Sabha found her guilty of jeopardising national security by sharing her parliamentary portal's login credentials with businessman Darshan Hiranandani.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e339d7da",
   "metadata": {},
   "source": [
    "##### 5.3.1.1 Utilizar spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c60f0849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "678fc631",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5191b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e695d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This word Trinamool Congress is this ORG entity.\n",
      "This word Mahua Moitra is this PERSON entity.\n",
      "This word the Supreme Court is this ORG entity.\n",
      "This word the Lok Sabha is this PERSON entity.\n",
      "This word Moitra is this ORG entity.\n",
      "This word Parliament is this ORG entity.\n",
      "This word last week is this DATE entity.\n",
      "This word the Ethics Committee is this ORG entity.\n",
      "This word Darshan Hiranandani is this PERSON entity.\n"
     ]
    }
   ],
   "source": [
    "for word in doc.ents:\n",
    "    print(f\"This word {word.text} is this {word.label_} entity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf8b521",
   "metadata": {},
   "source": [
    "#### 3.1.1.2 Utilizar Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f6ba1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f383e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\guill\\.cache\\huggingface\\hub\\models--dslim--bert-base-NER. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bert_ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6406bafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-ORG', 'score': 0.9995658, 'index': 2, 'word': 'Tri', 'start': 2, 'end': 5}, {'entity': 'I-ORG', 'score': 0.99883586, 'index': 3, 'word': '##nam', 'start': 5, 'end': 8}, {'entity': 'I-ORG', 'score': 0.99924135, 'index': 4, 'word': '##ool', 'start': 8, 'end': 11}, {'entity': 'I-ORG', 'score': 0.99926585, 'index': 5, 'word': 'Congress', 'start': 12, 'end': 20}, {'entity': 'B-PER', 'score': 0.999546, 'index': 7, 'word': 'Ma', 'start': 28, 'end': 30}, {'entity': 'I-PER', 'score': 0.772231, 'index': 8, 'word': '##hua', 'start': 30, 'end': 33}, {'entity': 'I-PER', 'score': 0.9996074, 'index': 9, 'word': 'Mo', 'start': 34, 'end': 36}, {'entity': 'I-PER', 'score': 0.99346304, 'index': 10, 'word': '##it', 'start': 36, 'end': 38}, {'entity': 'I-PER', 'score': 0.97454643, 'index': 11, 'word': '##ra', 'start': 38, 'end': 40}, {'entity': 'B-ORG', 'score': 0.99936503, 'index': 15, 'word': 'Supreme', 'start': 55, 'end': 62}, {'entity': 'I-ORG', 'score': 0.99817765, 'index': 16, 'word': 'Court', 'start': 63, 'end': 68}, {'entity': 'B-ORG', 'score': 0.9984717, 'index': 22, 'word': 'Lok', 'start': 100, 'end': 103}, {'entity': 'I-ORG', 'score': 0.9983095, 'index': 23, 'word': 'Sabha', 'start': 104, 'end': 109}, {'entity': 'B-PER', 'score': 0.9994796, 'index': 36, 'word': 'Mo', 'start': 159, 'end': 161}, {'entity': 'I-PER', 'score': 0.7041658, 'index': 37, 'word': '##it', 'start': 161, 'end': 163}, {'entity': 'I-PER', 'score': 0.7547127, 'index': 38, 'word': '##ra', 'start': 163, 'end': 165}, {'entity': 'B-ORG', 'score': 0.99915445, 'index': 49, 'word': 'Ethics', 'start': 217, 'end': 223}, {'entity': 'I-ORG', 'score': 0.9969874, 'index': 50, 'word': 'Committee', 'start': 224, 'end': 233}, {'entity': 'B-ORG', 'score': 0.99641955, 'index': 53, 'word': 'Lok', 'start': 241, 'end': 244}, {'entity': 'I-ORG', 'score': 0.99853647, 'index': 54, 'word': 'Sabha', 'start': 245, 'end': 250}, {'entity': 'B-PER', 'score': 0.99957997, 'index': 77, 'word': 'Dar', 'start': 375, 'end': 378}, {'entity': 'B-PER', 'score': 0.78188264, 'index': 78, 'word': '##shan', 'start': 378, 'end': 382}, {'entity': 'I-PER', 'score': 0.9996786, 'index': 79, 'word': 'Hi', 'start': 383, 'end': 385}, {'entity': 'I-PER', 'score': 0.99624854, 'index': 80, 'word': '##rana', 'start': 385, 'end': 389}, {'entity': 'I-PER', 'score': 0.99812657, 'index': 81, 'word': '##nda', 'start': 389, 'end': 392}]\n"
     ]
    }
   ],
   "source": [
    "ner_results = bert_ner(text)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbdc21a",
   "metadata": {},
   "source": [
    "## 6. GeneraciÃ³n de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d1fd75",
   "metadata": {},
   "source": [
    "### 6.1 **Pregunta 12**: Â¿CÃ³mo funcionan los modelos de generaciÃ³n de texto como GPT-2 o GPT-3?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bb2cbf",
   "metadata": {},
   "source": [
    "#### 6.1.1 **Ejercicio**: Usar Hugging Face para generar texto a partir de un prompt con GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9fa9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc0d3e2b",
   "metadata": {},
   "source": [
    "### 6.2 **Pregunta 13**: Â¿CuÃ¡les son los desafÃ­os Ã©ticos asociados con los modelos de generaciÃ³n de texto?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c004df",
   "metadata": {},
   "source": [
    "#### 6.2.1 **Ejercicio**: Discutir sobre sesgos en los modelos de lenguaje generativo y su impacto en aplicaciones reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20f0ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bae312f3",
   "metadata": {},
   "source": [
    "## 7. EvaluaciÃ³n de Modelos NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcff8ceb",
   "metadata": {},
   "source": [
    "### 7.1 **Pregunta 14**: Â¿QuÃ© mÃ©tricas se utilizan para evaluar la calidad de los modelos de NLP en tareas como clasificaciÃ³n o generaciÃ³n de texto?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd0515c",
   "metadata": {},
   "source": [
    "#### 7.1.1 **Ejercicio**: Usar mÃ©tricas como accuracy, precision, y recall para evaluar un modelo de clasificaciÃ³n de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfffb9d9",
   "metadata": {},
   "source": [
    "##### 7.1.1.1 Guardar las mÃ©tricas de accuracy, precision y recall en una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "658e60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"accuracy\", \"recall\", \"precision\", \"f1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4081e248",
   "metadata": {},
   "source": [
    "##### 7.1.1.2 Cargar los datos de imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1d121ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"imdb\", split=\"test\").shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663f3b2a",
   "metadata": {},
   "source": [
    "##### 7.1.1.3 Cargar el evalvador para clasificaciÃ³n de textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e8a231c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_evaluator = evaluator(\"text-classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb264e1",
   "metadata": {},
   "source": [
    "##### 7.1.1.4 Cargar el modelo de clasificaciÃ³n de textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "179a2456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "imdb_classification = pipeline(\"text-classification\", model=\"lvwerra/distilbert-imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f66c7",
   "metadata": {},
   "source": [
    "##### 7.1.1.5 Evaluar los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ce349859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"accuracy\": 0.918,\n",
      "    \"recall\": 0.9180327868852459,\n",
      "    \"precision\": 0.9142857142857143,\n",
      "    \"f1\": 0.9161554192229039,\n",
      "    \"total_time_in_seconds\": 119.51120690000243,\n",
      "    \"samples_per_second\": 8.36741612723166,\n",
      "    \"latency_in_seconds\": 0.11951120690000243\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline = imdb_classification,\n",
    "    data = data,\n",
    "    metric = evaluate.combine(metrics),\n",
    "    label_mapping = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    ")\n",
    "print(json.dumps(eval_results, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dfc506",
   "metadata": {},
   "source": [
    "### 7.2 **Pregunta 15**: Â¿QuÃ© es la perplejidad y cÃ³mo se utiliza para evaluar modelos de generaciÃ³n de texto?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e460386",
   "metadata": {},
   "source": [
    "#### 7.2.1 **Ejercicio**: Calcular la perplejidad de un modelo de lenguaje en Hugging Face para una tarea de generaciÃ³n de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a82a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
