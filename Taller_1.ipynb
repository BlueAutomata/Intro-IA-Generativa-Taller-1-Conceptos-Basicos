{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90fd06a3",
   "metadata": {},
   "source": [
    "# Taller 1 Conceptos bÃ¡sicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274752a1",
   "metadata": {},
   "source": [
    "Autores:\n",
    "- Guillermo Luigui Ubaldo Nieto Angarita\n",
    "- Daniel Hernando Moyano Salamanca\n",
    "- Jairo Antonio Viteri Rojas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7f2bfc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\guill\\Documents\\GitHub\\Intro-IA-Generativa-Taller-1-Conceptos-Basicos>() \n"
     ]
    }
   ],
   "source": [
    "!myenv\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a46b9e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.2.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: transformers==4.44.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 3)) (4.44.2)\n",
      "Requirement already satisfied: nltk==3.9.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 4)) (3.9.1)\n",
      "Requirement already satisfied: spacy==3.7.5 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 5)) (3.7.5)\n",
      "Requirement already satisfied: nbdime==4.0.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 7)) (4.0.2)\n",
      "Requirement already satisfied: datasets==3.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 8)) (3.0.1)\n",
      "Requirement already satisfied: model2vec in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 9)) (0.2.3)\n",
      "Requirement already satisfied: evaluate==0.4.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 10)) (0.4.3)\n",
      "Requirement already satisfied: tf-keras==2.16 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 11)) (2.16.0)\n",
      "Requirement already satisfied: scipy==1.13.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 12)) (1.13.1)\n",
      "Requirement already satisfied: gensim==4.3.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 13)) (4.3.3)\n",
      "Requirement already satisfied: matplotlib==3.7.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 14)) (3.7.1)\n",
      "Requirement already satisfied: tqdm==4.66.5 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 15)) (4.66.5)\n",
      "Requirement already satisfied: torch in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 16)) (2.4.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 17)) (0.19.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==2.2.3->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==2.2.3->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==2.2.3->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (0.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (0.19.1)\n",
      "Requirement already satisfied: click in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk==3.9.1->-r requirements.txt (line 4)) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk==3.9.1->-r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (0.12.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (2.6.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (65.5.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (3.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: gitpython!=2.1.4,!=2.1.5,!=2.1.6 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 7)) (3.1.43)\n",
      "Requirement already satisfied: jupyter-server in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 7)) (2.14.2)\n",
      "Requirement already satisfied: jupyter-server-mathjax>=0.2.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 7)) (0.2.6)\n",
      "Requirement already satisfied: nbformat in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 7)) (5.10.4)\n",
      "Requirement already satisfied: pygments in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 7)) (2.18.0)\n",
      "Requirement already satisfied: tornado in c:\\users\\guill\\appdata\\roaming\\python\\python311\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 7)) (6.4.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets==3.0.1->-r requirements.txt (line 8)) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets==3.0.1->-r requirements.txt (line 8)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets==3.0.1->-r requirements.txt (line 8)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets==3.0.1->-r requirements.txt (line 8)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==3.0.1->-r requirements.txt (line 8)) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets==3.0.1->-r requirements.txt (line 8)) (3.10.9)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tf-keras==2.16->-r requirements.txt (line 11)) (2.16.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim==4.3.3->-r requirements.txt (line 13)) (7.0.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib==3.7.1->-r requirements.txt (line 14)) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib==3.7.1->-r requirements.txt (line 14)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib==3.7.1->-r requirements.txt (line 14)) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib==3.7.1->-r requirements.txt (line 14)) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib==3.7.1->-r requirements.txt (line 14)) (10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib==3.7.1->-r requirements.txt (line 14)) (3.0.9)\n",
      "Requirement already satisfied: rich in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from model2vec->-r requirements.txt (line 9)) (13.9.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from model2vec->-r requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->-r requirements.txt (line 16)) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->-r requirements.txt (line 16)) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->-r requirements.txt (line 16)) (3.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets==3.0.1->-r requirements.txt (line 8)) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets==3.0.1->-r requirements.txt (line 8)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets==3.0.1->-r requirements.txt (line 8)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets==3.0.1->-r requirements.txt (line 8)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets==3.0.1->-r requirements.txt (line 8)) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets==3.0.1->-r requirements.txt (line 8)) (1.14.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gitpython!=2.1.4,!=2.1.5,!=2.1.6->nbdime==4.0.2->-r requirements.txt (line 7)) (4.0.11)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy==3.7.5->-r requirements.txt (line 5)) (2.1.5)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (4.3.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (23.1.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in c:\\users\\guill\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\guill\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (5.7.2)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (0.5.3)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (7.16.4)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (0.21.0)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (2.0.13)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\guill\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (26.2.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (0.18.1)\n",
      "Requirement already satisfied: traitlets>=5.6.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (5.14.3)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (1.8.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.5->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbformat->nbdime==4.0.2->-r requirements.txt (line 7)) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbformat->nbdime==4.0.2->-r requirements.txt (line 7)) (4.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5->-r requirements.txt (line 5)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5->-r requirements.txt (line 5)) (2.16.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.2.3->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.44.2->-r requirements.txt (line 3)) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.44.2->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.44.2->-r requirements.txt (line 3)) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.44.2->-r requirements.txt (line 3)) (2023.7.22)\n",
      "Requirement already satisfied: wrapt in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open>=1.8.1->gensim==4.3.3->-r requirements.txt (line 13)) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (4.25.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (2.5.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (3.6.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (0.31.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.5->-r requirements.txt (line 5)) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.5->-r requirements.txt (line 5)) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.5->-r requirements.txt (line 5)) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->model2vec->-r requirements.txt (line 9)) (3.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.5->-r requirements.txt (line 5)) (0.19.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->model2vec->-r requirements.txt (line 9)) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch->-r requirements.txt (line 16)) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio>=3.1.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (21.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=2.1.4,!=2.1.5,!=2.1.6->nbdime==4.0.2->-r requirements.txt (line 7)) (5.0.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat->nbdime==4.0.2->-r requirements.txt (line 7)) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat->nbdime==4.0.2->-r requirements.txt (line 7)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat->nbdime==4.0.2->-r requirements.txt (line 7)) (0.20.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (4.2.2)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\guill\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (306)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (0.1.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.5->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->model2vec->-r requirements.txt (line 9)) (0.1.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (0.10.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (1.5.1)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==3.0.1->-r requirements.txt (line 8)) (0.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (0.44.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (0.5.1)\n",
      "Requirement already satisfied: fqdn in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (3.0.0)\n",
      "Requirement already satisfied: uri-template in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (24.8.0)\n",
      "Requirement already satisfied: namex in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (0.13.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 11)) (3.0.4)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (2.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 7)) (2.9.0.20241003)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "efa107ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import nltk\n",
    "import spacy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from transformers import pipeline, set_seed, DataCollatorWithPadding, BertForSequenceClassification, Trainer, TrainingArguments, BertTokenizer, BertModel, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer\n",
    "from scipy.spatial.distance import cosine\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from evaluate import evaluator\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from tf_keras.api._v2.keras.layers import Input, Dense, SimpleRNN\n",
    "from tf_keras.api._v2.keras.models import Model\n",
    "from tf_keras.api._v2.keras.optimizers import SGD\n",
    "from tf_keras.api._v2.keras.utils import to_categorical\n",
    "from tf_keras.api._v2.keras import backend as K\n",
    "\n",
    "from model2vec import StaticModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "eec24ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c612c8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 5.6 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.1/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 5.0 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.7/12.8 MB 4.7 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 6.0/12.8 MB 5.0 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 5.1 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.9/12.8 MB 5.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.5/12.8 MB 5.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 6.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 5.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.2)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "\u001b[38;5;2mâ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "042ff67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove.6B.50d.txt already exists. No download needed.\n"
     ]
    }
   ],
   "source": [
    "# Define the URL and the output file path\n",
    "url = \"http://nlp.uoregon.edu/download/embeddings/glove.6B.50d.txt\"\n",
    "output_file = \"glove.6B.50d.txt\"\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.exists(output_file):\n",
    "    print(f\"Downloading {output_file}...\")\n",
    "    # Stream the download to handle large files\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(output_file, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "    print(f\"Downloaded {output_file}.\")\n",
    "else:\n",
    "    print(f\"{output_file} already exists. No download needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb0a2e",
   "metadata": {},
   "source": [
    "## 1. IntroducciÃ³n a NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce6930",
   "metadata": {},
   "source": [
    "### 1.1. Pregunta 1: Â¿QuÃ© es el Procesamiento de Lenguaje Natural y cuÃ¡les son sus principales aplicaciones?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a6fe09",
   "metadata": {},
   "source": [
    "El procesamiento del lenguaje natural (NLP) es un campo de la IA que permite a las mÃ¡quinas comprender, interpretar, generar y manipular el lenguaje humano de forma natural, lo que facilita las interacciones entre humanos y ordenadores.\n",
    "\n",
    "El NLP utiliza modelos estadÃ­sticos, tÃ©cnicas de aprendizaje automÃ¡tico y, mÃ¡s recientemente, arquitecturas de Deep Learning como Transformers (por ejemplo, BERT y GPT). Estos modelos convierten el texto en representaciones matemÃ¡ticas, como embeddings, permitiendo que las mÃ¡quinas detecten patrones, relaciones semÃ¡nticas y contexto.\n",
    "\n",
    "Las aplicaciones principales del PLN incluyen:\n",
    "- **AnÃ¡lisis de Sentimientos**: Detectar la polaridad (positiva, negativa o neutra) en textos como reseÃ±as, redes sociales o encuestas.\n",
    "- **Chatbots y Asistentes Virtuales**: Automatizar la interacciÃ³n con los usuarios mediante respuestas precisas y contextuales.\n",
    "- **TraducciÃ³n AutomÃ¡tica**: Transformar texto de un idioma a otro utilizando redes neuronales para mantener coherencia y fluidez.\n",
    "- **Resumen AutomÃ¡tico de Textos**: Condensar informaciÃ³n larga en versiones mÃ¡s breves sin perder los puntos clave.\n",
    "- **DetecciÃ³n de Spam y Filtrado de Contenidos**: Clasificar mensajes o contenido inapropiado para mejorar la experiencia del usuario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc47014",
   "metadata": {},
   "source": [
    "### 1.1.1 Ejercicio: Investigar 6 aplicaciones actuales de NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f7ea2",
   "metadata": {},
   "source": [
    "- **ChatGPT (Asistente Conversacional Generativo)**: es un modelo de lenguaje basado en la arquitectura GPT-4. Se utiliza en mÃºltiples sectores para ofrecer atenciÃ³n automatizada a clientes, resolver preguntas complejas y brindar soporte tÃ©cnico. Su capacidad de comprender y mantener conversaciones contextuales mejora la eficiencia y reduce los tiempos de respuesta en empresas de servicios financieros, comercio electrÃ³nico y salud. Mejora la experiencia del cliente al ofrecer atenciÃ³n continua y personalizada, reduciendo la carga de trabajo en los centros de atenciÃ³n al cliente.\n",
    "\n",
    "- **DALL-E3 (GeneraciÃ³n de ImÃ¡genes a partir de Texto)**: Aunque se centra en la generaciÃ³n de imÃ¡genes, DALL-E combina PLN con modelos de visiÃ³n computacional para interpretar descripciones complejas y convertirlas en representaciones visuales. Esta tecnologÃ­a tiene aplicaciones en marketing, diseÃ±o y educaciÃ³n. Permite crear contenido visual sin necesidad de conocimientos avanzados en diseÃ±o, optimizando la producciÃ³n creativa en campaÃ±as publicitarias y proyectos educativos.\n",
    "\n",
    "- **MedPaLM (Modelo especializado en PLN para salud)**: MedPaLM es un modelo optimizado para procesar informaciÃ³n mÃ©dica, responder preguntas clÃ­nicas y brindar recomendaciones basadas en evidencia. EstÃ¡ diseÃ±ado para manejar textos tÃ©cnicos y resolver preguntas complejas en medicina. Mejora la eficiencia de los profesionales de la salud al proporcionar resÃºmenes rÃ¡pidos y precisos de investigaciones, diagnÃ³sticos y tratamientos, facilitando la toma de decisiones en entornos clÃ­nicos.\n",
    "\n",
    "- **Asistentes Virtuales de Servicio al Cliente (AVSC)**: son sistemas alimentados por modelos de PLN para ofrecer soporte al cliente 24/7. Pueden responder preguntas, resolver problemas comunes y guiar a los usuarios a travÃ©s de procesos complejos. Los AVSC aumentan la eficiencia operativa al reducir el tiempo de espera y atender mÃºltiples consultas simultÃ¡neamente, permitiendo a las empresas ofrecer un mejor servicio sin aumentar la carga de trabajo en el personal. Los asistentes virtuales ayudan a reducir costos operativos y mejoran la satisfacciÃ³n del cliente al proporcionar respuestas rÃ¡pidas y precisas.\n",
    "\n",
    "- **TraducciÃ³n AutomÃ¡tica Avanzada**: Servicios como Google Translate han mejorado notablemente gracias a los avances en PLN. Utilizan modelos de traducciÃ³n automÃ¡tica neuronales que comprenden el contexto y las sutilezas del lenguaje humano. Estos sistemas facilitan la comunicaciÃ³n entre personas que hablan diferentes idiomas, rompiendo barreras lingÃ¼Ã­sticas en negocios, turismo y relaciones personales; ayudan a prevenir malentendidos en la comunicaciÃ³n y permiten el acceso a informaciÃ³n y servicios en mÃºltiples idiomas.\n",
    "\n",
    "- **AnÃ¡lisis de Sentimientos en Redes Sociales**: herramientas tecnolÃ³gicas que utilizan tÃ©cnicas de PLN para evaluar las opiniones y emociones expresadas en redes sociales y reseÃ±as en lÃ­nea. Estas herramientas pueden clasificar el contenido como positivo, negativo o neutral. Permiten a las empresas y organizaciones comprender mejor la percepciÃ³n pÃºblica sobre sus productos, servicios o iniciativas. Ayudan a las marcas a identificar problemas emergentes y Ã¡reas de mejora en sus ofertas, a la vez que permiten a los responsables de marketing ajustar estrategias en tiempo real basÃ¡ndose en la retroalimentaciÃ³n del consumidor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb28c41",
   "metadata": {},
   "source": [
    "### 1.2. Pregunta 2: Explica los conceptos de tokenizaciÃ³n y segmentaciÃ³n de oraciones. Â¿Por quÃ© son importantes en NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed4a79",
   "metadata": {},
   "source": [
    "La tokenizaciÃ³n es el proceso de dividir un texto en unidades mÃ¡s pequeÃ±as llamadas tokens. Esto puede implicar descomponer el texto en oraciones, palabras, subpalabras o incluso caracteres. Es importante porque permite convertir un texto no estructurado en uno estructurado, lo que facilita su anÃ¡lisis estadÃ­stico o su uso en modelos de machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4237df",
   "metadata": {},
   "source": [
    "#### 1.2.1 Ejercicio: Usar la biblioteca nltk o spaCy en Python para tokenizar un texto corto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb7a8f",
   "metadata": {},
   "source": [
    "##### 1.2.1.1 Ejercicio con Nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a299848",
   "metadata": {},
   "source": [
    "##### 1.2.1.1.1 Escribir un texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "31209251",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text = \"I am learning with Nltk. It is awesome!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b13422",
   "metadata": {},
   "source": [
    "##### 1.2.1.1.2 Tokenizar por oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bead825c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentance list: ['I am learning with Nltk.', 'It is awesome!'].\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(nltk_text)\n",
    "print(f\"Sentance list: {sentences}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638e2c3d",
   "metadata": {},
   "source": [
    "##### 1.2.1.1.3 Tokenizar por palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "073b19e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word list: ['I', 'am', 'learning', 'with', 'Nltk', '.', 'It', 'is', 'awesome', '!'].\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(nltk_text)\n",
    "print(f\"Word list: {words}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994196ea",
   "metadata": {},
   "source": [
    "##### 1.2.1.1.4 Tokenizar por subpalabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e8ab81ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subword list: ['i', 'am', 'learn', 'with', 'nltk', '.', 'it', 'is', 'awesom', '!'].\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer_subwords = [stemmer.stem(word) for word in words]\n",
    "print(f\"Subword list: {stemmer_subwords}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f94dc2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subword list: ['I', 'am', 'learning', 'with', 'Nltk', '.', 'It', 'is', 'awesome', '!'].\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer_subwords = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(f\"Subword list: {lemmatizer_subwords}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1826e77f",
   "metadata": {},
   "source": [
    "###### 1.2.1.1.5 Tokenizar por caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "34448aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character List: ['I', ' ', 'a', 'm', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'w', 'i', 't', 'h', ' ', 'N', 'l', 't', 'k', '.', ' ', 'I', 't', ' ', 'i', 's', ' ', 'a', 'w', 'e', 's', 'o', 'm', 'e', '!']\n"
     ]
    }
   ],
   "source": [
    "characters = list(nltk_text)\n",
    "print(f\"Character List: {characters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e09289",
   "metadata": {},
   "source": [
    "#### 1.2.1.2 Ejercicio con spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806836f2",
   "metadata": {},
   "source": [
    "##### 1.2.1.2.1 Cargar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8d770c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fcb12428",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_text = \"I am learning how to do tokenization with spacy. It is awesome!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ff991372",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(spacy_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d2e799",
   "metadata": {},
   "source": [
    "##### 1.2.1.2.2 Tokenizar por oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a1f94bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I am learning how to do tokenization with spacy., It is awesome!]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "for sentence in doc.sents:\n",
    "    sentences.append(sentence)\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab2178",
   "metadata": {},
   "source": [
    "##### 1.2.1.2.2 Tokenizar por words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ab00e897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'how',\n",
       " 'to',\n",
       " 'do',\n",
       " 'tokenization',\n",
       " 'with',\n",
       " 'spacy',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'awesome',\n",
       " '!']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for token in doc:\n",
    "    words.append(token.text)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db6c0b9",
   "metadata": {},
   "source": [
    "###### 1.2.1.2.3 Tokenizar por caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5736e1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " ' ',\n",
       " 'a',\n",
       " 'm',\n",
       " ' ',\n",
       " 'l',\n",
       " 'e',\n",
       " 'a',\n",
       " 'r',\n",
       " 'n',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'h',\n",
       " 'o',\n",
       " 'w',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'd',\n",
       " 'o',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " 'k',\n",
       " 'e',\n",
       " 'n',\n",
       " 'i',\n",
       " 'z',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'w',\n",
       " 'i',\n",
       " 't',\n",
       " 'h',\n",
       " ' ',\n",
       " 's',\n",
       " 'p',\n",
       " 'a',\n",
       " 'c',\n",
       " 'y',\n",
       " '.',\n",
       " ' ',\n",
       " 'I',\n",
       " 't',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 'w',\n",
       " 'e',\n",
       " 's',\n",
       " 'o',\n",
       " 'm',\n",
       " 'e',\n",
       " '!']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters = list(spacy_text)\n",
    "characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d227308c",
   "metadata": {},
   "source": [
    "## 2. Embeddings y Representaciones Vectoriales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da688d83",
   "metadata": {},
   "source": [
    "### 2.1 Pregunta 3: Â¿QuÃ© es un embedding de palabras y cÃ³mo representa el significado de una palabra en el espacio vectorial?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7f806d",
   "metadata": {},
   "source": [
    "Las incrustaciones de palabras (Embeddings) son representaciones vectoriales densas de palabras que capturan sus significados, propiedades sintÃ¡cticas y relaciones con otras palabras. las incrustaciones de palabras mapean palabras en un espacio vectorial continuo donde las palabras semÃ¡nticamente similares se encuentran cerca unas de otras.\n",
    "Un embedding es simplemente un vector de longitud fija, por ejemplo, de 100 o 300 dimensiones. Cada palabra se representa como un punto en este espacio de alta dimensionalidad, y las relaciones entre las palabras se pueden medir usando distancias o Ã¡ngulos entre los vectores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f6e946",
   "metadata": {},
   "source": [
    "#### 2.1.1 Ejercicio: Cargar un modelo de word embeddings de Hugging Face (por ejemplo, glove o word2vec) y visualizar los embeddings de palabras similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "231226db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained Model2Vec model\n",
    "model = StaticModel.from_pretrained(\"minishlab/M2V_base_glove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ce9fb547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the words you want to compare\n",
    "word1 = \"table\"\n",
    "word2 = \"chair\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5d68c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings for both words\n",
    "embeddings = model.encode([word1, word2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "80fa89b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'table' and 'chair': 0.26632168889045715\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarity\n",
    "similarity = cosine_similarity([embeddings[0]], [embeddings[1]])\n",
    "print(f\"Cosine similarity between '{word1}' and '{word2}': {similarity[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661ee107",
   "metadata": {},
   "source": [
    "### 2.2 Pregunta 4: Explica las diferencias entre embeddings estÃ¡ticos y dinÃ¡micos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3343df61",
   "metadata": {},
   "source": [
    "**Los embeddings estÃ¡ticos**, como GloVe o Word2Vec, generan una representaciÃ³n vectorial fija para cada palabra en el vocabulario. Estas representaciones se calculan una vez y no cambian segÃºn el contexto en el que se encuentra la palabra y tienen las siguientes caracterÃ­sticas:\n",
    "\n",
    "- **Contexto fijo**: Cada palabra tiene una Ãºnica representaciÃ³n (por ejemplo, \"banco\" como entidad financiera y \"banco\" como lugar para sentarse tienen la misma representaciÃ³n).\n",
    "\n",
    "- **RÃ¡pido**: Son computacionalmente mÃ¡s eficientes y requieren menos recursos de memoria.\n",
    "\n",
    "- **Limitaciones**: No capturan el significado contextual que puede variar en diferentes oraciones.\n",
    "\n",
    "Por otra parte, los **embeddings dinÃ¡micos** como BERT (Bidirectional Encoder Representations from Transformers), generan representaciones que dependen del contexto en que se usa la palabra. La ubicaciÃ³n de la palabra dentro de una frase influye en su vector de representaciÃ³n y tienen las siguientes caracterÃ­sticas:\n",
    "\n",
    "- **Contexto variable**: Cada palabra puede tener mÃºltiples representaciones dependiendo de su contexto; por ejemplo, \"banco\" en \"fui al banco\" vs. \"me sentÃ© en el banco\".\n",
    "\n",
    "- **MÃ¡s preciso**: Capturan significados mÃ¡s matizados y complejos, lo que resulta en una mejor comprensiÃ³n semÃ¡ntica.\n",
    "- **Limitaciones**: mayor carga computacional porque requieren mÃ¡s recursos para calcular y almacenar, y suelen ser mÃ¡s lentos debido a su complejidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0363cd7",
   "metadata": {},
   "source": [
    "#### 2.2.1 Ejercicio: Comparar GloVe (estÃ¡tico) con BERT (dinÃ¡mico) mediante un ejemplo prÃ¡ctico en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992484fc",
   "metadata": {},
   "source": [
    "#### 2.2.1.1 Definir frases que se van a utilizar para la comparaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e34eeb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The bank is near the river.\",\n",
    "    \"I need to open a bank account.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b70ff5",
   "metadata": {},
   "source": [
    "#### 2.2.1.2 Cargar modelo BERT para embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e1421b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a61bfb",
   "metadata": {},
   "source": [
    "#### 2.2.1.3 Cargar GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f56a463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "glove_path = \"glove.6B.50d.txt\"  # Actualiza esta ruta a tu archivo de GloVe\n",
    "glove_model = KeyedVectors.load_word2vec_format(glove_path, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a882c314",
   "metadata": {},
   "source": [
    "#### 2.2.1.4 FunciÃ³n para obtener la representaciÃ³n de BERT de la palabra \"bank\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f1bc941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(word, sentences):\n",
    "    # Tokenizamos las frases\n",
    "    inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Extraer el Ã­ndice de la palabra \"bank\" y calcular su embedding\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    bank_embeddings = []\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        token_ids = inputs['input_ids'][i]\n",
    "        # Obtener el Ã­ndice del token \"bank\"\n",
    "        bank_token_index = torch.where(token_ids == tokenizer.encode(word, add_special_tokens=False)[0])[0]\n",
    "        if len(bank_token_index) > 0:\n",
    "            # Usar el primer token \"bank\" en caso de que haya subpalabras\n",
    "            bank_embedding = embeddings[i][bank_token_index[0]]\n",
    "            bank_embeddings.append(bank_embedding)\n",
    "    \n",
    "    return bank_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50314f22",
   "metadata": {},
   "source": [
    "#### 2.2.1.5 FunciÃ³n para obtener la representaciÃ³n de GloVe de la palabra \"bank\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b6b7e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_embedding(word):\n",
    "    return glove_model[word] if word in glove_model else np.zeros((50,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb38cc4",
   "metadata": {},
   "source": [
    "#### 2.2.1.6 Obtener embeddings de \"bank\" usando BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "42c00e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_embeddings_bert = get_bert_embedding(\"bank\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fbf78e",
   "metadata": {},
   "source": [
    "#### 2.2.1.7 Obtener el embedding de \"bank\" usando GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8dee21c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_embedding_glove = get_glove_embedding(\"bank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e714097",
   "metadata": {},
   "source": [
    "#### 2.2.1.8 Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "868ff0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding de 'bank' usando BERT (para 'The bank is near the river.'):\n",
      "[-4.15259391e-01 -4.03172016e-01 -1.56830207e-01 -1.48958042e-01\n",
      "  6.45179331e-01  1.56543210e-01 -2.39042610e-01  5.84185302e-01\n",
      "  1.75475270e-01 -3.09580535e-01  5.02860487e-01  4.69578952e-02\n",
      "  2.99013890e-02  1.61932245e-01 -1.04944110e+00  1.39990002e-01\n",
      "  2.48269081e-01  9.99445692e-02  7.20661461e-01 -7.00905100e-02\n",
      " -1.06558166e-02  2.31066838e-01  5.12047336e-02 -3.61906700e-02\n",
      "  1.88942119e-01  1.89230755e-01  1.26893699e-01  5.60914397e-01\n",
      "  1.61382228e-01  9.01738629e-02  8.60862851e-01  3.07804607e-02\n",
      "  4.03439730e-01  2.39335448e-01 -3.18555176e-01 -2.12430239e-01\n",
      " -2.50645727e-02 -8.41783807e-02 -4.26190972e-01 -7.48774260e-02\n",
      "  3.24836910e-01 -3.09946060e-01 -3.60662490e-01  4.18322623e-01\n",
      " -2.27934837e-01 -3.88941944e-01 -2.87400961e-01  1.61438018e-01\n",
      " -1.42638907e-01  3.23887169e-01 -4.21454281e-01  3.96718830e-01\n",
      " -3.54039967e-01  4.15670872e-03  2.78093576e-01  4.79878694e-01\n",
      " -2.82103240e-01 -2.87813917e-02  1.97949618e-01 -1.77089304e-01\n",
      "  5.58946311e-01 -2.31101289e-01  3.84769380e-01 -1.26373664e-01\n",
      " -3.72296244e-01  7.19815791e-02 -2.77452528e-01 -1.12606138e-01\n",
      "  5.09608865e-01 -1.48239553e-01 -1.35282680e-01  6.87282145e-01\n",
      " -3.20654273e-01 -4.91646558e-01 -1.04748197e-01 -2.84687787e-01\n",
      " -1.80054665e-01  4.92520690e-01  1.40273154e-01 -1.89122185e-01\n",
      "  1.90155134e-01 -6.85138524e-01 -2.25909978e-01 -1.06216475e-01\n",
      "  3.17265332e-01  1.34814993e-01 -1.91662461e-01 -1.50887758e-01\n",
      " -1.32010892e-01  4.35010195e-01 -5.11070013e-01  1.21653087e-01\n",
      " -2.79054433e-01 -2.54363846e-02 -2.93008685e-01 -5.35631835e-01\n",
      " -1.25747412e-01 -1.85048446e-01  4.32890743e-01  1.36594567e-02\n",
      "  4.44327593e-01 -1.07857203e+00 -5.33795767e-02  5.66775858e-01\n",
      " -3.76496881e-01 -6.28616333e-01 -3.72382611e-01 -2.86927760e-01\n",
      "  2.78450698e-01 -4.26507920e-01 -3.55675578e-01  3.37116718e-01\n",
      "  3.13832253e-01  1.04616910e-01 -8.96579549e-02  5.39796948e-01\n",
      "  4.15291399e-01 -4.01806384e-01 -8.78178954e-01  3.25306915e-02\n",
      "  5.08871317e-01  3.69341612e-01  2.35049814e-01  3.40793908e-01\n",
      "  3.61425787e-01 -3.99397820e-01 -1.55650511e-01  1.01011109e+00\n",
      " -2.21769512e-01  1.31968707e-01  4.16887909e-01 -1.37616366e-01\n",
      "  4.76226583e-02  8.18480402e-02  4.16783929e-01 -5.46125285e-02\n",
      " -4.93400060e-02 -4.25429285e-01 -9.50879008e-02 -3.02496180e-02\n",
      " -5.41686833e-01  3.39709550e-01  2.67697632e-01  4.20249820e-01\n",
      "  5.03304422e-01  4.03434962e-01  1.11333862e-01 -7.48423398e-01\n",
      " -2.38072231e-01  8.09822232e-02 -5.37901223e-02  1.82195053e-01\n",
      " -1.24165937e-02 -5.15343726e-01 -2.93530613e-01  3.53481621e-01\n",
      "  2.84961939e-01  4.49363887e-03 -6.70958161e-01  4.43480760e-01\n",
      "  8.47232521e-01 -3.95959392e-02  2.11142167e-01  2.98803926e-01\n",
      "  2.25975916e-01 -4.51899618e-01  4.78064120e-01  2.13519305e-01\n",
      " -3.74259293e-01  5.28122425e-01 -6.02392972e-01 -3.56583506e-01\n",
      "  1.46839947e-01 -1.44181237e-01  6.74453795e-01 -3.13345701e-01\n",
      " -1.31485999e-01 -3.83441031e-01 -2.58359790e-01  3.06553602e-01\n",
      " -4.14115608e-01  7.33712196e-01  2.36034155e-01  8.27977777e-01\n",
      " -3.03145140e-01  3.00751805e-01  5.17832279e-01 -1.32586911e-01\n",
      "  2.95913190e-01  2.93175519e-01  1.84279412e-01 -5.89539468e-01\n",
      " -5.47409534e-01 -7.04187825e-02 -8.22966695e-02 -6.64169550e-01\n",
      " -1.89713418e-01 -2.80871540e-01 -3.33249539e-01  4.99769710e-02\n",
      " -1.09504849e-01 -5.79630733e-01  3.82285178e-01 -1.19997606e-01\n",
      " -3.70902032e-01  5.25990486e-01  4.20102775e-01 -4.23773825e-01\n",
      "  9.72659171e-01 -3.33023489e-01 -3.99537742e-01  2.20193356e-01\n",
      " -3.76163840e-01 -1.64819807e-02  4.76074964e-01 -2.47620761e-01\n",
      " -2.89153337e-01 -6.11923456e-01  2.90079176e-01 -1.77492142e-01\n",
      "  4.41102386e-01 -4.02747430e-02  6.37757897e-01  4.69610572e-01\n",
      "  3.19396108e-02  3.20343882e-01  3.55123311e-01  3.24276388e-02\n",
      " -4.13509876e-01  9.67798457e-02 -4.83947188e-01  8.03830147e-01\n",
      "  3.20358306e-01  7.46612176e-02 -4.11689371e-01 -7.04910979e-02\n",
      " -1.11743286e-01  1.62910685e-01 -1.44784912e-01  7.17299223e-01\n",
      " -9.98433530e-02  3.14331293e-01 -2.20189303e-01 -1.23210385e-01\n",
      "  3.10729861e-01 -1.20772198e-01  8.14330876e-01 -2.15013847e-02\n",
      " -5.61341882e-01 -2.77780950e-01  5.76655418e-02  6.14752844e-02\n",
      " -1.24003058e-02 -3.32971573e-01  2.85153419e-01  1.65898710e-01\n",
      " -3.62410605e-01  2.72043288e-01 -2.56061912e-01 -1.36390217e-02\n",
      "  2.39939988e-02  8.56066421e-02  3.16362888e-01 -2.36395031e-01\n",
      " -5.90122342e-01 -1.33399859e-01  9.08599645e-02  1.35169297e-01\n",
      "  4.71794575e-01  3.62731040e-01  1.16254568e-01  2.46915072e-01\n",
      " -2.50524998e-01  6.51488781e-01 -2.26075321e-01  1.53435484e-01\n",
      "  3.82350296e-01 -2.11000163e-02 -4.01064098e-01 -3.59485090e-01\n",
      "  2.81797051e-01  2.58479089e-01  5.47638163e-03 -3.39144588e-01\n",
      "  5.01236320e-01 -1.08580053e-01  2.09572941e-01  3.25755656e-01\n",
      " -6.08563244e-01 -5.40915608e-01 -4.39565144e-02  3.81800741e-01\n",
      " -2.76013285e-01 -1.45987794e-01  7.99600661e-01  3.86528701e-01\n",
      " -1.31881267e-01  2.17189670e-01  7.00082839e-01  2.79002219e-01\n",
      "  4.89323437e-02  3.77119541e-01  4.57467794e-01 -6.01492643e-01\n",
      "  3.53535593e-01 -2.58601513e-02 -8.52721334e-02  1.78065434e-01\n",
      " -5.97683716e+00 -9.05884281e-02 -3.13488275e-01  8.97825211e-02\n",
      "  6.08140230e-01 -1.41507685e-01  7.03659713e-01  2.96685517e-01\n",
      " -5.60529649e-01 -2.21754193e-01 -2.27266639e-01 -3.08434844e-01\n",
      "  1.30122870e-01  3.40949371e-03  2.74462372e-01 -1.64029300e-02\n",
      "  4.06502724e-01  2.19108000e-01  2.01474354e-01 -9.63398814e-02\n",
      " -9.34192896e-01 -2.75161207e-01 -6.43824637e-02  1.46352232e-01\n",
      " -4.64265533e-02  7.07442343e-01 -9.35859561e-01  3.65929723e-01\n",
      " -4.60193530e-02 -2.46436909e-01 -1.94515154e-01 -2.67605424e-01\n",
      "  1.90710910e-02 -5.32965660e-01 -2.35551875e-03 -3.13798368e-01\n",
      " -1.14347935e-01  2.43018508e-01  3.07579547e-01 -5.22813313e-02\n",
      " -4.20233548e-01  2.18349874e-01  5.55004254e-02  5.66572130e-01\n",
      "  6.59680724e-01 -8.94399524e-01 -2.86164790e-01  2.29393005e-01\n",
      "  4.50071633e-01  4.32279050e-01  1.86688542e-01 -1.89910248e-01\n",
      "  7.33997583e-01 -1.81873932e-01 -3.00962925e-01  4.11877334e-02\n",
      "  7.02909231e-02  1.57742649e-01  3.63782793e-01  1.12049423e-01\n",
      " -1.49491921e-01 -2.64353693e-01 -3.24915439e-01 -9.00942087e-02\n",
      " -2.33296558e-01 -4.77985084e-01 -3.49692106e-01 -6.02885485e-01\n",
      " -2.56408423e-01  2.70295799e-01 -1.85493194e-03  2.72505194e-01\n",
      " -4.22163010e-02 -1.23884785e+00 -4.57203567e-01 -2.37575531e-01\n",
      "  2.83233821e-01 -7.48292387e-01  1.50724605e-01  2.80279547e-01\n",
      " -6.13843277e-03  1.10091515e-01  2.30016619e-01  4.92447644e-01\n",
      " -2.08485603e-01  2.40001306e-02  1.98683411e-01 -4.41583060e-02\n",
      " -4.52649415e-01  2.88877469e-02  2.37948418e-01 -8.21279585e-02\n",
      "  4.16145295e-01 -4.75258827e-01 -5.33296466e-01  8.14791471e-02\n",
      "  5.34543097e-02 -5.61394989e-01  4.30597425e-01  1.43776387e-01\n",
      "  5.57135403e-01 -1.77480467e-02 -1.79972947e-01  2.00400129e-01\n",
      " -2.57002026e-01 -7.74617195e-02 -1.91841632e-01  2.40360156e-01\n",
      " -1.04343429e-01  4.47406948e-01  4.62674469e-01 -6.11607194e-01\n",
      "  5.58056951e-01 -4.19937491e-01 -1.38690576e-01  4.63748544e-01\n",
      " -2.30061293e-01 -8.08856547e-01 -2.76557475e-01 -1.47723481e-01\n",
      " -2.67204978e-02  2.28634968e-01 -4.66286778e-01 -8.62178579e-02\n",
      "  2.06221536e-01  2.79705465e-01  1.97965220e-01 -1.25772059e-01\n",
      "  1.28415436e-01  1.85050651e-01  1.45417809e-01  7.39561766e-02\n",
      " -1.65131614e-01 -9.80413929e-02  3.80969584e-01 -9.62484702e-02\n",
      " -3.30965161e-01 -2.88600922e-01  2.31148571e-01 -4.07977700e-01\n",
      "  1.28826186e-01 -1.03870556e-02 -3.18715304e-01  2.75790930e-01\n",
      " -1.00065611e-01  1.00319766e-01 -1.22282475e-01  5.27341813e-02\n",
      " -6.64326549e-01  8.63229260e-02 -4.45042849e-01  3.12059164e-01\n",
      " -3.87602687e-01 -1.63226247e-01 -3.88506055e-03  1.63389325e-01\n",
      "  7.01641321e-01 -1.99343890e-01 -2.30461136e-01  3.29547107e-01\n",
      " -5.34231402e-02 -4.05619383e-01  5.82961559e-01  8.80349427e-02\n",
      " -5.10829568e-01  5.21537304e-01 -3.22682768e-01 -2.52966702e-01\n",
      " -1.26351684e-01 -4.07122612e-01 -4.99761030e-02 -3.59695822e-01\n",
      " -3.43976170e-02  4.20718312e-01 -1.45467475e-01 -1.79655716e-01\n",
      " -3.88475537e-01  1.95693493e-01  3.32396954e-01  5.23452699e-01\n",
      " -7.48633500e-03  8.74492288e-01  1.99305266e-01  5.53981662e-01\n",
      "  3.78595650e-01 -2.42074177e-01 -4.61926758e-01 -2.04048142e-01\n",
      " -4.50344622e-01  4.69006538e-01 -3.77207935e-01  4.20470946e-02\n",
      "  5.33273555e-02  1.69698074e-01  5.76794520e-02 -7.16898561e-01\n",
      "  2.33432367e-01  3.33282530e-01  1.58303976e-01  6.37903869e-01\n",
      " -1.68597370e-01  7.27469325e-01  6.80590943e-02 -7.73288682e-03\n",
      "  1.62904829e-01 -5.07657111e-01  3.03952694e-01  3.23935211e-01\n",
      " -6.41253293e-01 -1.47079229e-01 -2.87382573e-01 -5.80499887e-01\n",
      " -1.17472005e+00 -4.27021146e-01 -1.79434851e-01 -6.71809912e-03\n",
      "  3.81875008e-01 -8.83728936e-02 -9.78982225e-02  3.96338776e-02\n",
      "  2.37605110e-01 -3.46844457e-02 -2.72701055e-01 -6.14108667e-02\n",
      " -5.82807243e-01  4.67931092e-01  1.20931737e-01  5.07059336e-01\n",
      "  2.39019364e-01  1.01826765e-01 -1.00724444e-01 -1.35866582e-01\n",
      "  4.36935097e-01 -1.48267120e-01 -3.96105975e-01  5.74333444e-02\n",
      "  9.68796909e-02 -6.46169782e-01  2.52863646e-01 -1.78316087e-01\n",
      " -5.68663716e-01 -4.36077446e-01  6.71759620e-03 -9.47349742e-02\n",
      " -1.89934999e-01  1.74451306e-01 -2.29776770e-01  2.91879289e-03\n",
      " -5.57582915e-01  5.13474345e-01  1.88290715e-01  4.32198085e-02\n",
      " -5.38478673e-01  3.47757153e-02 -1.36970684e-01  1.89119428e-02\n",
      "  3.07403564e-01  3.97215307e-01  2.41830945e-01  3.25171828e-01\n",
      "  4.47888792e-01 -5.35852373e-01 -5.15663087e-01  3.43649536e-01\n",
      " -3.67887437e-01 -1.93987221e-01  1.35949567e-01 -3.99656713e-01\n",
      "  6.16482854e-01 -8.04273337e-02  3.01909655e-01 -2.63790667e-01\n",
      "  2.22082213e-01 -6.18124723e-01  1.44195855e-01 -2.22241163e-01\n",
      " -2.88589716e-01 -3.27676177e-01  2.14776278e-01 -2.07408920e-01\n",
      " -3.94612163e-01  1.77216426e-01  5.97788513e-01 -6.36358038e-02\n",
      " -5.47607958e-01  4.66707885e-01  1.32165253e-01  2.15345114e-01\n",
      " -3.69607776e-01 -5.15256047e-01 -8.92088413e-02  4.75700736e-01\n",
      "  4.13495481e-01  2.55466849e-01 -1.30035400e-01  4.36480790e-01\n",
      "  5.29521167e-01 -1.01607740e-01 -6.22598790e-02  1.30827546e-01\n",
      "  1.23525672e-01  1.37167096e-01 -1.55910909e-01  1.92650363e-01\n",
      "  1.36880338e-01  5.17273843e-01 -3.49326760e-01 -4.19884086e-01\n",
      "  2.09892958e-01  1.99262708e-01  8.88138190e-02  7.59256333e-02\n",
      "  1.29844710e-01  1.40410155e-01  9.12401527e-02 -5.10172725e-01\n",
      "  7.30494484e-02 -5.61841726e-01 -3.48153710e-01 -1.14296135e-02\n",
      "  1.02851503e-02 -2.70635068e-01  2.04930499e-01  5.63158989e-02\n",
      " -1.26749091e-02  4.00982425e-02 -6.03311539e-01 -2.54267864e-02\n",
      "  6.77889138e-02  4.56211001e-01  3.91004264e-01  5.71998537e-01\n",
      " -1.77818283e-01  2.27927759e-01  1.01681221e-02  1.40714243e-01\n",
      " -3.60543311e-01 -4.78460401e-01  2.63059050e-01 -3.95020470e-02\n",
      " -3.14280689e-01 -3.86370391e-01  1.75525434e-02 -9.52996612e-02\n",
      "  4.94312346e-01 -5.68812728e-01  1.19598955e-01 -2.75356889e-01\n",
      " -4.15979862e-01  2.38006085e-01  2.73150057e-01 -4.85989988e-01\n",
      " -3.77725601e-01 -8.85723293e-01  4.38810110e-01  6.72648728e-01\n",
      "  2.21869200e-02 -6.60382807e-02 -3.40866596e-01  7.73397744e-01\n",
      "  5.56268990e-01  2.50050515e-01 -3.99002433e-01 -4.13173914e-01\n",
      " -8.97899419e-02  1.59340873e-01 -7.28442907e-01 -7.46726394e-01\n",
      " -4.55778688e-02  8.02884519e-01  6.21802062e-02 -4.48381603e-02\n",
      "  6.77783966e-01  6.46775544e-01  2.99176633e-01 -2.12381050e-01\n",
      " -3.48365843e-01  4.85567778e-01  2.16047749e-01 -2.44874150e-01\n",
      "  2.23656148e-01 -1.67345315e-01 -4.92656440e-01  4.28409092e-02\n",
      " -7.02607810e-01 -7.91658521e-01  2.55169988e-01  1.63613826e-01\n",
      "  1.12688094e-02 -5.08828580e-01 -2.85152972e-01 -2.99471349e-01\n",
      "  2.58282661e-01 -9.91513878e-02  1.43668979e-01 -3.56915072e-02\n",
      "  4.07075584e-01  3.49696308e-01  1.06475428e-01  5.34384012e-01\n",
      "  4.88109261e-01  2.00169027e-01  1.76721767e-01  2.90330380e-01\n",
      "  2.34820068e-01 -1.27980292e-01 -7.29368553e-02 -1.51527062e-01\n",
      " -4.37061280e-01  3.44225049e-01  6.00256503e-01 -2.41739064e-01\n",
      " -2.89788127e-01  7.21949875e-01 -1.00705802e-01 -3.85095358e-01\n",
      " -2.10005060e-01 -2.49631017e-01 -1.74972981e-01 -1.63825378e-02\n",
      " -2.16627270e-02 -4.93832603e-02 -3.13880891e-01  4.50079113e-01\n",
      " -6.01006925e-01 -5.59632480e-01 -2.29969323e-01 -3.36770453e-02\n",
      " -3.76453638e-01 -4.48049039e-01 -2.99814403e-01 -3.28158677e-01\n",
      "  5.62430978e-01  1.00462878e+00 -1.88555717e-02 -5.27184941e-02\n",
      "  8.76522511e-02 -1.43677801e-01  2.07250834e-01 -5.83689690e-01\n",
      " -5.59120029e-02  5.44615448e-01  4.73288745e-01  5.15444219e-01\n",
      " -3.40341702e-02 -4.01112497e-01 -4.89789724e-01 -4.08693910e-01\n",
      " -2.80320123e-02  5.49309671e-01  6.87226653e-01 -1.84300512e-01\n",
      "  2.04904765e-01  2.65778214e-01 -9.25114229e-02 -1.25092074e-01\n",
      " -5.27115524e-01 -6.95921838e-01 -2.09286623e-02  3.85543048e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding de 'bank' usando BERT (para 'The bank is near the river.'):\")\n",
    "print(bank_embeddings_bert[0].numpy())  # Embedding de BERT para el primer contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "03c447bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding de 'bank' usando BERT (para 'I need to open a bank account.'):\n",
      "[ 1.34544182e+00 -3.64325285e-01  2.19070897e-01 -4.78106588e-01\n",
      "  1.16727471e+00  2.66221792e-01 -5.94738126e-01  9.01681304e-01\n",
      "  1.88391611e-01 -1.82483792e-01 -3.08255013e-02 -5.01000643e-01\n",
      " -2.47993823e-02  3.79295088e-02 -1.83687508e-01 -6.09300256e-01\n",
      "  5.37129462e-01  1.10333264e+00  9.77232575e-01  2.27195576e-01\n",
      " -1.17837656e+00  3.11654568e-01  3.37865591e-01  4.11539912e-01\n",
      "  5.41460216e-01  7.23480582e-01  1.98293418e-01 -8.26220512e-02\n",
      " -3.97272147e-02 -1.31385767e+00  1.50691271e+00  1.09790847e-01\n",
      "  1.17916264e-01  7.60836363e-01  6.30491257e-01  6.13985360e-02\n",
      " -6.35443926e-01  2.56587148e-01 -1.16643834e+00 -1.99906275e-01\n",
      "  3.05586699e-02 -8.61756027e-01 -1.06976196e-01  2.49178708e-01\n",
      " -6.13669693e-01 -1.85206801e-01 -1.54895768e-01  8.44709456e-01\n",
      "  1.59066319e-02 -1.23520538e-01  1.87116712e-01  7.65666723e-01\n",
      " -5.32485485e-01 -8.40332866e-01  3.84237051e-01  2.18302414e-01\n",
      " -7.17905462e-01 -3.55482131e-01 -4.41377431e-01 -3.56678307e-01\n",
      "  7.69714057e-01  1.91853717e-01  8.99392426e-01 -1.27869928e+00\n",
      "  3.95465106e-01 -8.41199040e-01  4.06979024e-01 -5.27624607e-01\n",
      " -4.91529375e-01  4.06907685e-02  3.02534848e-01  6.19129598e-01\n",
      " -1.10390246e+00 -1.43919528e+00 -3.49250138e-02  2.19637632e-01\n",
      " -3.03454697e-01  7.40497887e-01  6.27266094e-02 -2.85290629e-01\n",
      " -5.78418314e-01 -1.92355365e-01 -1.65018618e-01  5.03076196e-01\n",
      "  4.60118979e-01 -4.37780581e-02 -6.36349499e-01 -5.67644477e-01\n",
      " -4.30267513e-01 -2.21920405e-02 -1.51534900e-01  3.48821133e-01\n",
      " -3.76499861e-01 -3.76931608e-01 -5.08989394e-01 -2.80767739e-01\n",
      " -6.37648165e-01  3.06488037e-01  1.49853081e-01  4.60306406e-01\n",
      "  8.02156806e-01 -1.75170839e+00 -6.32362247e-01  6.96282208e-01\n",
      " -5.10360062e-01 -9.84231681e-02 -8.93250346e-01 -1.16592932e+00\n",
      "  1.04275808e-01  5.42573869e-01 -3.90510373e-02 -4.96743828e-01\n",
      "  4.00000691e-01 -2.61524111e-01 -7.30191469e-01  1.29230961e-01\n",
      "  1.33965343e-01 -9.71475840e-01 -2.58843362e-01 -5.52295744e-01\n",
      " -1.85608357e-01  9.97944355e-01  4.62775290e-01  1.70551133e+00\n",
      "  4.92898792e-01 -1.55672505e-02  1.85845807e-01  5.93036056e-01\n",
      " -1.33060420e+00 -9.33692276e-01  1.04914136e-01  2.14239925e-01\n",
      " -2.18660578e-01 -1.90377235e-01  4.36726004e-01  3.94274652e-01\n",
      " -2.08622336e-01 -4.96593207e-01 -4.46383774e-01  4.76659536e-01\n",
      "  9.31038797e-01  9.70204532e-01  5.78706861e-02  1.30597007e+00\n",
      " -7.95284566e-03 -8.23201686e-02 -1.26302302e-01 -4.49357867e-01\n",
      " -8.63744557e-01 -6.18947446e-01  1.35779262e-01 -1.39511466e-01\n",
      "  2.30872482e-01 -3.92060220e-01 -1.22837134e-01 -3.90015364e-01\n",
      " -2.76433110e-01  3.07513475e-01 -5.11715293e-01  3.03037226e-01\n",
      "  1.81329265e-01  3.46524149e-01  4.41964120e-02  3.15908819e-01\n",
      " -2.60376453e-01 -1.63409233e-01  1.03265357e+00  9.84380126e-01\n",
      " -1.39931440e-01  2.39282072e-01 -4.85617340e-01  3.32112253e-01\n",
      "  1.43131900e+00 -5.61504066e-01  6.40817821e-01  7.14554548e-01\n",
      " -8.96944106e-01 -3.69936317e-01 -4.49390821e-02  2.66641498e-01\n",
      " -4.30975854e-02  8.54815304e-01  7.00417697e-01  5.56623459e-01\n",
      "  5.18154025e-01 -4.16732520e-01  1.22653890e+00 -2.22322032e-01\n",
      "  3.73156756e-01  4.65405762e-01 -4.93772209e-01 -4.78374571e-01\n",
      " -1.37928173e-01  2.63773128e-02  2.01394781e-03 -5.99254429e-01\n",
      "  2.67481416e-01 -2.19318241e-01 -5.23634613e-01  2.71769047e-01\n",
      " -1.14472139e+00 -9.28002059e-01 -2.98006415e-01  2.07906529e-01\n",
      " -1.86063811e-01  5.70087254e-01  3.63806784e-01  9.72186476e-02\n",
      "  9.67060149e-01 -3.37863684e-01 -2.28911161e-01  5.38835764e-01\n",
      " -2.73822427e-01 -1.77726135e-01 -5.60726345e-01  3.29759479e-01\n",
      " -3.95166397e-01 -1.34070575e-01 -1.07631922e-01  2.68804044e-01\n",
      "  5.09340346e-01  1.13342777e-01 -6.43747270e-01  8.48613381e-01\n",
      " -5.21893561e-01  3.23426247e-01  8.20333481e-01  1.53655678e-01\n",
      "  1.70585260e-01  1.91262349e-01  9.23466906e-02 -7.53922611e-02\n",
      "  7.59936094e-01 -8.52562264e-02 -2.50976272e-02  1.40584987e-02\n",
      " -1.53617823e+00 -9.57524359e-01 -6.09733820e-01  2.78958082e-01\n",
      " -3.58336985e-01 -2.71719605e-01 -5.84405124e-01 -1.29371285e-01\n",
      "  4.38635111e-01  5.56242526e-01 -1.74275815e-01 -1.66093290e-01\n",
      " -1.61604971e-01 -2.84429610e-01 -1.93463147e-01 -1.26636028e+00\n",
      " -1.42420650e-01 -5.66558599e-01  6.07851505e-01 -2.24108487e-01\n",
      " -5.36470294e-01  3.84871125e-01 -6.29054606e-01  1.53906167e-01\n",
      "  8.00391972e-01 -9.22861546e-02  5.46889484e-01  6.93188190e-01\n",
      " -3.62552404e-01 -7.23090887e-01 -5.80126345e-01  7.83999935e-02\n",
      "  6.59391522e-01  3.84541333e-01 -4.64827865e-02  8.05523917e-02\n",
      "  1.51789591e-01  1.45290446e+00  8.66299510e-01 -3.46478820e-01\n",
      " -3.79770666e-01  5.37633374e-02  4.30856436e-01  5.46014547e-01\n",
      "  5.95194362e-02  8.59587550e-01 -3.24502110e-01 -8.80059242e-01\n",
      " -1.65106118e-01 -4.24564838e-01 -1.09013028e-01  2.20233035e+00\n",
      " -7.03254640e-01 -2.97644526e-01  4.37191308e-01  3.13171625e-01\n",
      " -9.03791189e-01 -8.81807089e-01  5.23418069e-01  3.46390605e-01\n",
      "  1.43362299e-01  1.18919492e+00  2.62835979e-01 -2.15010583e-01\n",
      "  9.00301993e-01 -6.51205927e-02  4.21118736e-01 -5.72926879e-01\n",
      " -1.07950449e+00  4.90954965e-01 -6.72364771e-01 -5.26925325e-01\n",
      " -2.41863346e+00  5.86716950e-01 -3.84588957e-01 -2.99493551e-01\n",
      "  6.57722235e-01  2.49943256e-01 -1.27347529e-01 -5.53793490e-01\n",
      " -1.19208741e+00 -1.99323550e-01 -1.41171992e+00 -3.53860468e-01\n",
      "  1.05019689e+00 -4.40085530e-01 -4.85290468e-01 -1.12580657e-01\n",
      "  3.05635512e-01 -8.48656893e-02 -2.43823394e-01 -2.08320506e-02\n",
      " -4.12615716e-01 -6.74232006e-01 -4.48311567e-01  1.31110176e-01\n",
      "  9.57528472e-01  8.31263721e-01  1.63784564e-01 -1.90492310e-02\n",
      "  1.53672472e-01  1.33617914e+00 -1.50769353e-02 -5.62454104e-01\n",
      "  2.34566793e-01 -5.56340754e-01  6.83170035e-02 -2.55188376e-01\n",
      " -9.45736229e-01  3.26025635e-02 -1.68588206e-01 -6.85249150e-01\n",
      "  1.81162413e-02  5.71381330e-01  1.21418528e-01  5.45198619e-01\n",
      "  1.26563406e+00 -5.27976394e-01  2.66393811e-01  6.71719015e-01\n",
      " -3.22138727e-01  8.89320076e-01  3.31127584e-01 -1.05320767e-01\n",
      " -7.35310167e-02  3.81144643e-01  5.29855967e-01 -2.35359333e-02\n",
      "  5.72968960e-01  9.25119638e-01  4.23964381e-01 -1.09690353e-01\n",
      " -6.88487053e-01  2.11514644e-02  4.53240603e-01 -2.99010426e-01\n",
      " -3.60009335e-02 -4.78279263e-01 -1.72427392e+00 -8.80531013e-01\n",
      " -3.31957899e-02 -3.92527282e-01  1.27003804e-01 -2.25170508e-01\n",
      " -1.12683147e-01 -1.20317864e+00 -4.53365594e-01 -8.97005618e-01\n",
      "  5.19169688e-01  8.50067616e-01 -2.06655785e-01 -8.71641994e-01\n",
      " -1.16458178e-01 -1.01060338e-01  3.97505015e-01  1.52866542e-03\n",
      " -1.38362348e-01  2.76658237e-01 -5.54841399e-01  2.25951858e-02\n",
      " -6.87893271e-01 -4.08704489e-01  4.24221992e-01  1.05435956e+00\n",
      "  7.36287594e-01 -6.16070814e-02  1.09958816e-02  8.23705316e-01\n",
      "  2.39518713e-02 -3.44288558e-01 -1.50080308e-01 -4.99287784e-01\n",
      " -3.79368439e-02  7.57533133e-01  3.62042457e-01 -2.61735111e-01\n",
      "  3.13560873e-01  3.92386138e-01 -2.25426704e-01 -2.26725727e-01\n",
      " -5.34948170e-01  1.05107415e+00  1.10490680e+00 -8.83831620e-01\n",
      "  1.51477933e+00 -4.62504923e-01 -5.76064661e-02 -3.69264372e-03\n",
      "  2.75288671e-01 -6.47111952e-01  8.86463940e-01 -1.54936031e-01\n",
      " -2.91651338e-01  2.50611007e-01  3.33626390e-01 -1.14016974e+00\n",
      "  2.36704752e-01 -1.63258493e-01  5.51888049e-01 -5.78434348e-01\n",
      " -4.80757684e-01 -5.88805854e-01  3.12846184e-01 -1.15241975e-01\n",
      " -4.27326679e-01  2.84418166e-01  1.74996525e-01 -2.96186358e-02\n",
      "  5.37366092e-01  2.63711572e-01 -1.40513569e-01 -1.33755326e-01\n",
      " -1.08877786e-01  9.95189726e-01 -2.46568084e-01 -3.85987967e-01\n",
      " -5.11291921e-01 -7.81475782e-01 -1.25572371e+00  1.35702625e-01\n",
      "  1.92114234e-01  1.71610519e-01 -1.22420400e-01 -3.02326679e-01\n",
      " -3.37670565e-01  6.49865493e-02  2.56694496e-01  7.11638212e-01\n",
      "  9.91691470e-01 -5.76396346e-01 -5.64065516e-01 -1.12579346e+00\n",
      "  8.38070512e-01 -4.66256618e-01  3.19022357e-01  2.65505284e-01\n",
      " -8.74524236e-01  7.65466213e-01 -6.99892402e-01 -9.37517405e-01\n",
      " -5.73949397e-01  4.67049539e-01 -2.23146066e-01 -4.07936126e-01\n",
      "  3.43480229e-01 -7.13142157e-02 -1.65836900e-01  2.74101168e-01\n",
      " -3.25569063e-01 -1.27336100e-01 -1.50667131e-01  4.49893773e-02\n",
      " -3.13551664e-01 -1.35648504e-01 -4.43075806e-01 -8.45220625e-01\n",
      "  6.91599965e-01  7.83904910e-01  3.82906795e-01 -2.92001933e-01\n",
      " -1.05494373e-02 -7.67955959e-01 -7.56313562e-01  1.26972866e+00\n",
      " -4.01423901e-01 -7.72191763e-01 -8.78401041e-01 -1.08202112e+00\n",
      "  7.02123791e-02 -6.53910160e-01 -9.13443193e-02  6.16932809e-01\n",
      " -3.30973327e-01 -3.38304192e-02 -1.56102329e-01  5.42911962e-02\n",
      "  5.69418311e-01  4.54995573e-01  1.18213989e-01  3.97132635e-01\n",
      " -8.12267184e-01  1.13582468e+00 -8.91701639e-01 -8.03680956e-01\n",
      " -1.14813673e+00 -5.96589088e-01  1.83654577e-01 -6.66153014e-01\n",
      "  2.75738128e-02 -2.80150902e-02 -1.84708666e-02  1.68520644e-01\n",
      " -2.66671538e-01 -2.38187298e-01 -1.84315994e-01  5.70721254e-02\n",
      " -1.38145006e+00 -4.18492168e-01 -8.01103637e-02  1.07762933e+00\n",
      " -4.10492390e-01  6.90828800e-01 -8.44118834e-01 -5.33944547e-01\n",
      "  1.28266498e-01  4.64818776e-01  4.66857940e-01  9.82176736e-02\n",
      " -3.64776820e-01 -1.81983411e+00  4.03529704e-02 -8.05198699e-02\n",
      " -2.83604443e-01 -8.48867297e-01 -6.89062059e-01 -2.29403615e-01\n",
      " -5.07110022e-02  6.06749117e-01  6.46088243e-01  1.44401550e+00\n",
      "  2.02486694e-01  1.09323847e+00  8.84413540e-01  4.01322812e-01\n",
      "  2.01271966e-01 -8.60749334e-02 -9.40941334e-01  2.96192050e-01\n",
      "  2.91389346e-01 -2.11174060e-02 -2.86150217e-01  2.81779855e-01\n",
      "  5.72065234e-01 -7.72401810e-01 -7.91863799e-01  4.21714246e-01\n",
      "  6.73921943e-01  1.87105119e-01 -1.21881515e-02  2.22568855e-01\n",
      " -7.76761398e-03 -2.80098587e-01  8.10834885e-01 -8.73870432e-01\n",
      "  4.82263118e-01 -1.00959611e+00 -1.42518520e-01  4.17266190e-01\n",
      " -1.41674316e+00 -1.51328528e+00 -4.32528198e-01 -3.81864965e-01\n",
      " -5.83472431e-01  8.25082004e-01 -9.35728550e-02 -3.84771913e-01\n",
      " -1.01016355e+00  2.72961706e-01 -6.43655956e-01 -2.88756639e-02\n",
      " -2.05960363e-01  2.11223543e-01  8.01987946e-02  6.02582157e-01\n",
      "  3.47430885e-01 -2.52779722e-02 -4.96118844e-01  5.06721020e-01\n",
      "  6.33808672e-01  2.55133361e-01 -5.18333733e-01  7.58786678e-01\n",
      "  4.12261486e-01 -1.30434081e-01  8.45464587e-01  5.61924756e-01\n",
      "  3.97597969e-01  2.17701226e-01 -4.79145586e-01 -6.84686720e-01\n",
      " -1.47731632e-01  1.99101627e-01 -7.27227271e-01 -1.01327562e+00\n",
      "  9.07387316e-01  5.52800596e-01 -1.14996290e+00  7.03966022e-02\n",
      " -9.22594965e-02 -3.76324594e-01 -6.46763921e-01  6.98177159e-01\n",
      " -6.17769957e-01  7.48221815e-01 -1.84711993e-01  5.19275844e-01\n",
      " -8.49651694e-02  2.44842023e-01  4.75992113e-02 -2.00150296e-01\n",
      " -1.43081531e-01  1.67246237e-01 -4.97552037e-01  1.48111618e+00\n",
      "  5.28117940e-02  6.27781153e-01  8.58331025e-01  2.96723098e-01\n",
      " -5.78842685e-02 -2.52109408e-01 -2.47176707e-01  2.41467133e-02\n",
      "  4.47269380e-01 -3.79391879e-01 -2.02179179e-01  1.71109176e+00\n",
      " -3.35727245e-01 -9.09837335e-02  1.66450709e-01  4.29971635e-01\n",
      "  9.95479465e-01  9.33670700e-01 -3.63349058e-02  3.82198125e-01\n",
      "  2.39207923e-01 -6.94950223e-01 -1.73769683e-01  6.08099997e-01\n",
      "  9.93555903e-01  4.18144464e-01 -2.60091811e-01  2.94502676e-01\n",
      " -1.34754509e-01  2.22059824e-02 -2.43306495e-02 -4.43131685e-01\n",
      " -3.66859347e-01  6.63596988e-01  7.04020336e-02 -6.59914732e-01\n",
      "  5.76442778e-01  4.87984419e-01  9.73723754e-02  1.93143800e-01\n",
      "  8.31232309e-01  3.84639263e-01 -2.37129182e-01 -3.27143490e-01\n",
      " -1.30478573e+00  3.63972366e-01  1.06549323e-01 -9.69609320e-01\n",
      "  5.66719294e-01 -1.02927856e-01 -6.93730533e-01 -5.27243257e-01\n",
      " -9.27339077e-01 -3.32310021e-01  6.10185742e-01 -2.71183074e-01\n",
      " -4.31864440e-01 -9.55899298e-01  6.43883824e-01 -2.29826912e-01\n",
      "  1.23194121e-01  5.91298103e-01  5.18869400e-01 -3.87043267e-01\n",
      "  5.73613882e-01 -9.07419398e-02  1.12147868e+00  1.92648962e-01\n",
      "  4.88279194e-01 -4.16727841e-01 -9.67276394e-02  3.53811860e-01\n",
      "  1.75823681e-02  1.47678792e-01  9.68099654e-01 -7.85186067e-02\n",
      " -9.37111855e-01 -1.88860953e-01  4.24915373e-01  7.68663228e-01\n",
      " -9.27861869e-01  1.20635474e+00  1.94916487e-01  2.55889893e-01\n",
      "  5.83059251e-01 -4.75872010e-02 -2.52786249e-01 -1.69773251e-02\n",
      "  1.59851909e-01  7.50082612e-01  6.98731691e-02  4.99995440e-01\n",
      " -1.42541492e+00 -4.41614211e-01 -6.23526096e-01  5.91008604e-01\n",
      "  4.24828641e-02  3.11974138e-02 -1.20357466e+00 -7.29797706e-02\n",
      "  1.17800248e+00  1.58922720e+00 -3.93237144e-01  5.60549051e-02\n",
      "  8.02203834e-01 -3.10282737e-01 -1.33175385e+00 -1.89040482e-01\n",
      "  6.35018766e-01  2.20847607e-01  1.36408299e-01 -3.09702545e-01\n",
      "  6.05412573e-03  2.34320074e-01 -2.74076641e-01 -3.21574658e-02\n",
      " -5.80134034e-01 -1.77689940e-01 -8.46872926e-02 -6.86333358e-01\n",
      " -2.14179784e-01 -2.46021286e-01  2.46237546e-01 -7.67024338e-01\n",
      " -1.41120881e-01 -7.82160163e-01 -4.83026534e-01  1.57927915e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEmbedding de 'bank' usando BERT (para 'I need to open a bank account.'):\")\n",
    "print(bank_embeddings_bert[1].numpy())  # Embedding de BERT para el segundo contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "dc3faa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding de 'bank' usando GloVe:\n",
      "[ 0.66488  -0.11391   0.67844   0.17951   0.6828   -0.47787  -0.30761\n",
      "  0.17489  -0.70512  -0.55022   0.1514    0.10214  -0.45063  -0.33069\n",
      "  0.056133  1.2271    0.55607  -0.68297   0.037364  0.70266   1.9093\n",
      " -0.61483  -0.83329  -0.3023   -1.1118   -1.55      0.2604    0.22957\n",
      " -1.0375   -0.31789   3.5091   -0.25871   1.0151    0.65927  -0.18231\n",
      " -0.75859  -0.30927  -0.91678   1.0633   -0.66761  -0.37464  -0.29143\n",
      "  0.65606  -0.44642  -0.075495 -1.0552   -0.60501   0.73582   1.0139\n",
      " -0.27749 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEmbedding de 'bank' usando GloVe:\")\n",
    "print(bank_embedding_glove)  # Embedding de GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e33f57b",
   "metadata": {},
   "source": [
    "Embeddings de BERT:\n",
    "\n",
    "- Los vectores de \"bank\" obtenidos de BERT para ambas frases son distintos, lo que indica que el modelo ajusta la representaciÃ³n segÃºn el contexto.\n",
    "\n",
    "- Al imprimir los embeddings de \"bank\" en cada contexto, podrÃ¡s observar cÃ³mo varÃ­an los valores.\n",
    "\n",
    "Embedding de GloVe:\n",
    "\n",
    "- El embedding de \"bank\" de GloVe es Ãºnico y no cambia, ya que no toma en cuenta el contexto. Este vector serÃ¡ el mismo independientemente de en quÃ© frase se utilice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd8259",
   "metadata": {},
   "source": [
    "## 3. Modelos Basados en Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4021a4e0",
   "metadata": {},
   "source": [
    "### 3.1 Pregunta 5: Â¿QuÃ© son las redes neuronales recurrentes (RNN) y las LSTM, y cÃ³mo se usan en NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d49fc12",
   "metadata": {},
   "source": [
    "Las **Redes Neuronales Recurrentes (RNN)** son un tipo de arquitectura de red neuronal diseÃ±ada para procesar datos secuenciales. A diferencia de las redes neuronales tradicionales, donde las entradas y salidas son independientes, las RNN mantienen una memoria interna que les permite recordar informaciÃ³n de entradas anteriores. Esto es Ãºtil en tareas como la traducciÃ³n automÃ¡tica, el reconocimiento de voz y el anÃ¡lisis de texto, donde el contexto es crucial para la predicciÃ³n.\n",
    "\n",
    "Uno de los principales problemas que enfrentan las RNN es el **desvanecimiento del gradiente**, que dificulta el aprendizaje de dependencias a largo plazo. A medida que se retropropagan los errores a travÃ©s del tiempo, los gradientes pueden volverse extremadamente pequeÃ±os, lo que impide que la red aprenda correctamente.\n",
    "\n",
    "En ese sentido, las **LSTM** (Long Short-Term Memory Ã³ Memoria a Largo Corto Plazo, en espaÃ±ol) son una variante avanzada de las RNN que fueron diseÃ±ada para aprender y recordar dependencias a largo plazo en secuencias de datos, abordando el problema del desvanecimiento del gradiente. Las LSTM incorporan una estructura mÃ¡s compleja con celdas de memoria y tres puertas:\n",
    "- **Puerta de entrada**: controla la cantidad de informaciÃ³n nueva que se almacena.\n",
    "- **Puerta de olvido**: decide quÃ© informaciÃ³n debe ser descartada.\n",
    "- **Puerta de salida**: regula quÃ© informaciÃ³n se utiliza para generar la salida.\n",
    "\n",
    "Esta arquitectura permite a las LSTM recordar informaciÃ³n durante perÃ­odos mÃ¡s largos, lo que es crucial en tareas donde el contexto puede estar lejos en la secuencia.\n",
    "\n",
    " \n",
    "\n",
    "**Aplicaciones en Procesamiento del Lenguaje Natural (PNL)**\n",
    "\n",
    "Las RNN y LSTM son ampliamente utilizadas en NLP para tareas como:\n",
    "- **TraducciÃ³n automÃ¡tica**: donde el modelo debe entender contextos complejos.\n",
    "- **AnÃ¡lisis de sentimientos**: donde se requiere comprender la secuencia completa para hacer una predicciÃ³n precisa.\n",
    "- **GeneraciÃ³n de texto**: donde el modelo genera texto coherente basado en entradas previas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1960ed7",
   "metadata": {},
   "source": [
    "#### 3.1.1 Ejercicio: Implementar una RNN simple para predicciÃ³n de secuencias en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cbf530",
   "metadata": {},
   "source": [
    "[![IntroducciÃ³n a las Redes Neuronales Recurrentes](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2FbKkjQx_PS_M)](https://youtu.be/bKkjQx_PS_M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b315f0",
   "metadata": {},
   "source": [
    "[![GENERACIÃN DE TEXTO con Redes Recurrentes en Python (Tutorial)](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DaA9QaPu_QpA)](https://www.youtube.com/watch?v=aA9QaPu_QpA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e531dc65",
   "metadata": {},
   "source": [
    "Tomado de: https://www.youtube.com/watch?v=aA9QaPu_QpA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e98abc",
   "metadata": {},
   "source": [
    "##### 3.1.1.1 Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "541ddf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres = open('nombres_dinosaurios.txt','r').read()\n",
    "nombres = nombres.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b53566be",
   "metadata": {},
   "outputs": [],
   "source": [
    "alfabeto = list(set(nombres))\n",
    "tam_datos, tam_alfabeto = len(nombres), len(alfabeto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "039b74d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_a_ind = { car:ind for ind,car in enumerate(sorted(alfabeto))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3e481b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_a_car = { ind:car for ind,car in enumerate(sorted(alfabeto))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ec7ef9",
   "metadata": {},
   "source": [
    "##### 3.1.1.2 Modelo RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8868f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 25    # NÃºmero de unidades en la capa oculta\n",
    "entrada  = Input(shape=(None,tam_alfabeto))\n",
    "a0 = Input(shape=(n_a,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "baa4c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "celda_recurrente = SimpleRNN(n_a, activation='tanh', return_state = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e768691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "capa_salida = Dense(tam_alfabeto, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1769b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs, _ = celda_recurrente(entrada, initial_state=a0)\n",
    "salida = []\n",
    "salida.append(capa_salida(hs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "db92ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = Model([entrada,a0],salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2c5163d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(learning_rate=0.0005)\n",
    "modelo.compile(optimizer=opt, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec8d15",
   "metadata": {},
   "source": [
    "##### 3.1.1.3 Ejemplos de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "8ef445e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nombres_dinosaurios.txt\") as f:\n",
    "    ejemplos = f.readlines()\n",
    "ejemplos = [x.lower().strip() for x in ejemplos]\n",
    "np.random.shuffle(ejemplos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ba605429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator():\n",
    "    while True:\n",
    "        # Tomar un ejemplo aleatorio\n",
    "        ejemplo = ejemplos[np.random.randint(0,len(ejemplos))]\n",
    "\n",
    "        # Convertir el ejemplo a representaciÃ³n numÃ©rica\n",
    "        X = [None] + [car_a_ind[c] for c in ejemplo]\n",
    "\n",
    "        # Crear \"Y\", resultado de desplazar \"X\" un caracter a la derecha\n",
    "        Y = X[1:] + [car_a_ind['\\n']]\n",
    "\n",
    "        # Representar \"X\" y \"Y\" en formato one-hot\n",
    "        x = np.zeros((len(X),1,tam_alfabeto))\n",
    "        onehot = to_categorical(X[1:],tam_alfabeto).reshape(len(X)-1,1,tam_alfabeto)\n",
    "        x[1:,:,:] = onehot\n",
    "        y = to_categorical(Y,tam_alfabeto).reshape(len(X),tam_alfabeto)\n",
    "\n",
    "        # ActivaciÃ³n inicial (matriz de ceros)\n",
    "        a = np.zeros((len(X), n_a))\n",
    "\n",
    "        yield [x, a], y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82052df",
   "metadata": {},
   "source": [
    "##### 3.1.1.4 ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9dd49e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guill\\AppData\\Local\\Temp\\ipykernel_182108\\1057266624.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  historia = modelo.fit_generator(train_generator(), steps_per_epoch=BATCH_SIZE, epochs=1, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IteraciÃ³n: 0, Error: 3.186241\n",
      "\n",
      "\n",
      "IteraciÃ³n: 100, Error: 2.937734\n",
      "\n",
      "\n",
      "IteraciÃ³n: 200, Error: 2.768003\n",
      "\n",
      "\n",
      "IteraciÃ³n: 300, Error: 2.671154\n",
      "\n",
      "\n",
      "IteraciÃ³n: 400, Error: 2.576772\n",
      "\n",
      "\n",
      "IteraciÃ³n: 500, Error: 2.566090\n",
      "\n",
      "\n",
      "IteraciÃ³n: 600, Error: 2.501309\n",
      "\n",
      "\n",
      "IteraciÃ³n: 700, Error: 2.463476\n",
      "\n",
      "\n",
      "IteraciÃ³n: 800, Error: 2.492368\n",
      "\n",
      "\n",
      "IteraciÃ³n: 900, Error: 2.394192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 80\t\t\t# NÃºmero de ejemplos de entrenamiento a usar en cada iteraciÃ³n\n",
    "NITS = 1000\t\t\t# NÃºmero de iteraciones\n",
    "\n",
    "for j in range(NITS):\n",
    "    historia = modelo.fit_generator(train_generator(), steps_per_epoch=BATCH_SIZE, epochs=1, verbose=0)\n",
    "\n",
    "    # Imprimir evoluciÃ³n del entrenamiento cada 1000 iteraciones\n",
    "    if j%100 == 0:\n",
    "        print('\\nIteraciÃ³n: %d, Error: %f' % (j, historia.history['loss'][0]) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b086b708",
   "metadata": {},
   "source": [
    "##### 3.1.1.5 GeneraciÃ³n de nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f1abf526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_nombre(modelo,car_a_num,tam_alfabeto,n_a):\n",
    "    # Inicializar x y a con ceros\n",
    "    x = np.zeros((1,1,tam_alfabeto,))\n",
    "    a = np.zeros((1, n_a))\n",
    "\n",
    "    # Nombre generado y caracter de fin de linea\n",
    "    nombre_generado = ''\n",
    "    fin_linea = '\\n'\n",
    "    car = -1\n",
    "\n",
    "    # Iterar sobre el modelo y generar predicciÃ³n hasta tanto no se alcance\n",
    "    # \"fin_linea\" o el nombre generado llegue a los 50 caracteres\n",
    "    contador = 0\n",
    "    while (car != fin_linea and contador != 50):\n",
    "          # Generar predicciÃ³n usando la celda RNN\n",
    "          a, _ = celda_recurrente(K.constant(x), initial_state=K.constant(a))\n",
    "          y = capa_salida(a)\n",
    "          prediccion = K.eval(y)\n",
    "\n",
    "          # Escoger aleatoriamente un elemento de la predicciÃ³n (el elemento con\n",
    "          # con probabilidad mÃ¡s alta tendrÃ¡ mÃ¡s opciones de ser seleccionado)\n",
    "          ix = np.random.choice(list(range(tam_alfabeto)),p=prediccion.ravel())\n",
    "\n",
    "          # Convertir el elemento seleccionado a caracter y aÃ±adirlo al nombre generado\n",
    "          car = ind_a_car[ix]\n",
    "          nombre_generado += car\n",
    "\n",
    "          # Crear x_(t+1) = y_t, y a_t = a_(t-1)\n",
    "          x = to_categorical(ix,tam_alfabeto).reshape(1,1,tam_alfabeto)\n",
    "          a = K.eval(a)\n",
    "\n",
    "          # Actualizar contador y continuar\n",
    "          contador += 1\n",
    "\n",
    "          # Agregar fin de lÃ­nea al nombre generado en caso de tener mÃ¡s de 50 caracteres\n",
    "          if (contador == 50):\n",
    "            nombre_generado += '\\n'\n",
    "\n",
    "    return nombre_generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d056a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar 100 ejemplos de nombres generados por el modelo ya entrenado\n",
    "lista_de_nombres = []\n",
    "for i in range(100):\n",
    "    lista_de_nombres.append(generar_nombre(modelo,car_a_ind,tam_alfabeto,n_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "28014a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_de_nombres = sorted(lista_de_nombres, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "83ec464e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 's\\n',\n",
       " 'h\\n',\n",
       " 's\\n',\n",
       " 's\\n',\n",
       " 'n\\n',\n",
       " 's\\n',\n",
       " 'ik\\n',\n",
       " 'os\\n',\n",
       " 'tos\\n',\n",
       " 'oxl\\n',\n",
       " 'rbo\\n',\n",
       " 'aos\\n',\n",
       " 'tns\\n',\n",
       " 'cos\\n',\n",
       " 'nkr\\n',\n",
       " 'oun\\n',\n",
       " 'woir\\n',\n",
       " 'haon\\n',\n",
       " 'usro\\n',\n",
       " 'nnys\\n',\n",
       " 'yonos\\n',\n",
       " 'usahs\\n',\n",
       " 'mrpxn\\n',\n",
       " 'nskts\\n',\n",
       " 'matos\\n',\n",
       " 'torus\\n',\n",
       " 'algws\\n',\n",
       " 'hiiis\\n',\n",
       " 'gporus\\n',\n",
       " 'laurus\\n',\n",
       " 'sureil\\n',\n",
       " 'scaoeu\\n',\n",
       " 'onoros\\n',\n",
       " 'nsanji\\n',\n",
       " 'gauistn\\n',\n",
       " 'aagrcot\\n',\n",
       " 'suroqzt\\n',\n",
       " 'hucotts\\n',\n",
       " 'osthsahg\\n',\n",
       " 'olernpos\\n',\n",
       " 'inicoeus\\n',\n",
       " 'irurilus\\n',\n",
       " 'uhirzotu\\n',\n",
       " 'ocnosrlur\\n',\n",
       " 'aulmgenus\\n',\n",
       " 'luferlros\\n',\n",
       " 'anisesata\\n',\n",
       " 'fnrmelura\\n',\n",
       " 'ectodnnps\\n',\n",
       " 'silykptaus\\n',\n",
       " 'weruchruss\\n',\n",
       " 'jsuesaicus\\n',\n",
       " 'nojasciuqoj\\n',\n",
       " 'atlhrurorus\\n',\n",
       " 'esdotloyntj\\n',\n",
       " 'glyvobanues\\n',\n",
       " 'dutosnanous\\n',\n",
       " 'rurticrurur\\n',\n",
       " 'iromitnhsfpn\\n',\n",
       " 'gnrroentmaos\\n',\n",
       " 'apauclrfolcs\\n',\n",
       " 'oktseulaloom\\n',\n",
       " 'dotsippepnits\\n',\n",
       " 'asttsipcgontl\\n',\n",
       " 'orysargryasau\\n',\n",
       " 'hurtiammepucr\\n',\n",
       " 'oophgsaeahopcn\\n',\n",
       " 'yrdiyerenualor\\n',\n",
       " 'oupnlionpoevgn\\n',\n",
       " 'tneranseoihyior\\n',\n",
       " 'usinnrurirtonob\\n',\n",
       " 'otasanbplerurus\\n',\n",
       " 'grurtvususiusar\\n',\n",
       " 'oceahsonusacrhke\\n',\n",
       " 'rtrtohtsraqyoeth\\n',\n",
       " 'kaenototleprthho\\n',\n",
       " 'ygeurupusasurrpjx\\n',\n",
       " 'mlalchprnrlnairwr\\n',\n",
       " 'ptsolarstneeaarbtos\\n',\n",
       " 'oltirosoeitioirhtan\\n',\n",
       " 'arsrbpoiathzuruluros\\n',\n",
       " 'uesrusioturorlajsaus\\n',\n",
       " 'rurururdsausejoamous\\n',\n",
       " 'ntruneoollorayopvdos\\n',\n",
       " 'josalometmasosdosasfd\\n',\n",
       " 'ogoasaborururushutdye\\n',\n",
       " 'uscxuloqtaorusarustts\\n',\n",
       " 'aucponlaaaacraloeurul\\n',\n",
       " 'rurgmiaitolvrorurhfecoo\\n',\n",
       " 'onpceatardupaanugsauszcr\\n',\n",
       " 'usopcitftorusikuplterorus\\n',\n",
       " 'tuesivseuiitausanaaoscneey\\n',\n",
       " 'fsenyontdntyicousclnuraonlbftlxpihrhue\\n']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_de_nombres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415c7d9",
   "metadata": {},
   "source": [
    "### 3.2 Pregunta 6: Â¿QuÃ© son los transformers y por quÃ© han reemplazado en gran medida a las RNN en NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d607a36",
   "metadata": {},
   "source": [
    "Un transformador es un modelo de una rede neural que aprende del contexto de una secuencia de datos para generar nuevos datos. Su funcionamiento empieza con la tokenizaciÃ³n en donde descompone el texto a unidades llamadas tokens. Estos tokens se convierten en valores numÃ©ricos mediante los embeddings y se les asigna una posiciÃ³n de cada palabra con la codificaciÃ³n posicional. La informaciÃ³n se envÃ­a al codificador en donde ejecuta un mecanismo de autoatenciÃ³n y un mecanismo de mecanismo de atenciÃ³n multicabezal con el fin de devolver la representaciÃ³n de la entrada. Por Ãºltimo, envÃ­a el texto al decodificador para generar el texto objetivo. Esta arquitectura ha reemplazado a las redes neuronales recurrentes (RNNs) porque procesan el texto de forma paralela en cambio de forma secuencial lo cual mejorar el rendimiento del tiempo, pueden seguir el contexto del tema aÃºn si la secuencia del texto es muy largo y aprenden mÃ¡s rÃ¡pido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f9054",
   "metadata": {},
   "source": [
    "#### 3.2.1 Ejercicio: Utilizar el modelo de transformers BERT o GPT-2 de Hugging Face para completar una frase simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1fb9f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Being a computer scientist is all about\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f25d02e",
   "metadata": {},
   "source": [
    "##### 3.2.1.1 Utilizar EleutherAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "c9beb236",
   "metadata": {},
   "outputs": [],
   "source": [
    "eleutherAI_generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d62a8b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Being a computer scientist is all about the ability to analyze and understand the world. The world is a'}]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(85)\n",
    "eleutherAI_prediction = eleutherAI_generator(text)\n",
    "eleutherAI_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748b30bd",
   "metadata": {},
   "source": [
    "##### 3.2.1.2 Utilizar GPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f36f38c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "gpt2_generator = pipeline(\"text-generation\", model=\"gpt2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5592502c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Being a computer scientist is all about getting what you want. But you're still a little frustrated when you can't make progress, and those things are what drive your problems: your frustration with your work, your frustration with technology.\\n\\nThere is\"}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_prediction = gpt2_generator(text)\n",
    "gpt2_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9df00d0",
   "metadata": {},
   "source": [
    "## 4. Modelos Pre-entrenados y Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade7df70",
   "metadata": {},
   "source": [
    "### 4.1 Pregunta 7: Â¿QuÃ© significa que un modelo estÃ© preentrenado? Â¿CÃ³mo se puede ajustar para una tarea especÃ­fica?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a191710a",
   "metadata": {},
   "source": [
    "**Modelo preentrenado**\n",
    "Un modelo de lenguaje es un tipo de algoritmo de machine learning diseÃ±ado para predecir la siguiente palabra en una oraciÃ³n, basÃ¡ndose en los segmentos predecesores. EstÃ¡ basado la arquitectura de Transformers. Los modelos de lenguaje pre-entrenados se entrenan con grandes cantidades de datos de texto, que permite a los LLM comprender los principios fundamentales que rige el idioma en el uso de las palabras y su disposiciÃ³n en el lenguaje natural.\n",
    "\n",
    "**Fine-Tuning**\n",
    "El Fine-tuning es el proceso de tomar un modelo preentrenado y entrenarlo mas allÃ¡ de lo preestablecido en un conjunto de datos mas especifico en el dominio. Esto ofrece ventajas como menor gasto de maquina en el cÃ¡lculo y mejora la capacidad de procesamiento de los datos. Con ello mejora el desempeÃ±o en tareas especÃ­ficas, como anÃ¡lisis de sentimientos, question answering, resumen de documentos, con mayor precisiÃ³n. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9dfe86",
   "metadata": {},
   "source": [
    "#### 4.1.1 Realizar el fine-tuning de un modelo pre-entrenado de Hugging Face (BERT o RoBERTa) para clasificaciÃ³n de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8967f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('glue', 'sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "178e228e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f59dbb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['sentence'], truncation=True, padding=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b35b5600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling proportion, e.g., 10%\n",
    "train_proportion = 0.01\n",
    "test_proportion = 0.01\n",
    "\n",
    "# Calculate sample sizes based on the proportions\n",
    "train_sample_size = int(len(tokenized_dataset['train']) * train_proportion)\n",
    "test_sample_size = int(len(tokenized_dataset['validation']) * test_proportion)\n",
    "\n",
    "# Randomly select samples from the tokenized data\n",
    "train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(train_sample_size))\n",
    "test_dataset = tokenized_dataset[\"validation\"].shuffle(seed=42).select(range(test_sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ca09bf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ð¤ Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "56e71a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "058b2d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|ââââ      | 43/129 [01:36<02:31,  1.76s/it]\n",
      " 33%|ââââ      | 43/129 [01:37<02:31,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4728577435016632, 'eval_runtime': 0.2895, 'eval_samples_per_second': 27.636, 'eval_steps_per_second': 3.455, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|âââââââ   | 86/129 [03:13<01:15,  1.76s/it]\n",
      " 67%|âââââââ   | 86/129 [03:13<01:15,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10971170663833618, 'eval_runtime': 0.2852, 'eval_samples_per_second': 28.051, 'eval_steps_per_second': 3.506, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 129/129 [04:49<00:00,  1.82s/it]\n",
      "100%|ââââââââââ| 129/129 [04:50<00:00,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08462903648614883, 'eval_runtime': 0.3078, 'eval_samples_per_second': 25.994, 'eval_steps_per_second': 3.249, 'epoch': 3.0}\n",
      "{'train_runtime': 290.9157, 'train_samples_per_second': 6.94, 'train_steps_per_second': 0.443, 'train_loss': 0.38769806263058687, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=129, training_loss=0.38769806263058687, metrics={'train_runtime': 290.9157, 'train_samples_per_second': 6.94, 'train_steps_per_second': 0.443, 'total_flos': 66670388650860.0, 'train_loss': 0.38769806263058687, 'epoch': 3.0})"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8b7f92b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 1/1 [00:00<00:00, 109.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08462903648614883, 'eval_runtime': 0.3151, 'eval_samples_per_second': 25.386, 'eval_steps_per_second': 3.173, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10309397",
   "metadata": {},
   "source": [
    "### 4.2 **Pregunta 8**: Â¿QuÃ© es el enmascaramiento de palabras en el preentrenamiento de modelos como BERT?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f65fdf5",
   "metadata": {},
   "source": [
    "El **enmascaramiento de palabras** es una tÃ©cnica fundamental en el preentrenamiento de modelos de lenguaje como BERT. Consiste en enmascarar aleatoriamente algunas palabras en una oraciÃ³n y entrenar al modelo para predecir la palabra original en funciÃ³n del contexto proporcionado por las palabras circundantes. Es importante porque permite:\n",
    "- **ComprensiÃ³n profunda del lenguaje**: Al intentar predecir palabras faltantes, el modelo aprende a captar las relaciones semÃ¡nticas y sintÃ¡cticas entre las palabras, lo que le permite desarrollar una comprensiÃ³n profunda del lenguaje.\n",
    "- **GeneraciÃ³n de contexto**: El enmascaramiento fuerza al modelo a generar un contexto rico y coherente para cada palabra enmascarada, lo que es esencial para tareas posteriores como la generaciÃ³n de texto y la traducciÃ³n.\n",
    "\n",
    "**Ejemplo**: Si tenemos la frase \"El gato <mask> un ratÃ³n\", el modelo podrÃ­a ser entrenado para predecir que la palabra faltante es \"cazÃ³\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47691e9",
   "metadata": {},
   "source": [
    "#### 4.1.2 Ejercicio: Usar BERT para predecir palabras enmascaradas en una oraciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a9c4c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Being a computer scientist is all about [MASK].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "38cd6370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bert_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "00cc48a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.11361545324325562,\n",
       "  'token': 7588,\n",
       "  'token_str': 'computers',\n",
       "  'sequence': 'being a computer scientist is all about computers.'},\n",
       " {'score': 0.06099292263388634,\n",
       "  'token': 2671,\n",
       "  'token_str': 'science',\n",
       "  'sequence': 'being a computer scientist is all about science.'},\n",
       " {'score': 0.05559048056602478,\n",
       "  'token': 2009,\n",
       "  'token_str': 'it',\n",
       "  'sequence': 'being a computer scientist is all about it.'},\n",
       " {'score': 0.048937730491161346,\n",
       "  'token': 2470,\n",
       "  'token_str': 'research',\n",
       "  'sequence': 'being a computer scientist is all about research.'},\n",
       " {'score': 0.03174487128853798,\n",
       "  'token': 8785,\n",
       "  'token_str': 'math',\n",
       "  'sequence': 'being a computer scientist is all about math.'}]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_predition = bert_mask(text)\n",
    "bert_predition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870140cb",
   "metadata": {},
   "source": [
    "## 5. Tareas ClÃ¡sicas de NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e938d75",
   "metadata": {},
   "source": [
    "### 5.1 **Pregunta 9**: Â¿QuÃ© es la clasificaciÃ³n de texto y quÃ© modelos se suelen utilizar para esta tarea?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9712d",
   "metadata": {},
   "source": [
    "Es una tarea del NLP (procesamiento del lenguaje natural) que consiste en asignar una o mÃ¡s categorÃ­as a un fragmento de texto. Este modelo tiene como objeto el aprendizaje automatizado de textos en base a su contenido. Se puede utilizar como anÃ¡lisis de sentimientos, detecciÃ³n de spam, categorizaciÃ³n de noticias, clasificaciÃ³n de temas o clusterizaciÃ³n, etc. \n",
    "\n",
    "#### **Modelos**\n",
    "\n",
    "â¢   **RegresiÃ³n logÃ­stica:** Son modelos lineales clÃ¡sicos que se utilizan para tareas simples de clasificaciÃ³n de texto.\n",
    "\n",
    "â¢   **Basados en redes neuronales:** Procesa secuencias de texto de longitud variable, lo que les permite modelar mejor la estructura y el contexto de las palabras en una oraciÃ³n.\n",
    "\n",
    "â¢   **Word embeddings:** Son tÃ©cnicas que convierten palabras en vectores en un espacio continuo, donde las palabras con significados similares estÃ¡n mÃ¡s cerca entre sÃ­. \n",
    "\n",
    "â¢   **BERT:** Es un modelo preentrenado que aprende relaciones bidireccionales entre palabras. Para clasificaciÃ³n de texto, se ajusta para predecir la etiqueta de una secuencia de texto. \n",
    "\n",
    "â¢   **DistilBERT:** Es una versiÃ³n mÃ¡s pequeÃ±a y eficiente de BERT que mantiene gran parte del rendimiento, pero con menos parÃ¡metros, lo que lo hace mÃ¡s rÃ¡pido de entrenar y ejecutar.\n",
    "\n",
    "â¢   **GPT (Generative Pretrained Transformer):** Aunque GPT estÃ¡ diseÃ±ado para tareas de generaciÃ³n de texto, tambiÃ©n se puede ajustar para tareas de clasificaciÃ³n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946135f3",
   "metadata": {},
   "source": [
    "#### 5.1.1 **Ejercicio**: Usar un modelo de Hugging Face (distilBERT) para clasificar un conjunto de datos de sentimientos (usar el dataset de Rotten tomatoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ff632c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_rotten_tomatos = load_dataset(\"rotten_tomatoes\", split=\"train\").shuffle(seed=42).select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d681f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_distilbert = pipeline(\"text-classification\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "083472b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_distilbert(dataset_rotten_tomatos['text'], truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "56a4d0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label     score\n",
      "0  . . . plays like somebody spliced random momen...  NEGATIVE  0.999571\n",
      "1  michael moore has perfected the art of highly ...  POSITIVE  0.999870\n",
      "2  . . . too gory to be a comedy and too silly to...  NEGATIVE  0.998669\n",
      "3  a graceful , contemplative film that gradually...  POSITIVE  0.999831\n",
      "4  the fact that the 'best part' of the movie com...  NEGATIVE  0.998292\n",
      "5  the thriller side of this movie is falling fla...  NEGATIVE  0.999801\n",
      "6                          more maudlin than sharp .  NEGATIVE  0.999730\n",
      "7  shot like a postcard and overacted with all th...  NEGATIVE  0.999769\n",
      "8  oedekerk mugs mercilessly , and the genuinely ...  NEGATIVE  0.999582\n",
      "9  haneke keeps us at arm's length . guided more ...  NEGATIVE  0.984584\n"
     ]
    }
   ],
   "source": [
    "# Convertir resultados a DataFrame y mostrar\n",
    "results_rotten_tomatos_df = pd.DataFrame(results)\n",
    "results_rotten_tomatos_df['text'] = dataset_rotten_tomatos['text']\n",
    "results_rotten_tomatos_df = results_rotten_tomatos_df[['text', 'label', 'score']]\n",
    "print(results_rotten_tomatos_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a000aa1f",
   "metadata": {},
   "source": [
    "### 5.2 **Pregunta 10**: Â¿QuÃ© es el anÃ¡lisis de sentimientos y cÃ³mo se relaciona con la clasificaciÃ³n de texto?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd7db0d",
   "metadata": {},
   "source": [
    "El **anÃ¡lisis de sentimientos** es una tÃ©cnica del procesamiento de lenguaje natural (PNL) que se utiliza para identificar y clasificar las opiniones expresadas en un texto. Generalmente, estas opiniones se categorizan en tres clases: positivas, negativas o neutras. Este proceso implica el uso de tÃ©cnicas de minerÃ­a de textos y aprendizaje automÃ¡tico para extraer informaciÃ³n subjetiva, permitiendo a las organizaciones comprender mejor las emociones y actitudes de los usuarios hacia productos, servicios o temas especÃ­ficos. El anÃ¡lisis de sentimientos se considera una forma especÃ­fica de clasificaciÃ³n de texto, donde el objetivo es asignar una etiqueta (polaridad) a un fragmento de texto basado en su contenido emocional. Esto puede incluir:\n",
    "- **ClasificaciÃ³n binaria**: donde el texto se clasifica como positivo o negativo.\n",
    "- **ClasificaciÃ³n multiclase**: que puede incluir categorÃ­as adicionales como neutral o mixta.\n",
    "\n",
    "El anÃ¡lisis de sentimientos puede ser visto como un subtipo dentro del campo mÃ¡s amplio de la clasificaciÃ³n de texto, que abarca tareas como la categorizaciÃ³n automÃ¡tica de documentos, el etiquetado temÃ¡tico y la detecciÃ³n de spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827fa79d",
   "metadata": {},
   "source": [
    "#### 5.2.1 Ejercicio: Utilizar Hugging Face para hacer anÃ¡lisis de sentimientos en un conjunto de datos de reseÃ±as (usar el dataset de IMDB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ff4157",
   "metadata": {},
   "source": [
    "##### 5.2.1.1 Cargar 10 datos del dataset IMDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "cb9bd2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\", split=\"train\").shuffle(seed=42).select(range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eae858",
   "metadata": {},
   "source": [
    "##### 5.2.1.2 Inicializar el pipeline de anÃ¡lisis de sentimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "71ede6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0adb8d",
   "metadata": {},
   "source": [
    "##### 5.2.1.3 Analizar los sentimientos de los textos del dataset de IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ea646647",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sentiment_pipeline(dataset['text'], truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eab347",
   "metadata": {},
   "source": [
    "##### 5.2.1.4 Imprimir los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "5e63c545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label     score\n",
      "0  There is no relation at all between Fortier an...  positive  0.466838\n",
      "1  This movie is a great. The plot is very true t...  positive  0.963030\n",
      "2  George P. Cosmatos' \"Rambo: First Blood Part I...  negative  0.684366\n",
      "3  In the process of trying to establish the audi...  negative  0.714222\n",
      "4  Yeh, I know -- you're quivering with excitemen...  negative  0.608305\n",
      "5  While this movie's style isn't as understated ...  positive  0.646290\n",
      "6  I give this movie 7 out of 10 because the vill...  positive  0.418127\n",
      "7  really awful... lead actor did OK... the film,...  negative  0.524330\n",
      "8  Good grief I can't even begin to describe how ...  negative  0.838564\n",
      "9  Home Room deals with a Columbine-like high-sch...  negative  0.594892\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convertir resultados a DataFrame y mostrar\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['text'] = dataset['text']\n",
    "results_df = results_df[['text', 'label', 'score']]\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d55173",
   "metadata": {},
   "source": [
    "### 5.3 **Pregunta 11**: Explica el concepto de etiquetado de secuencias (named entity recognition, NER). Â¿Para quÃ© se utiliza?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be75ae9",
   "metadata": {},
   "source": [
    "El **etiquetado de secuencias** o NER es una tarea de NLP que consiste en identificar y clasificar entidades nombradas en un texto. Las entidades nombradas pueden ser personas, organizaciones, ubicaciones, fechas, etc. Se utiliza para:\n",
    "- **ExtracciÃ³n de informaciÃ³n**: NER es fundamental para extraer informaciÃ³n relevante de grandes volÃºmenes de texto, como noticias, artÃ­culos cientÃ­ficos y documentos legales.\n",
    "- **AnÃ¡lisis de sentimientos**: Al identificar las entidades, se pueden analizar los sentimientos asociados a ellas.\n",
    "- **Sistemas de recomendaciÃ³n**: NER puede utilizarse para personalizar recomendaciones basadas en las preferencias del usuario.\n",
    "- **Chatbots y asistentes virtuales**: Los Chatbots pueden utilizar NER para comprender mejor las consultas de los usuarios y proporcionar respuestas mÃ¡s precisas.\n",
    "\n",
    "**Ejemplo**: En la frase \"El presidente de Estados Unidos visitÃ³ ParÃ­s el 14 de julio\", NER identificarÃ­a \"Estados Unidos\" como una ubicaciÃ³n, \"ParÃ­s\" como otra ubicaciÃ³n y \"14 de julio\" como una fecha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15aa3f7",
   "metadata": {},
   "source": [
    "#### 5.3.1 **Ejercicio**: Implementar un modelo NER utilizando spaCy o Hugging Face y aplicar etiquetas a un conjunto de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e7083953",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\"\n",
    "Trinamool Congress leader Mahua Moitra has moved the Supreme Court against her expulsion from the Lok Sabha over the cash-for-query allegations against her. Moitra was ousted from the Parliament last week after the Ethics Committee of the Lok Sabha found her guilty of jeopardising national security by sharing her parliamentary portal's login credentials with businessman Darshan Hiranandani.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e339d7da",
   "metadata": {},
   "source": [
    "##### 5.3.1.1 Utilizar spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c60f0849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "678fc631",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "5191b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "9e695d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in doc.ents:\n",
    "    print(f\"This word {word.text} is this {word.label_} entity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf8b521",
   "metadata": {},
   "source": [
    "#### 3.1.1.2 Utilizar Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9f383e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bert_ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "6406bafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "ner_results = bert_ner(text)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbdc21a",
   "metadata": {},
   "source": [
    "## 6. GeneraciÃ³n de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d1fd75",
   "metadata": {},
   "source": [
    "### 6.1 **Pregunta 12**: Â¿CÃ³mo funcionan los modelos de generaciÃ³n de texto como GPT-2 o GPT-3?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67e8672",
   "metadata": {},
   "source": [
    "Los modelos de GPT2 y GPT3 son modelos de lenguaje basados en Transformers. BÃ¡sicamente se usan para la predicciÃ³n de la siguiente palabra a travÃ©s de una alimentaciÃ³n de bases de informaciÃ³n para poder brindar un contexto acorde a lo que se estÃ¡ solicitando.\n",
    "\n",
    "El proceso de preentrenamiento de GPT-2 y GPT-3 consiste en exponer al modelo a grandes cantidades de texto de internet (corpus masivos) y hacer que aprenda a predecir la siguiente palabra en una secuencia. Este proceso se llama modelado de lenguaje causal y se basa en el principio de aprendizaje no supervisado.\n",
    "\n",
    "GPT-2 tiene 1.5 mil millones de parÃ¡metros, mientras que GPT-3 es significativamente mÃ¡s grande, con 175 mil millones de parÃ¡metros. Esto le da a GPT-3 una capacidad mucho mayor para aprender patrones mÃ¡s complejos y manejar tareas mÃ¡s diversas. Algunas diferencias clave son:\n",
    "\n",
    "â¢   Mayor capacidad de comprensiÃ³n y generaciÃ³n: GPT-3 puede generar texto mÃ¡s coherente y manejar mejor el contexto a largo plazo en comparaciÃ³n con GPT-2.\n",
    "\n",
    "â¢   Capacidades multitarea: GPT-3, gracias a su tamaÃ±o, es capaz de realizar una variedad de tareas sin necesitar entrenamiento adicional (fine-tuning). Puede traducir texto, responder preguntas, realizar razonamientos matemÃ¡ticos, y mÃ¡s, simplemente a travÃ©s de una buena promociÃ³n o instrucciÃ³n (few-shot learning).\n",
    "\n",
    "â¢   Few-shot, one-shot, y zero-shot learning: GPT-3 es muy efectivo en estos modos de aprendizaje, donde puede resolver tareas con solo un pequeÃ±o nÃºmero de ejemplos o incluso sin ejemplos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bb2cbf",
   "metadata": {},
   "source": [
    "#### 6.1.1 **Ejercicio**: Usar Hugging Face para generar texto a partir de un prompt con GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ab84d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del modelo localmente\n",
    "generator = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f3343d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generar texto\n",
    "response = generator(\"describe the benefits to eat salads everyday\",\n",
    "                     max_length=1024,\n",
    "                     do_sample=True,\n",
    "                     temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "2a9fa9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "describe the benefits to eat salads everyday.\n",
      "\n",
      "How can I get started with a nutrition plan?\n",
      "\n",
      "To find out more about nutrition, visit the Healthy Hunger-Free Eating Guide.\n",
      "\n",
      "Find out about any food-related health problems that you might be experiencing.\n",
      "\n",
      "Find out how you can get more out of your diet.\n",
      "\n",
      "And for more nutrition tips, visit the Healthy Hunger-Free Living Guide.\n",
      "\n",
      "Related\n",
      "\n",
      "Share this: Email\n",
      "\n",
      "Facebook\n",
      "\n",
      "LinkedIn\n",
      "\n",
      "Pinterest\n",
      "\n",
      "Twitter\n",
      "\n",
      "Google\n",
      "\n",
      "Tumblr\n",
      "\n",
      "Reddit\n",
      "\n",
      "More\n",
      "\n",
      "Print\n",
      "\n",
      "Pocket\n",
      "\n",
      "\n",
      "Like this: Like Loading...\n"
     ]
    }
   ],
   "source": [
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d3e2b",
   "metadata": {},
   "source": [
    "### 6.2 **Pregunta 13**: Â¿CuÃ¡les son los desafÃ­os Ã©ticos asociados con los modelos de generaciÃ³n de texto?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ed02b2",
   "metadata": {},
   "source": [
    "Los modelos de generaciÃ³n de texto (NLP), han avanzado significativamente en los Ãºltimos aÃ±os, pero tambiÃ©n plantean desafÃ­os Ã©ticos cruciales; algunos de los mÃ¡s relevantes pueden ser:\n",
    "- **Sesgos**: Los modelos aprenden de los datos con los que son entrenados. Si estos datos contienen sesgos sociales, culturales o de otro tipo, el modelo los reproducirÃ¡ y amplificarÃ¡ en sus salidas. Esto puede llevar a la discriminaciÃ³n, la perpetuaciÃ³n de estereotipos y la generaciÃ³n de contenido ofensivo o daÃ±ino.\n",
    "- **DesinformaciÃ³n y \"deepfakes\" de texto**: La capacidad de generar texto realista y coherente puede ser utilizada para crear noticias falsas, propaganda o contenido engaÃ±oso. Esto plantea serias amenazas a la democracia y la confianza en la informaciÃ³n.\n",
    "- **Privacidad**: Los modelos pueden ser entrenados con grandes cantidades de datos que incluyen informaciÃ³n personal. Esto plantea preocupaciones sobre la privacidad y la seguridad de los datos.\n",
    "- **Responsabilidad**: Â¿QuiÃ©n es responsable del contenido generado por un modelo? Â¿El desarrollador, el usuario o el modelo mismo? Esta pregunta es compleja y aÃºn no tiene una respuesta clara.\n",
    "- **Mal uso**: Los modelos de lenguaje pueden ser utilizados para cometer delitos, como el acoso en lÃ­nea o la extorsiÃ³n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c004df",
   "metadata": {},
   "source": [
    "#### 6.2.1 **Ejercicio**: Discutir sobre sesgos en los modelos de lenguaje generativo y su impacto en aplicaciones reales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7519a32",
   "metadata": {},
   "source": [
    " Los sesgos en los modelos de lenguaje generativo tienen un impacto significativo en diversas aplicaciones reales. Algunos ejemplos incluyen:\n",
    "\n",
    "- **TraducciÃ³n automÃ¡tica**: Un modelo entrenado con datos sesgados puede generar traducciones que refuerzan estereotipos o discriminan a ciertos grupos.\n",
    "\n",
    "- **Chatbots y asistentes virtuales**: Un chatbot sesgado puede proporcionar respuestas discriminatorias u ofensivas a preguntas o solicitudes.\n",
    "\n",
    "- **GeneraciÃ³n de contenido creativo**: Un modelo puede generar contenido que perpetÃºe estereotipos de gÃ©nero, raza u orientaciÃ³n sexual.\n",
    "\n",
    "- **Toma de decisiones**: Los modelos de lenguaje se utilizan cada vez mÃ¡s para tomar decisiones en Ã¡reas como la contrataciÃ³n, la justicia penal y la atenciÃ³n mÃ©dica. Si estos modelos estÃ¡n sesgados, pueden llevar a resultados injustos.\n",
    "\n",
    "Para mitigar estos problemas, es necesario:\n",
    "\n",
    "- **Datos de entrenamiento diversos y representativos**: Los conjuntos de datos utilizados para entrenar los modelos deben ser lo mÃ¡s diversos y representativos posible para reducir la probabilidad de sesgos.\n",
    "\n",
    "- **TÃ©cnicas de mitigaciÃ³n de sesgos**: Existen diversas tÃ©cnicas para identificar y mitigar los sesgos en los modelos, como la detecciÃ³n de sesgos, la correcciÃ³n de sesgos y la auditorÃ­a de modelos.\n",
    "\n",
    "- **Transparencia y explicabilidad**: Es importante que los desarrolladores de modelos sean transparentes sobre los datos utilizados para entrenar los modelos y sobre las tÃ©cnicas de mitigaciÃ³n de sesgos empleadas.\n",
    "\n",
    "- **EvaluaciÃ³n continua**: Los modelos deben ser evaluados continuamente para detectar y corregir nuevos sesgos que puedan surgir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae312f3",
   "metadata": {},
   "source": [
    "## 7. EvaluaciÃ³n de Modelos NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcff8ceb",
   "metadata": {},
   "source": [
    "### 7.1 **Pregunta 14**: Â¿QuÃ© mÃ©tricas se utilizan para evaluar la calidad de los modelos de NLP en tareas como clasificaciÃ³n o generaciÃ³n de texto?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099fde33",
   "metadata": {},
   "source": [
    "Existen diversas mÃ©tricas para evaluar la calidad de los modelos de NLP, dependiendo de la tarea especÃ­fica. Algunas de las mÃ¡s comunes son:\n",
    "\n",
    "- **PrecisiÃ³n (Precision), Recuerdo (Recall) y F1-score**: Estas mÃ©tricas se utilizan comÃºnmente en tareas de clasificaciÃ³n, como el anÃ¡lisis de sentimientos o la clasificaciÃ³n de textos. Miden la proporciÃ³n de predicciones correctas, la proporciÃ³n de elementos relevantes recuperados y el equilibrio entre ambas, respectivamente.\n",
    "- **BLEU (Bilingual Evaluation Understudy)**: Se utiliza para evaluar la calidad de la traducciÃ³n automÃ¡tica y la generaciÃ³n de texto. Compara las n-gramas de las traducciones generadas con las de las traducciones de referencia.\n",
    "- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: Similar a BLEU, pero se enfoca en la recuperaciÃ³n de n-gramas, lo que lo hace mÃ¡s adecuado para tareas de resumen automÃ¡tico.\n",
    "- **Perplejidad**: Mide la sorpresa de un modelo ante una nueva secuencia de palabras. Un modelo bien entrenado deberÃ­a tener una baja perplejidad.\n",
    "- **Coherencia y fluidez**: Para evaluar la calidad de la generaciÃ³n de texto, se pueden utilizar mÃ©tricas subjetivas como la coherencia y la fluidez del texto generado.\n",
    "\n",
    "La elecciÃ³n de la mÃ©trica adecuada depende aspectos como:\n",
    "- La tarea especÃ­fica (clasificaciÃ³n, generaciÃ³n, etc.)\n",
    "- Los datos disponibles\n",
    "- Los objetivos del proyecto\n",
    "\n",
    "En general, el enmascaramiento de palabras es una tÃ©cnica clave para preentrenar modelos de lenguaje, NER es una tarea fundamental para extraer informaciÃ³n de texto, y existen diversas mÃ©tricas para evaluar la calidad de los modelos de NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd0515c",
   "metadata": {},
   "source": [
    "#### 7.1.1 **Ejercicio**: Usar mÃ©tricas como accuracy, precision, y recall para evaluar un modelo de clasificaciÃ³n de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfffb9d9",
   "metadata": {},
   "source": [
    "##### 7.1.1.1 Guardar las mÃ©tricas de accuracy, precision y recall en una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "658e60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"accuracy\", \"recall\", \"precision\", \"f1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4081e248",
   "metadata": {},
   "source": [
    "##### 7.1.1.2 Cargar los datos de imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "1d121ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"imdb\", split=\"test\").shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663f3b2a",
   "metadata": {},
   "source": [
    "##### 7.1.1.3 Cargar el evalvador para clasificaciÃ³n de textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "e8a231c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_evaluator = evaluator(\"text-classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb264e1",
   "metadata": {},
   "source": [
    "##### 7.1.1.4 Cargar el modelo de clasificaciÃ³n de textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "179a2456",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_classification = pipeline(\"text-classification\", model=\"lvwerra/distilbert-imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f66c7",
   "metadata": {},
   "source": [
    "##### 7.1.1.5 Evaluar los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ce349859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"accuracy\": 0.918,\n",
      "    \"recall\": 0.9180327868852459,\n",
      "    \"precision\": 0.9142857142857143,\n",
      "    \"f1\": 0.9161554192229039,\n",
      "    \"total_time_in_seconds\": 100.83727800002089,\n",
      "    \"samples_per_second\": 9.91696741357688,\n",
      "    \"latency_in_seconds\": 0.10083727800002089\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline = imdb_classification,\n",
    "    data = data,\n",
    "    metric = evaluate.combine(metrics),\n",
    "    label_mapping = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    ")\n",
    "print(json.dumps(eval_results, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dfc506",
   "metadata": {},
   "source": [
    "### 7.2 **Pregunta 15**: Â¿QuÃ© es la perplejidad y cÃ³mo se utiliza para evaluar modelos de generaciÃ³n de texto?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8388fd80",
   "metadata": {},
   "source": [
    "#### **Â¿QuÃ© es la perplejidad?**\n",
    "La perplejidad es uno de las mÃ©tricas mas comunes para evaluar modelos de lenguaje. Esta mÃ©trica aplica especÃ­ficamente al clÃ¡sico modelo de lenguaje (a veces llamado autorregresivo o modelos de lenguaje causal) y no es definido como modelos similares a BERT. \n",
    "\n",
    "#### **Â¿CÃ³mo se utiliza para evaluar modelos de generaciÃ³n de texto?**\n",
    "Se evalÃºa por factorizaciÃ³n autorregresiva una secuencia y condicionando todo el subsecuente precedente en cada paso. Cuando se trabaja con modelos de aproximaciÃ³n, sin embargo, tÃ­picamente se tiene una restricciÃ³n en el numero de tokens del modelo puede proceder. La versiÃ³n mas grande de GPT-2, por ejemplo, se ha arreglado la longitud de 1024 tokens, entonces no se puede calcular la probabilidad si es mas grande que 1024 tokens.\n",
    "\n",
    "En cambio, la secuencia esta tÃ­picamente dividida en subsecuencias iguales al mÃ¡ximo numero de entradas del modelo. Si el tamaÃ±o mÃ¡ximo de la entrada es K, se aproxima la probabilidad condicionando solo K. El token que lo precede en el lugar del texto completo. Cuando se evalÃºa un modelo de perplejidad de una secuencia, un enfoque puede ser la divisiÃ³n en fragmentos disjuntos y sumar las probabilidades logarÃ­tmicas descompuestas de cada segmento en forma independiente. \n",
    "\n",
    "Esta es la forma rÃ¡pida para computar desde la perplejidad de cada segmento puede ser computado una pasada hacia adelante, pero funciona como una aproximaciÃ³n pobre de la perplejidad completamente factorizada, por lo cual se producirÃ¡ un PPL mÃ¡s porque el modelo tendrÃ¡ menos contexto o genera respuestas errÃ³neas en el resultado de predicciÃ³n. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e460386",
   "metadata": {},
   "source": [
    "#### 7.2.1 **Ejercicio**: Calcular la perplejidad de un modelo de lenguaje en Hugging Face para una tarea de generaciÃ³n de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fcea42",
   "metadata": {},
   "source": [
    "##### 7.2.1.1 Utilizar gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "129a82a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model_id = \"openai-community/gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c48f89b",
   "metadata": {},
   "source": [
    "##### 7.2.1.2 Cargar los datos de WikiText-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "ff629f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2b3cfa",
   "metadata": {},
   "source": [
    "##### 7.2.1.3 Calcular la perplejidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9fc18826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 560/562 [11:14<00:02,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "max_length = model.config.n_positions\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # puede ser diferente del paso en el Ãºltimo bucle\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # la pÃ©rdida se calcula utilizando CrossEntropyLoss, que promedia las etiquetas vÃ¡lidas\n",
    "        # N.B. el modelo solo calcula la pÃ©rdida sobre trg_len - 1 etiquetas, porque desplaza las etiqueta internamente\n",
    "        # a la izquierda por 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "aa68dd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 25.187597274780273\n"
     ]
    }
   ],
   "source": [
    "print(f\"Perplexity: {ppl.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
