{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90fd06a3",
   "metadata": {},
   "source": [
    "# Taller 1 Conceptos bÃ¡sicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274752a1",
   "metadata": {},
   "source": [
    "Autores:\n",
    "- Guillermo Luigui Ubaldo Nieto Angarita\n",
    "- Daniel Hernando Moyano Salamanca\n",
    "- Jairo Antonio Viteri Rojas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f2bfc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\guill\\Documents\\GitHub\\Intro-IA-Generativa-Taller-1-Conceptos-Basicos>() \n"
     ]
    }
   ],
   "source": [
    "!myenv\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a46b9e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.2.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: transformers==4.44.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 3)) (4.44.2)\n",
      "Requirement already satisfied: nltk==3.9.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 4)) (3.9.1)\n",
      "Requirement already satisfied: spacy==3.7.5 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 5)) (3.7.5)\n",
      "Requirement already satisfied: torch in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 7)) (2.3.1)\n",
      "Requirement already satisfied: nbdime==4.0.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 8)) (4.0.2)\n",
      "Requirement already satisfied: datasets==3.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 9)) (3.0.1)\n",
      "Requirement already satisfied: model2vec in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 10)) (0.2.3)\n",
      "Requirement already satisfied: evaluate==0.4.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 11)) (0.4.3)\n",
      "Requirement already satisfied: tf-keras==2.16 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements.txt (line 12)) (2.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==2.2.3->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==2.2.3->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==2.2.3->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (0.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.44.2->-r requirements.txt (line 3)) (4.66.5)\n",
      "Requirement already satisfied: click in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk==3.9.1->-r requirements.txt (line 4)) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk==3.9.1->-r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (0.12.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (2.6.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (65.5.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy==3.7.5->-r requirements.txt (line 5)) (3.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 8)) (0.4.6)\n",
      "Requirement already satisfied: gitpython!=2.1.4,!=2.1.5,!=2.1.6 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 8)) (3.1.43)\n",
      "Requirement already satisfied: jupyter-server in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 8)) (2.14.2)\n",
      "Requirement already satisfied: jupyter-server-mathjax>=0.2.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 8)) (0.2.6)\n",
      "Requirement already satisfied: nbformat in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 8)) (5.10.4)\n",
      "Requirement already satisfied: pygments in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 8)) (2.18.0)\n",
      "Requirement already satisfied: tornado in c:\\users\\guill\\appdata\\roaming\\python\\python311\\site-packages (from nbdime==4.0.2->-r requirements.txt (line 8)) (6.4.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets==3.0.1->-r requirements.txt (line 9)) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets==3.0.1->-r requirements.txt (line 9)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets==3.0.1->-r requirements.txt (line 9)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets==3.0.1->-r requirements.txt (line 9)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==3.0.1->-r requirements.txt (line 9)) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets==3.0.1->-r requirements.txt (line 9)) (3.10.9)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tf-keras==2.16->-r requirements.txt (line 12)) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->-r requirements.txt (line 7)) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->-r requirements.txt (line 7)) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->-r requirements.txt (line 7)) (3.3)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->-r requirements.txt (line 7)) (2021.4.0)\n",
      "Requirement already satisfied: rich in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from model2vec->-r requirements.txt (line 10)) (13.9.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from model2vec->-r requirements.txt (line 10)) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets==3.0.1->-r requirements.txt (line 9)) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets==3.0.1->-r requirements.txt (line 9)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets==3.0.1->-r requirements.txt (line 9)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets==3.0.1->-r requirements.txt (line 9)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets==3.0.1->-r requirements.txt (line 9)) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets==3.0.1->-r requirements.txt (line 9)) (1.14.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gitpython!=2.1.4,!=2.1.5,!=2.1.6->nbdime==4.0.2->-r requirements.txt (line 8)) (4.0.11)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy==3.7.5->-r requirements.txt (line 5)) (2.1.5)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (4.3.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (23.1.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in c:\\users\\guill\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\guill\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (5.7.2)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (0.5.3)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (7.16.4)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (0.21.0)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (2.0.13)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\guill\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (26.2.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (0.18.1)\n",
      "Requirement already satisfied: traitlets>=5.6.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (5.14.3)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (1.8.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.5->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->-r requirements.txt (line 7)) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->-r requirements.txt (line 7)) (2021.13.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbformat->nbdime==4.0.2->-r requirements.txt (line 8)) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbformat->nbdime==4.0.2->-r requirements.txt (line 8)) (4.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5->-r requirements.txt (line 5)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.5->-r requirements.txt (line 5)) (2.16.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.2.3->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.44.2->-r requirements.txt (line 3)) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.44.2->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.44.2->-r requirements.txt (line 3)) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.44.2->-r requirements.txt (line 3)) (2023.7.22)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (4.25.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (3.6.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (0.31.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.5->-r requirements.txt (line 5)) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.5->-r requirements.txt (line 5)) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.5->-r requirements.txt (line 5)) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->model2vec->-r requirements.txt (line 10)) (3.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.5->-r requirements.txt (line 5)) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.5->-r requirements.txt (line 5)) (7.0.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->model2vec->-r requirements.txt (line 10)) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->model2vec->-r requirements.txt (line 10)) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch->-r requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio>=3.1.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (21.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=2.1.4,!=2.1.5,!=2.1.6->nbdime==4.0.2->-r requirements.txt (line 8)) (5.0.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat->nbdime==4.0.2->-r requirements.txt (line 8)) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat->nbdime==4.0.2->-r requirements.txt (line 8)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=2.6->nbformat->nbdime==4.0.2->-r requirements.txt (line 8)) (0.20.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (4.2.2)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\guill\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (306)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (0.1.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.5->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->model2vec->-r requirements.txt (line 10)) (0.1.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (0.10.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (1.5.1)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==3.0.1->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (0.44.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (0.5.1)\n",
      "Requirement already satisfied: fqdn in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (3.0.0)\n",
      "Requirement already satisfied: uri-template in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (24.8.0)\n",
      "Requirement already satisfied: namex in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (0.13.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras==2.16->-r requirements.txt (line 12)) (3.0.4)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (2.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server->nbdime==4.0.2->-r requirements.txt (line 8)) (2.9.0.20241003)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efa107ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import json\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from transformers import pipeline, set_seed\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from evaluate import evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eec24ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c612c8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     - ------------------------------------- 0.5/12.8 MB 670.4 kB/s eta 0:00:19\n",
      "     - ------------------------------------- 0.5/12.8 MB 670.4 kB/s eta 0:00:19\n",
      "     -- ------------------------------------ 0.8/12.8 MB 621.2 kB/s eta 0:00:20\n",
      "     -- ------------------------------------ 0.8/12.8 MB 621.2 kB/s eta 0:00:20\n",
      "     --- ----------------------------------- 1.0/12.8 MB 636.8 kB/s eta 0:00:19\n",
      "     --- ----------------------------------- 1.0/12.8 MB 636.8 kB/s eta 0:00:19\n",
      "     --- ----------------------------------- 1.0/12.8 MB 636.8 kB/s eta 0:00:19\n",
      "     --- ----------------------------------- 1.3/12.8 MB 621.2 kB/s eta 0:00:19\n",
      "     --- ----------------------------------- 1.3/12.8 MB 621.2 kB/s eta 0:00:19\n",
      "     ---- ---------------------------------- 1.6/12.8 MB 582.5 kB/s eta 0:00:20\n",
      "     ---- ---------------------------------- 1.6/12.8 MB 582.5 kB/s eta 0:00:20\n",
      "     ---- ---------------------------------- 1.6/12.8 MB 582.5 kB/s eta 0:00:20\n",
      "     ----- --------------------------------- 1.8/12.8 MB 572.0 kB/s eta 0:00:20\n",
      "     ----- --------------------------------- 1.8/12.8 MB 572.0 kB/s eta 0:00:20\n",
      "     ------ -------------------------------- 2.1/12.8 MB 584.2 kB/s eta 0:00:19\n",
      "     ------ -------------------------------- 2.1/12.8 MB 584.2 kB/s eta 0:00:19\n",
      "     ------- ------------------------------- 2.4/12.8 MB 588.6 kB/s eta 0:00:18\n",
      "     ------- ------------------------------- 2.4/12.8 MB 588.6 kB/s eta 0:00:18\n",
      "     ------- ------------------------------- 2.6/12.8 MB 589.8 kB/s eta 0:00:18\n",
      "     ------- ------------------------------- 2.6/12.8 MB 589.8 kB/s eta 0:00:18\n",
      "     -------- ------------------------------ 2.9/12.8 MB 594.8 kB/s eta 0:00:17\n",
      "     -------- ------------------------------ 2.9/12.8 MB 594.8 kB/s eta 0:00:17\n",
      "     --------- ----------------------------- 3.1/12.8 MB 597.1 kB/s eta 0:00:17\n",
      "     --------- ----------------------------- 3.1/12.8 MB 597.1 kB/s eta 0:00:17\n",
      "     ---------- ---------------------------- 3.4/12.8 MB 606.3 kB/s eta 0:00:16\n",
      "     ----------- --------------------------- 3.7/12.8 MB 616.0 kB/s eta 0:00:15\n",
      "     ----------- --------------------------- 3.7/12.8 MB 616.0 kB/s eta 0:00:15\n",
      "     ----------- --------------------------- 3.7/12.8 MB 616.0 kB/s eta 0:00:15\n",
      "     ----------- --------------------------- 3.9/12.8 MB 613.2 kB/s eta 0:00:15\n",
      "     ------------ -------------------------- 4.2/12.8 MB 618.3 kB/s eta 0:00:14\n",
      "     ------------ -------------------------- 4.2/12.8 MB 618.3 kB/s eta 0:00:14\n",
      "     ------------- ------------------------- 4.5/12.8 MB 619.9 kB/s eta 0:00:14\n",
      "     ------------- ------------------------- 4.5/12.8 MB 619.9 kB/s eta 0:00:14\n",
      "     -------------- ------------------------ 4.7/12.8 MB 622.7 kB/s eta 0:00:13\n",
      "     -------------- ------------------------ 4.7/12.8 MB 622.7 kB/s eta 0:00:13\n",
      "     --------------- ----------------------- 5.0/12.8 MB 625.2 kB/s eta 0:00:13\n",
      "     --------------- ----------------------- 5.0/12.8 MB 625.2 kB/s eta 0:00:13\n",
      "     --------------- ----------------------- 5.2/12.8 MB 628.7 kB/s eta 0:00:13\n",
      "     --------------- ----------------------- 5.2/12.8 MB 628.7 kB/s eta 0:00:13\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 626.0 kB/s eta 0:00:12\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 626.0 kB/s eta 0:00:12\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 626.0 kB/s eta 0:00:12\n",
      "     ----------------- --------------------- 5.8/12.8 MB 618.1 kB/s eta 0:00:12\n",
      "     ----------------- --------------------- 5.8/12.8 MB 618.1 kB/s eta 0:00:12\n",
      "     ------------------ -------------------- 6.0/12.8 MB 612.1 kB/s eta 0:00:12\n",
      "     ------------------ -------------------- 6.0/12.8 MB 612.1 kB/s eta 0:00:12\n",
      "     ------------------ -------------------- 6.0/12.8 MB 612.1 kB/s eta 0:00:12\n",
      "     ------------------- ------------------- 6.3/12.8 MB 603.8 kB/s eta 0:00:11\n",
      "     ------------------- ------------------- 6.3/12.8 MB 603.8 kB/s eta 0:00:11\n",
      "     ------------------- ------------------- 6.6/12.8 MB 600.0 kB/s eta 0:00:11\n",
      "     ------------------- ------------------- 6.6/12.8 MB 600.0 kB/s eta 0:00:11\n",
      "     -------------------- ------------------ 6.8/12.8 MB 599.2 kB/s eta 0:00:10\n",
      "     -------------------- ------------------ 6.8/12.8 MB 599.2 kB/s eta 0:00:10\n",
      "     --------------------- ----------------- 7.1/12.8 MB 600.8 kB/s eta 0:00:10\n",
      "     --------------------- ----------------- 7.1/12.8 MB 600.8 kB/s eta 0:00:10\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 605.6 kB/s eta 0:00:10\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 605.6 kB/s eta 0:00:10\n",
      "     ----------------------- --------------- 7.6/12.8 MB 603.0 kB/s eta 0:00:09\n",
      "     ----------------------- --------------- 7.6/12.8 MB 603.0 kB/s eta 0:00:09\n",
      "     ----------------------- --------------- 7.6/12.8 MB 603.0 kB/s eta 0:00:09\n",
      "     ----------------------- --------------- 7.9/12.8 MB 599.2 kB/s eta 0:00:09\n",
      "     ----------------------- --------------- 7.9/12.8 MB 599.2 kB/s eta 0:00:09\n",
      "     ------------------------ -------------- 8.1/12.8 MB 596.3 kB/s eta 0:00:08\n",
      "     ------------------------ -------------- 8.1/12.8 MB 596.3 kB/s eta 0:00:08\n",
      "     ------------------------- ------------- 8.4/12.8 MB 596.4 kB/s eta 0:00:08\n",
      "     ------------------------- ------------- 8.4/12.8 MB 596.4 kB/s eta 0:00:08\n",
      "     -------------------------- ------------ 8.7/12.8 MB 599.2 kB/s eta 0:00:07\n",
      "     -------------------------- ------------ 8.7/12.8 MB 599.2 kB/s eta 0:00:07\n",
      "     -------------------------- ------------ 8.7/12.8 MB 599.2 kB/s eta 0:00:07\n",
      "     --------------------------- ----------- 8.9/12.8 MB 597.2 kB/s eta 0:00:07\n",
      "     --------------------------- ----------- 8.9/12.8 MB 597.2 kB/s eta 0:00:07\n",
      "     --------------------------- ----------- 9.2/12.8 MB 597.3 kB/s eta 0:00:07\n",
      "     ---------------------------- ---------- 9.4/12.8 MB 599.8 kB/s eta 0:00:06\n",
      "     ---------------------------- ---------- 9.4/12.8 MB 599.8 kB/s eta 0:00:06\n",
      "     ----------------------------- --------- 9.7/12.8 MB 603.4 kB/s eta 0:00:06\n",
      "     ----------------------------- --------- 9.7/12.8 MB 603.4 kB/s eta 0:00:06\n",
      "     ----------------------------- -------- 10.0/12.8 MB 603.8 kB/s eta 0:00:05\n",
      "     ----------------------------- -------- 10.0/12.8 MB 603.8 kB/s eta 0:00:05\n",
      "     ------------------------------ ------- 10.2/12.8 MB 604.9 kB/s eta 0:00:05\n",
      "     ------------------------------ ------- 10.2/12.8 MB 604.9 kB/s eta 0:00:05\n",
      "     ------------------------------- ------ 10.5/12.8 MB 605.3 kB/s eta 0:00:04\n",
      "     ------------------------------- ------ 10.5/12.8 MB 605.3 kB/s eta 0:00:04\n",
      "     ------------------------------- ------ 10.7/12.8 MB 606.2 kB/s eta 0:00:04\n",
      "     ------------------------------- ------ 10.7/12.8 MB 606.2 kB/s eta 0:00:04\n",
      "     -------------------------------- ----- 11.0/12.8 MB 604.4 kB/s eta 0:00:03\n",
      "     -------------------------------- ----- 11.0/12.8 MB 604.4 kB/s eta 0:00:03\n",
      "     --------------------------------- ---- 11.3/12.8 MB 605.3 kB/s eta 0:00:03\n",
      "     --------------------------------- ---- 11.3/12.8 MB 605.3 kB/s eta 0:00:03\n",
      "     ---------------------------------- --- 11.5/12.8 MB 609.8 kB/s eta 0:00:03\n",
      "     ---------------------------------- --- 11.5/12.8 MB 609.8 kB/s eta 0:00:03\n",
      "     ----------------------------------- -- 11.8/12.8 MB 611.6 kB/s eta 0:00:02\n",
      "     ----------------------------------- -- 11.8/12.8 MB 611.6 kB/s eta 0:00:02\n",
      "     ----------------------------------- -- 12.1/12.8 MB 610.3 kB/s eta 0:00:02\n",
      "     ----------------------------------- -- 12.1/12.8 MB 610.3 kB/s eta 0:00:02\n",
      "     ------------------------------------ - 12.3/12.8 MB 611.5 kB/s eta 0:00:01\n",
      "     ------------------------------------ - 12.3/12.8 MB 611.5 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.6/12.8 MB 612.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 12.8/12.8 MB 613.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.2)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\guill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "\u001b[38;5;2mâ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb0a2e",
   "metadata": {},
   "source": [
    "## 1. IntroducciÃ³n a NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce6930",
   "metadata": {},
   "source": [
    "### 1.1. Pregunta 1: Â¿QuÃ© es el Procesamiento de Lenguaje Natural y cuÃ¡les son sus principales aplicaciones?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a6fe09",
   "metadata": {},
   "source": [
    "El procesamiento del lenguaje natural (NLP) es un campo de la IA que permite a las mÃ¡quinas comprender, interpretar, generar y manipular el lenguaje humano de forma natural, lo que facilita las interacciones entre humanos y ordenadores.\n",
    "\n",
    "El NLP utiliza modelos estadÃ­sticos, tÃ©cnicas de aprendizaje automÃ¡tico y, mÃ¡s recientemente, arquitecturas de Deep Learning como Transformers (por ejemplo, BERT y GPT). Estos modelos convierten el texto en representaciones matemÃ¡ticas, como embeddings, permitiendo que las mÃ¡quinas detecten patrones, relaciones semÃ¡nticas y contexto.\n",
    "\n",
    "Las aplicaciones principales del PLN incluyen:\n",
    "- **AnÃ¡lisis de Sentimientos**: Detectar la polaridad (positiva, negativa o neutra) en textos como reseÃ±as, redes sociales o encuestas.\n",
    "- **Chatbots y Asistentes Virtuales**: Automatizar la interacciÃ³n con los usuarios mediante respuestas precisas y contextuales.\n",
    "- **TraducciÃ³n AutomÃ¡tica**: Transformar texto de un idioma a otro utilizando redes neuronales para mantener coherencia y fluidez.\n",
    "- **Resumen AutomÃ¡tico de Textos**: Condensar informaciÃ³n larga en versiones mÃ¡s breves sin perder los puntos clave.\n",
    "- **DetecciÃ³n de Spam y Filtrado de Contenidos**: Clasificar mensajes o contenido inapropiado para mejorar la experiencia del usuario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc47014",
   "metadata": {},
   "source": [
    "### 1.1.1 Ejercicio: Investigar 6 aplicaciones actuales de NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f7ea2",
   "metadata": {},
   "source": [
    "- **ChatGPT (Asistente Conversacional Generativo)**: es un modelo de lenguaje basado en la arquitectura GPT-4. Se utiliza en mÃºltiples sectores para ofrecer atenciÃ³n automatizada a clientes, resolver preguntas complejas y brindar soporte tÃ©cnico. Su capacidad de comprender y mantener conversaciones contextuales mejora la eficiencia y reduce los tiempos de respuesta en empresas de servicios financieros, comercio electrÃ³nico y salud. Mejora la experiencia del cliente al ofrecer atenciÃ³n continua y personalizada, reduciendo la carga de trabajo en los centros de atenciÃ³n al cliente.\n",
    "\n",
    "- **DALL-E3 (GeneraciÃ³n de ImÃ¡genes a partir de Texto)**: Aunque se centra en la generaciÃ³n de imÃ¡genes, DALL-E combina PLN con modelos de visiÃ³n computacional para interpretar descripciones complejas y convertirlas en representaciones visuales. Esta tecnologÃ­a tiene aplicaciones en marketing, diseÃ±o y educaciÃ³n. Permite crear contenido visual sin necesidad de conocimientos avanzados en diseÃ±o, optimizando la producciÃ³n creativa en campaÃ±as publicitarias y proyectos educativos.\n",
    "\n",
    "- **MedPaLM (Modelo especializado en PLN para salud)**: MedPaLM es un modelo optimizado para procesar informaciÃ³n mÃ©dica, responder preguntas clÃ­nicas y brindar recomendaciones basadas en evidencia. EstÃ¡ diseÃ±ado para manejar textos tÃ©cnicos y resolver preguntas complejas en medicina. Mejora la eficiencia de los profesionales de la salud al proporcionar resÃºmenes rÃ¡pidos y precisos de investigaciones, diagnÃ³sticos y tratamientos, facilitando la toma de decisiones en entornos clÃ­nicos.\n",
    "\n",
    "- **Asistentes Virtuales de Servicio al Cliente (AVSC)**: son sistemas alimentados por modelos de PLN para ofrecer soporte al cliente 24/7. Pueden responder preguntas, resolver problemas comunes y guiar a los usuarios a travÃ©s de procesos complejos. Los AVSC aumentan la eficiencia operativa al reducir el tiempo de espera y atender mÃºltiples consultas simultÃ¡neamente, permitiendo a las empresas ofrecer un mejor servicio sin aumentar la carga de trabajo en el personal. Los asistentes virtuales ayudan a reducir costos operativos y mejoran la satisfacciÃ³n del cliente al proporcionar respuestas rÃ¡pidas y precisas.\n",
    "\n",
    "- **TraducciÃ³n AutomÃ¡tica Avanzada**: Servicios como Google Translate han mejorado notablemente gracias a los avances en PLN. Utilizan modelos de traducciÃ³n automÃ¡tica neuronales que comprenden el contexto y las sutilezas del lenguaje humano. Estos sistemas facilitan la comunicaciÃ³n entre personas que hablan diferentes idiomas, rompiendo barreras lingÃ¼Ã­sticas en negocios, turismo y relaciones personales; ayudan a prevenir malentendidos en la comunicaciÃ³n y permiten el acceso a informaciÃ³n y servicios en mÃºltiples idiomas.\n",
    "\n",
    "- **AnÃ¡lisis de Sentimientos en Redes Sociales**: herramientas tecnolÃ³gicas que utilizan tÃ©cnicas de PLN para evaluar las opiniones y emociones expresadas en redes sociales y reseÃ±as en lÃ­nea. Estas herramientas pueden clasificar el contenido como positivo, negativo o neutral. Permiten a las empresas y organizaciones comprender mejor la percepciÃ³n pÃºblica sobre sus productos, servicios o iniciativas. Ayudan a las marcas a identificar problemas emergentes y Ã¡reas de mejora en sus ofertas, a la vez que permiten a los responsables de marketing ajustar estrategias en tiempo real basÃ¡ndose en la retroalimentaciÃ³n del consumidor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Pregunta 2: Explica los conceptos de tokenizaciÃ³n y segmentaciÃ³n de oraciones. Â¿Por quÃ© son importantes en NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed4a79",
   "metadata": {},
   "source": [
    "La tokenizaciÃ³n es el proceso de dividir un texto en unidades mÃ¡s pequeÃ±as llamadas tokens. Esto puede implicar descomponer el texto en oraciones, palabras, subpalabras o incluso caracteres. Es importante porque permite convertir un texto no estructurado en uno estructurado, lo que facilita su anÃ¡lisis estadÃ­stico o su uso en modelos de machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4237df",
   "metadata": {},
   "source": [
    "#### 1.2.1 Ejercicio: Usar la biblioteca nltk o spaCy en Python para tokenizar un texto corto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb7a8f",
   "metadata": {},
   "source": [
    "##### 1.2.1.1 Ejercicio con Nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a299848",
   "metadata": {},
   "source": [
    "###### 1.2.1.1.1 Escribir un texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31209251",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text = \"I am learning about tokenization!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b13422",
   "metadata": {},
   "source": [
    "###### 1.2.1.1.2 Tokenizar por oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bead825c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentance list: ['I am learning about tokenization!'].\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(nltk_text)\n",
    "print(f\"Sentance list: {sentences}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638e2c3d",
   "metadata": {},
   "source": [
    "###### 1.2.1.1.3 Tokenizar por palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "073b19e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word list: ['I', 'am', 'learning', 'about', 'tokenization', '!'].\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(nltk_text)\n",
    "print(f\"Word list: {words}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994196ea",
   "metadata": {},
   "source": [
    "###### 1.2.1.1.4 Tokenizar por subpalabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8ab81ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subword list: ['i', 'am', 'learn', 'about', 'token', '!'].\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer_subwords = [stemmer.stem(word) for word in words]\n",
    "print(f\"Subword list: {stemmer_subwords}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f94dc2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subword list: ['I', 'am', 'learning', 'about', 'tokenization', '!'].\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer_subwords = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(f\"Subword list: {lemmatizer_subwords}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1826e77f",
   "metadata": {},
   "source": [
    "###### 1.2.1.1.5 Tokenizar por caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "34448aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character List: ['I', ' ', 'a', 'm', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'a', 'b', 'o', 'u', 't', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', '!']\n"
     ]
    }
   ],
   "source": [
    "characters = list(nltk_text)\n",
    "print(f\"Character List: {characters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e09289",
   "metadata": {},
   "source": [
    "#### 1.2.1.2 Ejercicio con spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806836f2",
   "metadata": {},
   "source": [
    "###### 1.2.1.2.1 Cargar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d770c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fcb12428",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_text = \"I am learning how to do tokenization with spacy. It is awesome!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff991372",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(spacy_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d2e799",
   "metadata": {},
   "source": [
    "###### 1.2.1.2.2 Tokenizar por oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a1f94bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I am learning how to do tokenization with spacy., It is awesome!]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "for sentence in doc.sents:\n",
    "    sentences.append(sentence)\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab2178",
   "metadata": {},
   "source": [
    "###### 1.2.1.2.2 Tokenizar por words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ab00e897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'how',\n",
       " 'to',\n",
       " 'do',\n",
       " 'tokenization',\n",
       " 'with',\n",
       " 'spacy',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'awesome',\n",
       " '!']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for token in doc:\n",
    "    words.append(token.text)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db6c0b9",
   "metadata": {},
   "source": [
    "###### 1.2.1.2.3 Tokenizar por caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5736e1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " ' ',\n",
       " 'a',\n",
       " 'm',\n",
       " ' ',\n",
       " 'l',\n",
       " 'e',\n",
       " 'a',\n",
       " 'r',\n",
       " 'n',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'h',\n",
       " 'o',\n",
       " 'w',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'd',\n",
       " 'o',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " 'k',\n",
       " 'e',\n",
       " 'n',\n",
       " 'i',\n",
       " 'z',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'w',\n",
       " 'i',\n",
       " 't',\n",
       " 'h',\n",
       " ' ',\n",
       " 's',\n",
       " 'p',\n",
       " 'a',\n",
       " 'c',\n",
       " 'y',\n",
       " '.',\n",
       " ' ',\n",
       " 'I',\n",
       " 't',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 'w',\n",
       " 'e',\n",
       " 's',\n",
       " 'o',\n",
       " 'm',\n",
       " 'e',\n",
       " '!']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters = list(spacy_text)\n",
    "characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d227308c",
   "metadata": {},
   "source": [
    "## 2. Embeddings y Representaciones Vectoriales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da688d83",
   "metadata": {},
   "source": [
    "### 2.1 Pregunta 3: Â¿QuÃ© es un embedding de palabras y cÃ³mo representa el significado de una palabra en el espacio vectorial?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f6e946",
   "metadata": {},
   "source": [
    "#### 2.1.1 Ejercicio: Cargar un modelo de word embeddings de Hugging Face (por ejemplo, glove o word2vec) y visualizar los embeddings de palabras similares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661ee107",
   "metadata": {},
   "source": [
    "### 2.2 Pregunta 4: Explica las diferencias entre embeddings estÃ¡ticos y dinÃ¡micos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3343df61",
   "metadata": {},
   "source": [
    "**Los embeddings estÃ¡ticos**, como GloVe o Word2Vec, generan una representaciÃ³n vectorial fija para cada palabra en el vocabulario. Estas representaciones se calculan una vez y no cambian segÃºn el contexto en el que se encuentra la palabra y tienen las siguientes caracterÃ­sticas:\n",
    "\n",
    "- **Contexto fijo**: Cada palabra tiene una Ãºnica representaciÃ³n (por ejemplo, \"banco\" como entidad financiera y \"banco\" como lugar para sentarse tienen la misma representaciÃ³n).\n",
    "\n",
    "- **RÃ¡pido**: Son computacionalmente mÃ¡s eficientes y requieren menos recursos de memoria.\n",
    "\n",
    "- **Limitaciones**: No capturan el significado contextual que puede variar en diferentes oraciones.\n",
    "\n",
    "Por otra parte, los **embeddings dinÃ¡micos** como BERT (Bidirectional Encoder Representations from Transformers), generan representaciones que dependen del contexto en que se usa la palabra. La ubicaciÃ³n de la palabra dentro de una frase influye en su vector de representaciÃ³n y tienen las siguientes caracterÃ­sticas:\n",
    "\n",
    "- **Contexto variable**: Cada palabra puede tener mÃºltiples representaciones dependiendo de su contexto; por ejemplo, \"banco\" en \"fui al banco\" vs. \"me sentÃ© en el banco\".\n",
    "\n",
    "- **MÃ¡s preciso**: Capturan significados mÃ¡s matizados y complejos, lo que resulta en una mejor comprensiÃ³n semÃ¡ntica.\n",
    "- **Limitaciones**: mayor carga computacional porque requieren mÃ¡s recursos para calcular y almacenar, y suelen ser mÃ¡s lentos debido a su complejidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0363cd7",
   "metadata": {},
   "source": [
    "#### 2.2.1 Ejercicio: Comparar GloVe (estÃ¡tico) con BERT (dinÃ¡mico) mediante un ejemplo prÃ¡ctico en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd8259",
   "metadata": {},
   "source": [
    "## 3. Modelos Basados en Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4021a4e0",
   "metadata": {},
   "source": [
    "### 3.1 Pregunta 5: Â¿QuÃ© son las redes neuronales recurrentes (RNN) y las LSTM, y cÃ³mo se usan en NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d49fc12",
   "metadata": {},
   "source": [
    "Las **Redes Neuronales Recurrentes (RNN)** son un tipo de arquitectura de red neuronal diseÃ±ada para procesar datos secuenciales. A diferencia de las redes neuronales tradicionales, donde las entradas y salidas son independientes, las RNN mantienen una memoria interna que les permite recordar informaciÃ³n de entradas anteriores. Esto es Ãºtil en tareas como la traducciÃ³n automÃ¡tica, el reconocimiento de voz y el anÃ¡lisis de texto, donde el contexto es crucial para la predicciÃ³n.\n",
    "\n",
    "Uno de los principales problemas que enfrentan las RNN es el **desvanecimiento del gradiente**, que dificulta el aprendizaje de dependencias a largo plazo. A medida que se retropropagan los errores a travÃ©s del tiempo, los gradientes pueden volverse extremadamente pequeÃ±os, lo que impide que la red aprenda correctamente.\n",
    "\n",
    "En ese sentido, las **LSTM** (Long Short-Term Memory Ã³ Memoria a Largo Corto Plazo, en espaÃ±ol) son una variante avanzada de las RNN que fueron diseÃ±ada para aprender y recordar dependencias a largo plazo en secuencias de datos, abordando el problema del desvanecimiento del gradiente. Las LSTM incorporan una estructura mÃ¡s compleja con celdas de memoria y tres puertas:\n",
    "- **Puerta de entrada**: controla la cantidad de informaciÃ³n nueva que se almacena.\n",
    "- **Puerta de olvido**: decide quÃ© informaciÃ³n debe ser descartada.\n",
    "- **Puerta de salida**: regula quÃ© informaciÃ³n se utiliza para generar la salida.\n",
    "\n",
    "Esta arquitectura permite a las LSTM recordar informaciÃ³n durante perÃ­odos mÃ¡s largos, lo que es crucial en tareas donde el contexto puede estar lejos en la secuencia.\n",
    "\n",
    " \n",
    "\n",
    "**Aplicaciones en Procesamiento del Lenguaje Natural (PNL)**\n",
    "\n",
    "Las RNN y LSTM son ampliamente utilizadas en NLP para tareas como:\n",
    "- **TraducciÃ³n automÃ¡tica**: donde el modelo debe entender contextos complejos.\n",
    "- **AnÃ¡lisis de sentimientos**: donde se requiere comprender la secuencia completa para hacer una predicciÃ³n precisa.\n",
    "- **GeneraciÃ³n de texto**: donde el modelo genera texto coherente basado en entradas previas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1960ed7",
   "metadata": {},
   "source": [
    "#### 3.1.1 Ejercicio: Implementar una RNN simple para predicciÃ³n de secuencias en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415c7d9",
   "metadata": {},
   "source": [
    "### 3.2 Pregunta 6: Â¿QuÃ© son los transformers y por quÃ© han reemplazado en gran medida a las RNN en NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d607a36",
   "metadata": {},
   "source": [
    "Un transformador es un modelo de una rede neural que aprende del contexto de una secuencia de datos para generar nuevos datos. Su funcionamiento empieza con la tokenizaciÃ³n en donde descompone el texto a unidades llamadas tokens. Estos tokens se convierten en valores numÃ©ricos mediante los embeddings y se les asigna una posiciÃ³n de cada palabra con la codificaciÃ³n posicional. La informaciÃ³n se envÃ­a al codificador en donde ejecuta un mecanismo de autoatenciÃ³n y un mecanismo de mecanismo de atenciÃ³n multicabezal con el fin de devolver la representaciÃ³n de la entrada. Por Ãºltimo, envÃ­a el texto al decodificador para generar el texto objetivo. Esta arquitectura ha reemplazado a las redes neuronales recurrentes (RNNs) porque procesan el texto de forma paralela en cambio de forma secuencial lo cual mejorar el rendimiento del tiempo, pueden seguir el contexto del tema aÃºn si la secuencia del texto es muy largo y aprenden mÃ¡s rÃ¡pido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f9054",
   "metadata": {},
   "source": [
    "#### 3.2.1 Ejercicio: Utilizar el modelo de transformers BERT o GPT-2 de Hugging Face para completar una frase simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1fb9f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Being a computer scientist is all about\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f25d02e",
   "metadata": {},
   "source": [
    "##### 3.2.1.1 Utilizar EleutherAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c9beb236",
   "metadata": {},
   "outputs": [],
   "source": [
    "eleutherAI_generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d62a8b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Being a computer scientist is all about the ability to analyze and understand the world. The world is a'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(85)\n",
    "eleutherAI_prediction = eleutherAI_generator(text)\n",
    "eleutherAI_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748b30bd",
   "metadata": {},
   "source": [
    "##### 3.2.1.2 Utilizar GPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f36f38c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "gpt2_generator = pipeline(\"text-generation\", model=\"gpt2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5592502c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Being a computer scientist is all about getting what you want. But you're still a little frustrated when you can't make progress, and those things are what drive your problems: your frustration with your work, your frustration with technology.\\n\\nThere is\"}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_prediction = gpt2_generator(text)\n",
    "gpt2_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9df00d0",
   "metadata": {},
   "source": [
    "## 4. Modelos Pre-entrenados y Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade7df70",
   "metadata": {},
   "source": [
    "### 4.1 Pregunta 7: Â¿QuÃ© significa que un modelo estÃ© preentrenado? Â¿CÃ³mo se puede ajustar para una tarea especÃ­fica?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9dfe86",
   "metadata": {},
   "source": [
    "#### 4.1.1 Ejercicio: Usar un modelo de Hugging Face (distilBERT) para clasificar un conjunto de datos de sentimientos (usar el dataset de Rotten tomatoes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10309397",
   "metadata": {},
   "source": [
    "### 4.2 Pregunta 8: Â¿QuÃ© es el enmascaramiento de palabras en el preentrenamiento de modelos como BERT?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f65fdf5",
   "metadata": {},
   "source": [
    "El **enmascaramiento de palabras** es una tÃ©cnica fundamental en el preentrenamiento de modelos de lenguaje como BERT. Consiste en enmascarar aleatoriamente algunas palabras en una oraciÃ³n y entrenar al modelo para predecir la palabra original en funciÃ³n del contexto proporcionado por las palabras circundantes. Es importante porque permite:\n",
    "- **ComprensiÃ³n profunda del lenguaje**: Al intentar predecir palabras faltantes, el modelo aprende a captar las relaciones semÃ¡nticas y sintÃ¡cticas entre las palabras, lo que le permite desarrollar una comprensiÃ³n profunda del lenguaje.\n",
    "- **GeneraciÃ³n de contexto**: El enmascaramiento fuerza al modelo a generar un contexto rico y coherente para cada palabra enmascarada, lo que es esencial para tareas posteriores como la generaciÃ³n de texto y la traducciÃ³n.\n",
    "\n",
    "**Ejemplo**: Si tenemos la frase \"El gato <mask> un ratÃ³n\", el modelo podrÃ­a ser entrenado para predecir que la palabra faltante es \"cazÃ³\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47691e9",
   "metadata": {},
   "source": [
    "#### 4.1.2 Ejercicio: Usar BERT para predecir palabras enmascaradas en una oraciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a9c4c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Being a computer scientist is all about [MASK].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "38cd6370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "00cc48a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.11361566185951233,\n",
       "  'token': 7588,\n",
       "  'token_str': 'computers',\n",
       "  'sequence': 'being a computer scientist is all about computers.'},\n",
       " {'score': 0.06099285930395126,\n",
       "  'token': 2671,\n",
       "  'token_str': 'science',\n",
       "  'sequence': 'being a computer scientist is all about science.'},\n",
       " {'score': 0.05559048056602478,\n",
       "  'token': 2009,\n",
       "  'token_str': 'it',\n",
       "  'sequence': 'being a computer scientist is all about it.'},\n",
       " {'score': 0.048937588930130005,\n",
       "  'token': 2470,\n",
       "  'token_str': 'research',\n",
       "  'sequence': 'being a computer scientist is all about research.'},\n",
       " {'score': 0.031744930893182755,\n",
       "  'token': 8785,\n",
       "  'token_str': 'math',\n",
       "  'sequence': 'being a computer scientist is all about math.'}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_predition = bert_mask(text)\n",
    "bert_predition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870140cb",
   "metadata": {},
   "source": [
    "## 5. Tareas ClÃ¡sicas de NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e938d75",
   "metadata": {},
   "source": [
    "### 5.1 **Pregunta 9**: Â¿QuÃ© es la clasificaciÃ³n de texto y quÃ© modelos se suelen utilizar para esta tarea?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e6d4d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "946135f3",
   "metadata": {},
   "source": [
    "#### Ejercicio: Implementar un modelo NER utilizando spaCy o Hugging Face y aplicar etiquetas a un conjunto de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff632c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a000aa1f",
   "metadata": {},
   "source": [
    "### 5.2 **Pregunta 10**: Â¿QuÃ© es el anÃ¡lisis de sentimientos y cÃ³mo se relaciona con la clasificaciÃ³n de texto?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd7db0d",
   "metadata": {},
   "source": [
    "El **anÃ¡lisis de sentimientos** es una tÃ©cnica del procesamiento de lenguaje natural (PNL) que se utiliza para identificar y clasificar las opiniones expresadas en un texto. Generalmente, estas opiniones se categorizan en tres clases: positivas, negativas o neutras. Este proceso implica el uso de tÃ©cnicas de minerÃ­a de textos y aprendizaje automÃ¡tico para extraer informaciÃ³n subjetiva, permitiendo a las organizaciones comprender mejor las emociones y actitudes de los usuarios hacia productos, servicios o temas especÃ­ficos. El anÃ¡lisis de sentimientos se considera una forma especÃ­fica de clasificaciÃ³n de texto, donde el objetivo es asignar una etiqueta (polaridad) a un fragmento de texto basado en su contenido emocional. Esto puede incluir:\n",
    "- **ClasificaciÃ³n binaria**: donde el texto se clasifica como positivo o negativo.\n",
    "- **ClasificaciÃ³n multiclase**: que puede incluir categorÃ­as adicionales como neutral o mixta.\n",
    "\n",
    "El anÃ¡lisis de sentimientos puede ser visto como un subtipo dentro del campo mÃ¡s amplio de la clasificaciÃ³n de texto, que abarca tareas como la categorizaciÃ³n automÃ¡tica de documentos, el etiquetado temÃ¡tico y la detecciÃ³n de spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827fa79d",
   "metadata": {},
   "source": [
    "#### 5.2.1 Ejercicio: Utilizar Hugging Face para hacer anÃ¡lisis de sentimientos en un conjunto de datos de reseÃ±as (usar el dataset de IMDB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ff4157",
   "metadata": {},
   "source": [
    "##### 5.2.1.1 Cargar 10 datos del dataset IMDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb9bd2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\", split=\"train\").shuffle(seed=42).select(range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eae858",
   "metadata": {},
   "source": [
    "##### 5.2.1.2 Inicializar el pipeline de anÃ¡lisis de sentimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71ede6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\guill\\.cache\\huggingface\\hub\\models--lxyuan--distilbert-base-multilingual-cased-sentiments-student. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0adb8d",
   "metadata": {},
   "source": [
    "##### 5.2.1.3 Analizar los sentimientos de los textos del dataset de IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea646647",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sentiment_pipeline(dataset['text'], truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eab347",
   "metadata": {},
   "source": [
    "##### 5.2.1.4 Imprimir los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e63c545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label     score\n",
      "0  There is no relation at all between Fortier an...  positive  0.466838\n",
      "1  This movie is a great. The plot is very true t...  positive  0.963030\n",
      "2  George P. Cosmatos' \"Rambo: First Blood Part I...  negative  0.684366\n",
      "3  In the process of trying to establish the audi...  negative  0.714222\n",
      "4  Yeh, I know -- you're quivering with excitemen...  negative  0.608305\n",
      "5  While this movie's style isn't as understated ...  positive  0.646290\n",
      "6  I give this movie 7 out of 10 because the vill...  positive  0.418127\n",
      "7  really awful... lead actor did OK... the film,...  negative  0.524330\n",
      "8  Good grief I can't even begin to describe how ...  negative  0.838564\n",
      "9  Home Room deals with a Columbine-like high-sch...  negative  0.594892\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convertir resultados a DataFrame y mostrar\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['text'] = dataset['text']\n",
    "results_df = results_df[['text', 'label', 'score']]\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d55173",
   "metadata": {},
   "source": [
    "### 5.3 **Pregunta 11**: Explica el concepto de etiquetado de secuencias (named entity recognition, NER). Â¿Para quÃ© se utiliza?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be75ae9",
   "metadata": {},
   "source": [
    "El **etiquetado de secuencias** o NER es una tarea de NLP que consiste en identificar y clasificar entidades nombradas en un texto. Las entidades nombradas pueden ser personas, organizaciones, ubicaciones, fechas, etc. Se utiliza para:\n",
    "- **ExtracciÃ³n de informaciÃ³n**: NER es fundamental para extraer informaciÃ³n relevante de grandes volÃºmenes de texto, como noticias, artÃ­culos cientÃ­ficos y documentos legales.\n",
    "- **AnÃ¡lisis de sentimientos**: Al identificar las entidades, se pueden analizar los sentimientos asociados a ellas.\n",
    "- **Sistemas de recomendaciÃ³n**: NER puede utilizarse para personalizar recomendaciones basadas en las preferencias del usuario.\n",
    "- **Chatbots y asistentes virtuales**: Los Chatbots pueden utilizar NER para comprender mejor las consultas de los usuarios y proporcionar respuestas mÃ¡s precisas.\n",
    "\n",
    "**Ejemplo**: En la frase \"El presidente de Estados Unidos visitÃ³ ParÃ­s el 14 de julio\", NER identificarÃ­a \"Estados Unidos\" como una ubicaciÃ³n, \"ParÃ­s\" como otra ubicaciÃ³n y \"14 de julio\" como una fecha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15aa3f7",
   "metadata": {},
   "source": [
    "#### 5.3.1 **Ejercicio**: Implementar un modelo NER utilizando spaCy o Hugging Face y aplicar etiquetas a un conjunto de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e7083953",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\"\n",
    "Trinamool Congress leader Mahua Moitra has moved the Supreme Court against her expulsion from the Lok Sabha over the cash-for-query allegations against her. Moitra was ousted from the Parliament last week after the Ethics Committee of the Lok Sabha found her guilty of jeopardising national security by sharing her parliamentary portal's login credentials with businessman Darshan Hiranandani.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e339d7da",
   "metadata": {},
   "source": [
    "##### 5.3.1.1 Utilizar spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c60f0849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "678fc631",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5191b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9e695d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This word Trinamool Congress is this ORG entity.\n",
      "This word Mahua Moitra is this PERSON entity.\n",
      "This word the Supreme Court is this ORG entity.\n",
      "This word the Lok Sabha is this PERSON entity.\n",
      "This word Moitra is this ORG entity.\n",
      "This word Parliament is this ORG entity.\n",
      "This word last week is this DATE entity.\n",
      "This word the Ethics Committee is this ORG entity.\n",
      "This word Darshan Hiranandani is this PERSON entity.\n"
     ]
    }
   ],
   "source": [
    "for word in doc.ents:\n",
    "    print(f\"This word {word.text} is this {word.label_} entity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf8b521",
   "metadata": {},
   "source": [
    "#### 3.1.1.2 Utilizar Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9f383e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bert_ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6406bafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-ORG', 'score': 0.9995658, 'index': 2, 'word': 'Tri', 'start': 2, 'end': 5}, {'entity': 'I-ORG', 'score': 0.99883586, 'index': 3, 'word': '##nam', 'start': 5, 'end': 8}, {'entity': 'I-ORG', 'score': 0.99924135, 'index': 4, 'word': '##ool', 'start': 8, 'end': 11}, {'entity': 'I-ORG', 'score': 0.99926585, 'index': 5, 'word': 'Congress', 'start': 12, 'end': 20}, {'entity': 'B-PER', 'score': 0.999546, 'index': 7, 'word': 'Ma', 'start': 28, 'end': 30}, {'entity': 'I-PER', 'score': 0.772231, 'index': 8, 'word': '##hua', 'start': 30, 'end': 33}, {'entity': 'I-PER', 'score': 0.9996074, 'index': 9, 'word': 'Mo', 'start': 34, 'end': 36}, {'entity': 'I-PER', 'score': 0.99346304, 'index': 10, 'word': '##it', 'start': 36, 'end': 38}, {'entity': 'I-PER', 'score': 0.97454643, 'index': 11, 'word': '##ra', 'start': 38, 'end': 40}, {'entity': 'B-ORG', 'score': 0.99936503, 'index': 15, 'word': 'Supreme', 'start': 55, 'end': 62}, {'entity': 'I-ORG', 'score': 0.99817765, 'index': 16, 'word': 'Court', 'start': 63, 'end': 68}, {'entity': 'B-ORG', 'score': 0.9984717, 'index': 22, 'word': 'Lok', 'start': 100, 'end': 103}, {'entity': 'I-ORG', 'score': 0.9983095, 'index': 23, 'word': 'Sabha', 'start': 104, 'end': 109}, {'entity': 'B-PER', 'score': 0.9994796, 'index': 36, 'word': 'Mo', 'start': 159, 'end': 161}, {'entity': 'I-PER', 'score': 0.7041658, 'index': 37, 'word': '##it', 'start': 161, 'end': 163}, {'entity': 'I-PER', 'score': 0.7547127, 'index': 38, 'word': '##ra', 'start': 163, 'end': 165}, {'entity': 'B-ORG', 'score': 0.99915445, 'index': 49, 'word': 'Ethics', 'start': 217, 'end': 223}, {'entity': 'I-ORG', 'score': 0.9969874, 'index': 50, 'word': 'Committee', 'start': 224, 'end': 233}, {'entity': 'B-ORG', 'score': 0.99641955, 'index': 53, 'word': 'Lok', 'start': 241, 'end': 244}, {'entity': 'I-ORG', 'score': 0.99853647, 'index': 54, 'word': 'Sabha', 'start': 245, 'end': 250}, {'entity': 'B-PER', 'score': 0.99957997, 'index': 77, 'word': 'Dar', 'start': 375, 'end': 378}, {'entity': 'B-PER', 'score': 0.78188264, 'index': 78, 'word': '##shan', 'start': 378, 'end': 382}, {'entity': 'I-PER', 'score': 0.9996786, 'index': 79, 'word': 'Hi', 'start': 383, 'end': 385}, {'entity': 'I-PER', 'score': 0.99624854, 'index': 80, 'word': '##rana', 'start': 385, 'end': 389}, {'entity': 'I-PER', 'score': 0.99812657, 'index': 81, 'word': '##nda', 'start': 389, 'end': 392}]\n"
     ]
    }
   ],
   "source": [
    "ner_results = bert_ner(text)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbdc21a",
   "metadata": {},
   "source": [
    "## 6. GeneraciÃ³n de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d1fd75",
   "metadata": {},
   "source": [
    "### 6.1 **Pregunta 12**: Â¿CÃ³mo funcionan los modelos de generaciÃ³n de texto como GPT-2 o GPT-3?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bb2cbf",
   "metadata": {},
   "source": [
    "#### 6.1.1 **Ejercicio**: Usar Hugging Face para generar texto a partir de un prompt con GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9fa9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc0d3e2b",
   "metadata": {},
   "source": [
    "### 6.2 **Pregunta 13**: Â¿CuÃ¡les son los desafÃ­os Ã©ticos asociados con los modelos de generaciÃ³n de texto?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ed02b2",
   "metadata": {},
   "source": [
    "Los modelos de generaciÃ³n de texto (NLP), han avanzado significativamente en los Ãºltimos aÃ±os, pero tambiÃ©n plantean desafÃ­os Ã©ticos cruciales; algunos de los mÃ¡s relevantes pueden ser:\n",
    "- **Sesgos**: Los modelos aprenden de los datos con los que son entrenados. Si estos datos contienen sesgos sociales, culturales o de otro tipo, el modelo los reproducirÃ¡ y amplificarÃ¡ en sus salidas. Esto puede llevar a la discriminaciÃ³n, la perpetuaciÃ³n de estereotipos y la generaciÃ³n de contenido ofensivo o daÃ±ino.\n",
    "- **DesinformaciÃ³n y \"deepfakes\" de texto**: La capacidad de generar texto realista y coherente puede ser utilizada para crear noticias falsas, propaganda o contenido engaÃ±oso. Esto plantea serias amenazas a la democracia y la confianza en la informaciÃ³n.\n",
    "- **Privacidad**: Los modelos pueden ser entrenados con grandes cantidades de datos que incluyen informaciÃ³n personal. Esto plantea preocupaciones sobre la privacidad y la seguridad de los datos.\n",
    "- **Responsabilidad**: Â¿QuiÃ©n es responsable del contenido generado por un modelo? Â¿El desarrollador, el usuario o el modelo mismo? Esta pregunta es compleja y aÃºn no tiene una respuesta clara.\n",
    "- **Mal uso**: Los modelos de lenguaje pueden ser utilizados para cometer delitos, como el acoso en lÃ­nea o la extorsiÃ³n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c004df",
   "metadata": {},
   "source": [
    "#### 6.2.1 **Ejercicio**: Discutir sobre sesgos en los modelos de lenguaje generativo y su impacto en aplicaciones reales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7519a32",
   "metadata": {},
   "source": [
    " Los sesgos en los modelos de lenguaje generativo tienen un impacto significativo en diversas aplicaciones reales. Algunos ejemplos incluyen:\n",
    "\n",
    "- **TraducciÃ³n automÃ¡tica**: Un modelo entrenado con datos sesgados puede generar traducciones que refuerzan estereotipos o discriminan a ciertos grupos.\n",
    "\n",
    "- **Chatbots y asistentes virtuales**: Un chatbot sesgado puede proporcionar respuestas discriminatorias u ofensivas a preguntas o solicitudes.\n",
    "\n",
    "- **GeneraciÃ³n de contenido creativo**: Un modelo puede generar contenido que perpetÃºe estereotipos de gÃ©nero, raza u orientaciÃ³n sexual.\n",
    "\n",
    "- **Toma de decisiones**: Los modelos de lenguaje se utilizan cada vez mÃ¡s para tomar decisiones en Ã¡reas como la contrataciÃ³n, la justicia penal y la atenciÃ³n mÃ©dica. Si estos modelos estÃ¡n sesgados, pueden llevar a resultados injustos.\n",
    "\n",
    "Para mitigar estos problemas, es necesario:\n",
    "\n",
    "- **Datos de entrenamiento diversos y representativos**: Los conjuntos de datos utilizados para entrenar los modelos deben ser lo mÃ¡s diversos y representativos posible para reducir la probabilidad de sesgos.\n",
    "\n",
    "- **TÃ©cnicas de mitigaciÃ³n de sesgos**: Existen diversas tÃ©cnicas para identificar y mitigar los sesgos en los modelos, como la detecciÃ³n de sesgos, la correcciÃ³n de sesgos y la auditorÃ­a de modelos.\n",
    "\n",
    "- **Transparencia y explicabilidad**: Es importante que los desarrolladores de modelos sean transparentes sobre los datos utilizados para entrenar los modelos y sobre las tÃ©cnicas de mitigaciÃ³n de sesgos empleadas.\n",
    "\n",
    "- **EvaluaciÃ³n continua**: Los modelos deben ser evaluados continuamente para detectar y corregir nuevos sesgos que puedan surgir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae312f3",
   "metadata": {},
   "source": [
    "## 7. EvaluaciÃ³n de Modelos NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcff8ceb",
   "metadata": {},
   "source": [
    "### 7.1 **Pregunta 14**: Â¿QuÃ© mÃ©tricas se utilizan para evaluar la calidad de los modelos de NLP en tareas como clasificaciÃ³n o generaciÃ³n de texto?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099fde33",
   "metadata": {},
   "source": [
    "Existen diversas mÃ©tricas para evaluar la calidad de los modelos de NLP, dependiendo de la tarea especÃ­fica. Algunas de las mÃ¡s comunes son:\n",
    "\n",
    "- **PrecisiÃ³n (Precision), Recuerdo (Recall) y F1-score**: Estas mÃ©tricas se utilizan comÃºnmente en tareas de clasificaciÃ³n, como el anÃ¡lisis de sentimientos o la clasificaciÃ³n de textos. Miden la proporciÃ³n de predicciones correctas, la proporciÃ³n de elementos relevantes recuperados y el equilibrio entre ambas, respectivamente.\n",
    "- **BLEU (Bilingual Evaluation Understudy)**: Se utiliza para evaluar la calidad de la traducciÃ³n automÃ¡tica y la generaciÃ³n de texto. Compara las n-gramas de las traducciones generadas con las de las traducciones de referencia.\n",
    "- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: Similar a BLEU, pero se enfoca en la recuperaciÃ³n de n-gramas, lo que lo hace mÃ¡s adecuado para tareas de resumen automÃ¡tico.\n",
    "- **Perplejidad**: Mide la sorpresa de un modelo ante una nueva secuencia de palabras. Un modelo bien entrenado deberÃ­a tener una baja perplejidad.\n",
    "- **Coherencia y fluidez**: Para evaluar la calidad de la generaciÃ³n de texto, se pueden utilizar mÃ©tricas subjetivas como la coherencia y la fluidez del texto generado.\n",
    "\n",
    "La elecciÃ³n de la mÃ©trica adecuada depende aspectos como:\n",
    "- La tarea especÃ­fica (clasificaciÃ³n, generaciÃ³n, etc.)\n",
    "- Los datos disponibles\n",
    "- Los objetivos del proyecto\n",
    "\n",
    "En general, el enmascaramiento de palabras es una tÃ©cnica clave para preentrenar modelos de lenguaje, NER es una tarea fundamental para extraer informaciÃ³n de texto, y existen diversas mÃ©tricas para evaluar la calidad de los modelos de NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd0515c",
   "metadata": {},
   "source": [
    "#### 7.1.1 **Ejercicio**: Usar mÃ©tricas como accuracy, precision, y recall para evaluar un modelo de clasificaciÃ³n de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfffb9d9",
   "metadata": {},
   "source": [
    "##### 7.1.1.1 Guardar las mÃ©tricas de accuracy, precision y recall en una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "658e60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"accuracy\", \"recall\", \"precision\", \"f1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4081e248",
   "metadata": {},
   "source": [
    "##### 7.1.1.2 Cargar los datos de imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1d121ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"imdb\", split=\"test\").shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663f3b2a",
   "metadata": {},
   "source": [
    "##### 7.1.1.3 Cargar el evalvador para clasificaciÃ³n de textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e8a231c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_evaluator = evaluator(\"text-classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb264e1",
   "metadata": {},
   "source": [
    "##### 7.1.1.4 Cargar el modelo de clasificaciÃ³n de textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "179a2456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "imdb_classification = pipeline(\"text-classification\", model=\"lvwerra/distilbert-imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f66c7",
   "metadata": {},
   "source": [
    "##### 7.1.1.5 Evaluar los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ce349859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"accuracy\": 0.918,\n",
      "    \"recall\": 0.9180327868852459,\n",
      "    \"precision\": 0.9142857142857143,\n",
      "    \"f1\": 0.9161554192229039,\n",
      "    \"total_time_in_seconds\": 119.51120690000243,\n",
      "    \"samples_per_second\": 8.36741612723166,\n",
      "    \"latency_in_seconds\": 0.11951120690000243\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline = imdb_classification,\n",
    "    data = data,\n",
    "    metric = evaluate.combine(metrics),\n",
    "    label_mapping = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    ")\n",
    "print(json.dumps(eval_results, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dfc506",
   "metadata": {},
   "source": [
    "### 7.2 **Pregunta 15**: Â¿QuÃ© es la perplejidad y cÃ³mo se utiliza para evaluar modelos de generaciÃ³n de texto?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e460386",
   "metadata": {},
   "source": [
    "#### 7.2.1 **Ejercicio**: Calcular la perplejidad de un modelo de lenguaje en Hugging Face para una tarea de generaciÃ³n de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a82a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
