{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90fd06a3",
   "metadata": {},
   "source": [
    "# Taller 1 Conceptos básicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb0a2e",
   "metadata": {},
   "source": [
    "## 1. Introducción a NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc47014",
   "metadata": {},
   "source": [
    "### 1.1 Ejercicio: Investigar 3 aplicaciones actuales de NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4237df",
   "metadata": {},
   "source": [
    "### 1.2 Ejercicio: Usar la biblioteca nltk o spaCy en Python para tokenizar un texto corto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b436a3c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7eb7a8f",
   "metadata": {},
   "source": [
    "#### 1.2.1 Ejercicio con Nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36dc56",
   "metadata": {},
   "source": [
    "##### 1.2.1.1 Intalación de NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fce0e038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\guill\\appdata\\roaming\\python\\python310\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\guill\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\guill\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\guill\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\guill\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\guill\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ab5736",
   "metadata": {},
   "source": [
    "##### 1.2.1.2 Importar las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6923a91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guill\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:143: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 2.0.2)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize, sent_tokenize\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer, WordNetLemmatizer\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\__init__.py:133\u001b[0m\n\u001b[0;32m    125\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mPopen \u001b[38;5;241m=\u001b[39m _fake_Popen\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# TOP-LEVEL MODULES\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# Import top-level functionality into top-level namespace\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollocations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator, memoize\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatstruct\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\collocations.py:36\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_itertools\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# these two unused imports are referenced in collocations.doctest\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     37\u001b[0m     BigramAssocMeasures,\n\u001b[0;32m     38\u001b[0m     ContingencyMeasures,\n\u001b[0;32m     39\u001b[0m     QuadgramAssocMeasures,\n\u001b[0;32m     40\u001b[0m     TrigramAssocMeasures,\n\u001b[0;32m     41\u001b[0m )\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspearman\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ranks_from_scores, spearman_correlation\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FreqDist\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\metrics\\__init__.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magreement\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnnotationTask\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m align\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massociation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     19\u001b[0m     BigramAssocMeasures,\n\u001b[0;32m     20\u001b[0m     ContingencyMeasures,\n\u001b[0;32m     21\u001b[0m     NgramAssocMeasures,\n\u001b[0;32m     22\u001b[0m     QuadgramAssocMeasures,\n\u001b[0;32m     23\u001b[0m     TrigramAssocMeasures,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfusionmatrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrix\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     binary_distance,\n\u001b[0;32m     28\u001b[0m     custom_distance,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     presence,\n\u001b[0;32m     36\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\metrics\\association.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m _SMALL \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-20\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fisher_exact\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfisher_exact\u001b[39m(\u001b[38;5;241m*\u001b[39m_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\stats\\__init__.py:485\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    480\u001b[0m \n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 485\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyVersion\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m suppress_warnings\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cdist\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _measurements\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (check_random_state, MapWrapper,\n\u001b[0;32m     42\u001b[0m                               rng_integers, _rename_parameter, _contains_nan)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\spatial\\__init__.py:105\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m=============================================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mSpatial algorithms and data structures (:mod:`scipy.spatial`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m   QhullError\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_kdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_qhull\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\spatial\\_kdtree.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright Anne M. Archibald 2008\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Released under the scipy license\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cKDTree, cKDTreeNode\n\u001b[0;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance_p\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRectangle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKDTree\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminkowski_distance_p\u001b[39m(x, y, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[1;32m_ckdtree.pyx:1\u001b[0m, in \u001b[0;36minit scipy.spatial._ckdtree\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65705a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d0c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a946ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a299848",
   "metadata": {},
   "source": [
    "##### 1.2.1.3 Escribir un texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31209251",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text = \"I am learning about tokenization!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b13422",
   "metadata": {},
   "source": [
    "##### 1.2.1.3 Tokenizar por oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(nltk_text)\n",
    "print(f\"Sentance list: {sentences}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638e2c3d",
   "metadata": {},
   "source": [
    "##### 1.2.1.4 Tokenizar por palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b19e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(nltk_text)\n",
    "print(f\"Word list: {words}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994196ea",
   "metadata": {},
   "source": [
    "##### 1.2.1.5 Tokenizar por subpalabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ab81ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer_subwords = [stemmer.stem(word) for word in words]\n",
    "print(f\"Subword list: {stemmer_subwords}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94dc2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer_subwords = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(f\"Subword list: {lemmatizer_subwords}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1826e77f",
   "metadata": {},
   "source": [
    "##### 1.2.1.6 Tokenizar por caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34448aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = list(text)\n",
    "print(f\"Character List: {characters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e09289",
   "metadata": {},
   "source": [
    "#### 1.2.2 Ejercicio con spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5111bc6d",
   "metadata": {},
   "source": [
    "##### 1.2.2.1 Intalación de NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc1fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00076a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806836f2",
   "metadata": {},
   "source": [
    "##### 1.2.2.2 Importar las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbd6be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d770c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c1d2e",
   "metadata": {},
   "source": [
    "##### 1.2.2.3 Escribir un texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb12428",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_text = \"I am learning how to do tokenization with spacy. It is awesome!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f94bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1066d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab00e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for token in doc:\n",
    "    words.append(token.text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5736e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = list(spacy_text)\n",
    "characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d227308c",
   "metadata": {},
   "source": [
    "## 2. Embeddings y Representaciones Vectoriales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f6e946",
   "metadata": {},
   "source": [
    "### Ejercicio: Cargar un modelo de word embeddings de Hugging Face (por ejemplo, glove o word2vec) y visualizar los embeddings de palabras similares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0363cd7",
   "metadata": {},
   "source": [
    "### Ejercicio: Comparar GloVe (estático) con BERT (dinámico) mediante un ejemplo práctico en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd8259",
   "metadata": {},
   "source": [
    "## 3. Modelos Basados en Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1960ed7",
   "metadata": {},
   "source": [
    "### Ejercicio: Implementar una RNN simple para predicción de secuencias en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f9054",
   "metadata": {},
   "source": [
    "### Ejercicio: Realizar el fine-tuning de un modelo pre-entrenado de Hugging Face (BERT o RoBERTa) para completar una frase simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9df00d0",
   "metadata": {},
   "source": [
    "## 4. Modelos Pre-entrenados y Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47691e9",
   "metadata": {},
   "source": [
    "### Ejercicio: Usar BERT para predecir palabras enmascaradas en una oración"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9dfe86",
   "metadata": {},
   "source": [
    "### Ejercicio: Usar un modelo de Hugging Face (distilBERT) para clasificar un conjunto de datos de sentimientos (usar el dataset de Rotten tomatoes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870140cb",
   "metadata": {},
   "source": [
    "## 5. Tareas Clásicas de NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827fa79d",
   "metadata": {},
   "source": [
    "### Ejercicio: Utilizar Hugging Face para hacer análisis de sentimientos en un conjunto de datos de reseñas (usar el dataset de IMDB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946135f3",
   "metadata": {},
   "source": [
    "### Ejercicio: Implementar un modelo NER utilizando spaCy o Hugging Face y aplicar etiquetas a un conjunto de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbdc21a",
   "metadata": {},
   "source": [
    "## 6. Generación de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bb2cbf",
   "metadata": {},
   "source": [
    "### Ejercicio: Usar Hugging Face para generar texto a partir de un prompt con GPT-2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c004df",
   "metadata": {},
   "source": [
    "### Ejercicio: Discutir sobre sesgos en los modelos de lenguaje generativo y su impacto en aplicaciones reales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae312f3",
   "metadata": {},
   "source": [
    "## 7. Evaluación de Modelos NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd0515c",
   "metadata": {},
   "source": [
    "### Ejercicio: Usar métricas como accuracy, precision, y recall para evaluar un modelo de clasificación de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e460386",
   "metadata": {},
   "source": [
    "### Ejercicio: Calcular la perplejidad de un modelo de lenguaje en Hugging Face para una tarea de generación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a82a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
